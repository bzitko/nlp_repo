{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from helper import look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram\n",
    "\n",
    "Vocabulary size 5:\n",
    "\n",
    "$V = \\{w_0, w_1, w_2, w_3, w_4\\}$\n",
    "\n",
    "One-hot of context word:\n",
    "\n",
    "$o_c = o_1 = \\begin{bmatrix}0 \\\\ 1 \\\\  0 \\\\  0 \\\\  0\\end{bmatrix}$\n",
    "\n",
    "3D embeddings for context words:\n",
    "\n",
    "$E^I = \\begin{bmatrix}-0.4024 & 1.0039 & -0.0477 &  0.8278 & -0.0195 \\\\ -0.8807 & 2.2967 & -0.1497 & -0.4184 & -1.0950 \\\\  0.4404 & 1.3009 &  0.0189 &  0.9072 &  1.0590 \\end{bmatrix} $\n",
    "\n",
    "Embedding of context word:\n",
    "\n",
    "$e_c = E^I \\cdot o_c = \\begin{bmatrix} 1.0039 \\\\ 2.2967 \\\\ 1.3009\\end{bmatrix}$\n",
    "\n",
    "One-hot of target word:\n",
    "\n",
    "$o_t = o_3 = \\begin{bmatrix}0 \\\\ 0 \\\\  0 \\\\  1 \\\\  0\\end{bmatrix}$\n",
    "\n",
    "3D embeddings for target words:\n",
    "\n",
    "$E^O = \\begin{bmatrix} 1.7711 & -0.2803 & -0.6955 &  1.0053 & -0.1339 \\\\ -1.6385 &  0.1701 &  0.2023 &  0.4325 & -0.4994 \\\\ -2.2435 &  0.1664 &  1.4737 & -1.1486 & -0.0510 \\end{bmatrix} = \\begin{bmatrix} e_{w_0} & e_{w_1} & e_{w_2} & e_{w_3} & e_{w_4} \\end{bmatrix}$\n",
    "\n",
    "Each possible target embedding $e_{w_i}$ multiply with content embedding $e_c$ and turn to probabilities.\n",
    "\n",
    "$\\begin{bmatrix} \\frac{e_{w_0} e_c}{\\sum_{j=0}^{4} e_{w_j} e_c} & \\frac{e_{w_1} e_c}{\\sum_{j=0}^{4} e_{w_j} e_c} & \\frac{e_{w_2} e_c}{\\sum_{j=0}^{4} e_{w_j} e_c} & \\frac{e_{w_3} e_c}{\\sum_{j=0}^{4} e_{w_j} e_c} & \\frac{e_{w_4} e_c}{\\sum_{i=0}^{4} e_{w_j} e_c}\\end{bmatrix}$\n",
    "\n",
    "Target one-hot:\n",
    "\n",
    "$o_t = \\begin{bmatrix}0 \\\\ 0 \\\\  0 \\\\  1 \\\\  0\\end{bmatrix}$\n",
    "\n",
    "Maximizing the probability is minimizing the loss:\n",
    "\n",
    "$L(\\Theta) = \\prod_{t = 1}^{T} \\prod_{c \\in C(t)} p(t | c)$\n",
    "\n",
    "where $p(t|c) = \\frac{e_t e_c}{\\sum_{j=1}^{T} e_j e_c}$\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{T} log L(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$o_c$= 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$o_t$= $\\begin{bmatrix} 3\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$E^I=$ $\\begin{bmatrix} 0.337 & 0.129 & 0.234 \\\\ 0.23 & -1.12 & -0.186 \\\\ 2.21 & -0.638 & 0.462 \\\\ 0.267 & 0.535 & 0.809 \\\\ 1.11 & -1.69 & -0.989\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$e_c =$ $\\begin{bmatrix} 0.23 & -1.12 & -0.186\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Similarities of all w= $\\begin{bmatrix} -1.42 & 0.414 & 0.378 & -0.0956 & -0.0969\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output logits: $\\begin{bmatrix} -3.03 & -1.2 & -1.24 & -1.71 & -1.71\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "nll loss: 1.71"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "D = 3 # embedding size\n",
    "V = 5 # vobavulary size\n",
    "\n",
    "# context word, one-hot \n",
    "c = 1\n",
    "o_c = torch.tensor(c)\n",
    "look(\"$o_c$=\", o_c)\n",
    "\n",
    "# target word, one-hot\n",
    "t = 3\n",
    "o_t = torch.tensor(t).unsqueeze(0)\n",
    "look(\"$o_t$=\", o_t)\n",
    "\n",
    "# embeddings for context words\n",
    "E_i = torch.nn.Embedding(num_embeddings=V, embedding_dim=D)\n",
    "look(\"$E^I=$\", E_i.weight)\n",
    "e_c = E_i(o_c) # context word, embedding\n",
    "assert e_c.shape == (D, )\n",
    "look(\"$e_c =$\", e_c)\n",
    "\n",
    "# embeddings for target words\n",
    "E_o = torch.nn.Embedding(num_embeddings=V, embedding_dim=D)\n",
    "\n",
    "# all embeddings of words\n",
    "all_e_w = E_o(torch.tensor([range(V)])).squeeze()\n",
    "\n",
    "# how similar are embedding of all words and context words\n",
    "all_sim_w_with_c = torch.sum(all_e_w * e_c, dim=1)\n",
    "assert all_sim_w_with_c.shape == (V,)\n",
    "look(\"Similarities of all w=\", all_sim_w_with_c)\n",
    "\n",
    "# turn similarities into probabilities and apply log (because of )\n",
    "y_hat = F.log_softmax(all_sim_w_with_c, dim=0).view(1, -1)\n",
    "assert y_hat.shape == (1, V)\n",
    "\n",
    "look(\"Output logits:\", y_hat)\n",
    "\n",
    "loss = F.nll_loss(y_hat, o_t)\n",
    "look(\"nll loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLL Loss: Negative log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "input $\\begin{bmatrix} 0.882 & 0.915 & 0.383 & 0.959 & 0.39 \\\\ 0.601 & 0.257 & 0.794 & 0.941 & 0.133 \\\\ 0.935 & 0.594 & 0.869 & 0.568 & 0.741\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "logit = log softmax input $\\begin{bmatrix} -1.47 & -1.43 & -1.97 & -1.39 & -1.96 \\\\ -1.6 & -1.94 & -1.41 & -1.26 & -2.07 \\\\ -1.43 & -1.77 & -1.49 & -1.79 & -1.62\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "target $\\begin{bmatrix} 1 & 0 & 4\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gather pred by target $\\begin{bmatrix} -1.43 \\\\ -1.6 \\\\ -1.62\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "nll loss 1.55"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework output: 1.551107406616211\n",
      "Manual output   : 1.551107406616211\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "input = torch.rand(3, 5)\n",
    "target = torch.tensor([1, 0, 4])\n",
    "\n",
    "pred = F.log_softmax(input, dim=1)\n",
    "fw_output = F.nll_loss(pred, target)\n",
    "\n",
    "man_output = -torch.mean(torch.gather(pred, dim=1, index=target.view(-1, 1)))\n",
    "\n",
    "look(\"input\", input)\n",
    "look(\"logit = log softmax input\", pred)\n",
    "look(\"target\", target)\n",
    "look(\"gather pred by target\", torch.gather(pred, dim=1, index=target.view(-1, 1)))\n",
    "look(\"nll loss\", fw_output)\n",
    "\n",
    "print(\"Framework output:\", fw_output.item())\n",
    "print(\"Manual output   :\", man_output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative log-likelihood loss\n",
    "\n",
    "Likelihood $$\\prod_{i=1}^{n} \\hat{y_i}^{y_i}(1-\\hat{y_i})^{1 - y_i}$$\n",
    "\n",
    "Log-likelihood $$\\sum_{i=1}^{n} ({y_i}log(\\hat{y_i}) + (1 - y_i)log(1-\\hat{y_i}))$$\n",
    "\n",
    "Minimazing log-likelihood $$L(\\hat{y}, y) = -\\sum_{i=1}^{n} ({y_i}log(\\hat{y_i}) + (1 - y_i)log(1-\\hat{y_i}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$o_c=$ 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$o_t=$ 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$o_ns=$ $\\begin{bmatrix} 0 & 2\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$e_c=$ $\\begin{bmatrix} 0.23 & -1.12 & -0.186\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$e_t=$ $\\begin{bmatrix} 0.282 & 0.0562 & 0.523\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$e_ns=$ $\\begin{bmatrix} 0.958 & 1.32 & 0.817 \\\\ 0.686 & -0.328 & 0.795\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$sim_{c,t}=$ -0.0956"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$sims_{ns,t}=$ $\\begin{bmatrix} 0.771 & 0.59\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\hat{y}=$ $\\begin{bmatrix} -0.0956 & 0.771 & 0.59\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$y=$ $\\begin{bmatrix} 1.0 & 0.0 & 0.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "BCE loss 0.975"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "D = 3 # embedding size\n",
    "V = 5 # vobavulary size\n",
    "\n",
    "# context word, one-hot \n",
    "c = 1\n",
    "o_c = torch.tensor(c)\n",
    "look(\"$o_c=$\", o_c)\n",
    "\n",
    "# target word, one-hot\n",
    "t = 3\n",
    "o_t = torch.tensor(t)\n",
    "look(\"$o_t=$\", o_t)\n",
    "\n",
    "# sampled negative words\n",
    "ns = [0, 2]\n",
    "o_ns = torch.tensor(ns)\n",
    "look(\"$o_ns=$\", o_ns)\n",
    "\n",
    "# embeddings for context words\n",
    "E_i = torch.nn.Embedding(num_embeddings=V, embedding_dim=D)\n",
    "\n",
    "e_c = E_i(o_c) # context word, embedding\n",
    "assert e_c.shape == (D, )\n",
    "look(\"$e_c=$\", e_c)\n",
    "\n",
    "# embeddings for target and negative words\n",
    "E_o = torch.nn.Embedding(num_embeddings=V, embedding_dim=D)\n",
    "\n",
    "# embedding of target word\n",
    "e_t = E_o(o_t)\n",
    "look(\"$e_t=$\", e_t)\n",
    "\n",
    "# all embeddings of negative words\n",
    "e_ns = E_o(o_ns)\n",
    "look(\"$e_ns=$\", e_ns)\n",
    "\n",
    "# how similar are context and target\n",
    "sim_c_t = torch.sum(e_c * e_t)\n",
    "look(\"$sim_{c,t}=$\", sim_c_t)\n",
    "# how similar are negative context words and target\n",
    "sims_ns_t = torch.matmul(e_ns, e_t.unsqueeze(0).T).squeeze()\n",
    "look(\"$sims_{ns,t}=$\", sims_ns_t)\n",
    "\n",
    "# concatenate similarities (first positive, other negative)\n",
    "y_hat = torch.cat([sim_c_t.unsqueeze(0), sims_ns_t])\n",
    "look(\"$\\hat{y}=$\", y_hat)\n",
    "\n",
    "y = torch.zeros(y_hat.shape)\n",
    "y[0] = 1\n",
    "look(\"$y=$\", y)\n",
    "\n",
    "# calculate loss\n",
    "loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "look(\"BCE loss\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
