{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"helper.py\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/demos/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from helper import look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary cross-entropy Loss\n",
    "\n",
    "$L(\\hat{y}, y) = -\\frac{1}{n}\\sum_{i=1}^{n}(y_i log(\\hat{y}_i) + (y_i log(\\hat{y}_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Binary case"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$z=$ $\\begin{bmatrix} 1.54 & -0.293 & -2.18 & 0.568 & -1.08\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\hat{y} = \\sigma(z)=$ $\\begin{bmatrix} 0.824 & 0.427 & 0.102 & 0.638 & 0.253\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$y=$ $\\begin{bmatrix} 0.0 & 1.0 & 1.0 & 0.0 & 1.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Negative log-likelihood** $-(y log(\\hat{y}) + (1 - y) log(1 - \\hat{y}))=$ $\\begin{bmatrix} 1.74 & 0.851 & 2.29 & 1.02 & 1.38\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "&nbsp;&nbsp;&nbsp;&nbsp; 0 $ \\cdot log($ 0.824 $) + (1 - $ 0 $) \\cdot log(1-$ 0.824 $)=-$ 1.74"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "&nbsp;&nbsp;&nbsp;&nbsp; 1 $ \\cdot log($ 0.427 $) + (1 - $ 1 $) \\cdot log(1-$ 0.427 $)=-$ 0.851"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "&nbsp;&nbsp;&nbsp;&nbsp; 1 $ \\cdot log($ 0.102 $) + (1 - $ 1 $) \\cdot log(1-$ 0.102 $)=-$ 2.29"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "&nbsp;&nbsp;&nbsp;&nbsp; 0 $ \\cdot log($ 0.638 $) + (1 - $ 0 $) \\cdot log(1-$ 0.638 $)=-$ 1.02"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "&nbsp;&nbsp;&nbsp;&nbsp; 1 $ \\cdot log($ 0.253 $) + (1 - $ 1 $) \\cdot log(1-$ 0.253 $)=-$ 1.38"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**BCE** $L(\\hat{y}, y)=$ $\\begin{bmatrix} 1.74 & 0.851 & 2.29 & 1.02 & 1.38\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**BCELogit** $L(\\hat{y}, z)=$ $\\begin{bmatrix} 1.74 & 0.851 & 2.29 & 1.02 & 1.38\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Binary setting ##############################################################\n",
    "look(\"# Binary case\")\n",
    "z = torch.randn(5)\n",
    "look(\"$z=$\", z)\n",
    "yhat = torch.sigmoid(z)\n",
    "look(\"$\\hat{y} = \\sigma(z)=$\", yhat)\n",
    "y = torch.Tensor([0, 1, 1, 0, 1])\n",
    "look(\"$y=$\", y)\n",
    "\n",
    "# First compute the negative log likelihoods using the derived formula\n",
    "l = -(y * yhat.log() + (1 - y) * (1 - yhat).log())\n",
    "look(\"**Negative log-likelihood** $-(y log(\\hat{y}) + (1 - y) log(1 - \\hat{y}))=$\", l)\n",
    "for yhat_i, y_i, l_i in zip(yhat, y, l):\n",
    "    look(\"&nbsp;&nbsp;&nbsp;&nbsp;\", y_i.long(), \"$ \\cdot log($\", yhat_i ,\"$) + (1 - $\", y_i.long(), \"$) \\cdot log(1-$\", yhat_i ,\"$)=-$\", l_i)\n",
    "\n",
    "# Observe that BCELoss and BCEWithLogitsLoss can produce the same results\n",
    "l_BCELoss_nored = torch.nn.BCELoss(reduction=\"none\")(yhat, y)\n",
    "l_BCEWithLogitsLoss_nored = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(z, y)\n",
    "look(\"**BCE** $L(\\hat{y}, y)=$\", l_BCELoss_nored)\n",
    "look(\"**BCELogit** $L(\\hat{y}, z)=$\", l_BCEWithLogitsLoss_nored)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Multiclass case"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$z=$ $\\begin{bmatrix} 0.0627 & -0.766 & 1.1 \\\\ 2.76 & 0.175 & -0.932 \\\\ -1.51 & -0.661 & 1.32 \\\\ 0.0371 & -0.285 & -0.133 \\\\ 1.89 & 3.11 & -0.458\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\hat{y} = softmax(z)=$ $\\begin{bmatrix} 0.235 & 0.103 & 0.662 \\\\ 0.909 & 0.0688 & 0.0227 \\\\ 0.0494 & 0.115 & 0.836 \\\\ 0.389 & 0.282 & 0.328 \\\\ 0.223 & 0.755 & 0.0213\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$y=$ $\\begin{bmatrix} 0 & 2 & 1 & 1 & 0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$-log(\\hat{y})=$ $\\begin{bmatrix} 1.45 & 2.28 & 0.412 \\\\ 0.0959 & 2.68 & 3.78 \\\\ 3.01 & 2.16 & 0.179 \\\\ 0.943 & 1.27 & 1.11 \\\\ 1.5 & 0.281 & 3.85\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$-log(\\hat{y})$ masked with $y=$ $\\begin{bmatrix} 1.45 & 3.78 & 2.16 & 1.27 & 1.5\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$-log(softmax(z))=$ $\\begin{bmatrix} 1.45 & 2.28 & 0.412 \\\\ 0.0959 & 2.68 & 3.78 \\\\ 3.01 & 2.16 & 0.179 \\\\ 0.943 & 1.27 & 1.11 \\\\ 1.5 & 0.281 & 3.85\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$-log(softmax(z))$ masked with $y=$ $\\begin{bmatrix} 1.45 & 3.78 & 2.16 & 1.27 & 1.5\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$NLLLoss(log(\\hat{y}), y)$ $\\begin{bmatrix} 1.45 & 3.78 & 2.16 & 1.27 & 1.5\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$CELoss(z, y)$ $\\begin{bmatrix} 1.45 & 3.78 & 2.16 & 1.27 & 1.5\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multiclass setting ##########################################################\n",
    "look(\"# Multiclass case\")\n",
    "z = torch.randn(5, 3)\n",
    "look(\"$z=$\", z)\n",
    "yhat = torch.softmax(z, dim=-1)\n",
    "look(\"$\\hat{y} = softmax(z)=$\", yhat)\n",
    "y = torch.Tensor([0, 2, 1, 1, 0]).long()\n",
    "look(\"$y=$\", y)\n",
    "\n",
    "\n",
    "# First compute the negative log likelihoods using the derived formulat\n",
    "look(\"$-log(\\hat{y})=$\", -yhat.log())\n",
    "look(\"$-log(\\hat{y})$\", \"masked with\", \"$y=$\", -yhat.log()[torch.arange(5), y])\n",
    "\n",
    "look(\"$-log(softmax(z))=$\", -torch.log_softmax(z, dim=-1))\n",
    "look(\"$-log(softmax(z))$\", \"masked with\", \"$y=$\", -torch.log_softmax(z, dim=-1)[torch.arange(5), y])\n",
    "\n",
    "# Observe that NLLLoss and CrossEntropyLoss can produce the same results\n",
    "l2_NLLLoss_nored = torch.nn.NLLLoss(reduction=\"none\")(yhat.log(), y)\n",
    "look(\"$NLLLoss(log(\\hat{y}), y)$\", l2_NLLLoss_nored)\n",
    "l2_CrossEntropyLoss_nored = torch.nn.CrossEntropyLoss(reduction=\"none\")(z, y)\n",
    "look(\"$CELoss(z, y)$\", l2_CrossEntropyLoss_nored)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
