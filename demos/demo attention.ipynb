{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from helper import look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.hidden = None\n",
    "        self.basic_rnn = nn.GRU(self.n_features,\n",
    "                                self.hidden_dim,\n",
    "                                batch_first=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        rnn_out, self.hidden = self.basic_rnn(X)\n",
    "        return rnn_out # N, L, F\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_features = n_features\n",
    "        self.hidden = None\n",
    "        self.basic_rnn = nn.GRU(self.n_features,\n",
    "                                self.hidden_dim,\n",
    "                                batch_first=True)\n",
    "        self.regression = nn.Linear(self.hidden_dim,\n",
    "                                    self.n_features)\n",
    "\n",
    "    def init_hidden(self, hidden_seq):\n",
    "        # We only need the final hidden state\n",
    "        hidden_final = hidden_seq[:, -1:] # N, 1, H\n",
    "        # But we need to make it sequence-first\n",
    "        self.hidden = hidden_final.permute(1, 0, 2) # 1, N, H\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is N, 1, F\n",
    "        batch_first_output, self.hidden = self.basic_rnn(X, self.hidden)\n",
    "        last_output = batch_first_output[:, -1:]\n",
    "        out = self.regression(last_output)\n",
    "        return out.view(-1, 1, self.n_features)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "X $\\begin{bmatrix} \\begin{bmatrix} -1.0 & -1.0 \\\\ -1.0 & 1.0\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Y $\\begin{bmatrix} \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & -1.0\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_seq = torch.FloatTensor([[-1, -1], [-1, 1], [1, 1], [1, -1]]).view(1, 4, 2)\n",
    "source_seq = full_seq[:, :2]\n",
    "target_seq = full_seq[:, 2:]\n",
    "\n",
    "look(\"X\", source_seq)\n",
    "look(\"Y\", target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "values $\\begin{bmatrix} \\begin{bmatrix} 0.0832 & -0.0356 \\\\ 0.311 & -0.526\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(21)\n",
    "encoder = Encoder(n_features=2, hidden_dim=2)\n",
    "hidden_seq = encoder(source_seq)\n",
    "values = hidden_seq # N, L, H values\n",
    "look(\"values\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "keys $\\begin{bmatrix} \\begin{bmatrix} 0.0832 & -0.0356 \\\\ 0.311 & -0.526\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = hidden_seq # N, L, H keys\n",
    "look(\"keys\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "inputs $\\begin{bmatrix} \\begin{bmatrix} -1.0 & 1.0\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "out $\\begin{bmatrix} \\begin{bmatrix} -0.234 & 0.47\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(21)\n",
    "decoder = Decoder(n_features=2, hidden_dim=2)\n",
    "decoder.init_hidden(hidden_seq)\n",
    "inputs = source_seq[:, -1:]\n",
    "out = decoder(inputs)\n",
    "\n",
    "look(\"inputs\", inputs)\n",
    "look(\"out\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "query $\\begin{bmatrix} \\begin{bmatrix} 0.391 & -0.685\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = decoder.hidden.permute(1, 0, 2) # N, 1, H query\n",
    "\n",
    "look(\"query\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "alphas $\\begin{bmatrix} \\begin{bmatrix} 0.5 & 0.5\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_alphas(ks, q):\n",
    "    N, L, H = ks.size()\n",
    "    alphas = torch.ones(N, 1, L).float() * 1/L \n",
    "    return alphas\n",
    "\n",
    "alphas = calc_alphas(keys, query)\n",
    "look(\"alphas\", alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} \\begin{bmatrix} 0.821 & 0.871 & 0.735\\end{bmatrix} & \\begin{bmatrix} 0.555 & 0.669 & 0.306\\end{bmatrix}\\end{bmatrix}$ $\\begin{bmatrix} \\begin{bmatrix} 0.702 & 0.554 & 0.437 & 0.281 \\\\ 0.205 & 0.885 & 0.917 & 0.711 \\\\ 0.508 & 0.023 & 0.277 & 0.156\\end{bmatrix} & \\begin{bmatrix} 0.577 & 0.929 & 0.205 & 0.385 \\\\ 0.0596 & 0.245 & 0.26 & 0.671 \\\\ 0.562 & 0.129 & 0.438 & 0.161\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} \\begin{bmatrix} 1.13 & 1.24 & 1.36 & 0.964\\end{bmatrix} & \\begin{bmatrix} 0.532 & 0.719 & 0.422 & 0.711\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.1279, 1.2421, 1.3610, 0.9643]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlh = torch.rand(2, 3, 4)\n",
    "n1l = torch.rand(2, 1, 3)\n",
    "\n",
    "look(n1l, nlh)\n",
    "look(torch.bmm(n1l, nlh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "context vector $\\begin{bmatrix} \\begin{bmatrix} 0.197 & -0.281\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N, 1, L x N, L, H -> 1, L x L, H -> 1, H\n",
    "context_vector = torch.bmm(alphas, values)\n",
    "look(\"context vector\", context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "concatenated $\\begin{bmatrix} \\begin{bmatrix} 0.197 & -0.281 & 0.391 & -0.685\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concatenated = torch.cat([context_vector, query], axis=-1)\n",
    "look(\"concatenated\", concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "products $\\begin{bmatrix} \\begin{bmatrix} 0.0569 & 0.482\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N, 1, H x N, H, L -> N, 1, L\n",
    "products = torch.bmm(query, keys.permute(0, 2, 1))\n",
    "look(\"products\", products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "alphas $\\begin{bmatrix} \\begin{bmatrix} 0.395 & 0.605\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "calc alphas $\\begin{bmatrix} \\begin{bmatrix} 0.395 & 0.605\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = F.softmax(products, dim=-1)\n",
    "look(\"alphas\", alphas)\n",
    "\n",
    "def calc_alphas(ks, q):\n",
    "    # N, 1, H x N, H, L -> N, 1, L\n",
    "    products = torch.bmm(q, ks.permute(0, 2, 1)) \n",
    "    alphas = F.softmax(products, dim=-1)\n",
    "    return alphas\n",
    "\n",
    "\n",
    "look(\"calc alphas\", calc_alphas(keys, query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "alphas $\\begin{bmatrix} \\begin{bmatrix} 0.425 & 0.575\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "context_vector $\\begin{bmatrix} \\begin{bmatrix} 0.214 & -0.318\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calc_alphas(ks, q):\n",
    "    dims = q.size(-1)\n",
    "    print(dims)\n",
    "    # N, 1, H x N, H, L -> N, 1, L\n",
    "    products = torch.bmm(q, ks.permute(0, 2, 1)) \n",
    "    scaled_products = products / dims ** 0.5\n",
    "    alphas = F.softmax(scaled_products, dim=-1) \n",
    "    return alphas\n",
    "\n",
    "alphas = calc_alphas(keys, query)\n",
    "look(\"alphas\", alphas)\n",
    "context_vector = torch.bmm(alphas, values) \n",
    "look(\"context_vector\", context_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, input_dim=None, proj_values=False):\n",
    "        super().__init__()\n",
    "        self.d_k = hidden_dim\n",
    "        self.input_dim = hidden_dim \\\n",
    "                         if input_dim is None \\\n",
    "                         else \\\n",
    "                         input_dim\n",
    "\n",
    "        self.proj_values = proj_values\n",
    "        # Affine transformations for Q, K, and V\n",
    "        self.linear_query = torch.nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.linear_key = torch.nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.linear_value = torch.nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.alphas = None\n",
    "\n",
    "    def init_keys(self, keys):\n",
    "        self.keys = keys\n",
    "        self.proj_keys = self.linear_key(self.keys)\n",
    "        self.values = self.linear_value(self.keys) \\\n",
    "                      if self.proj_values \\\n",
    "                      else \\\n",
    "                      self.keys\n",
    "\n",
    "    def score_function(self, query):\n",
    "        proj_query = self.linear_query(query)\n",
    "        # scaled dot product\n",
    "        # N, 1, H x N, H, L -> N, 1, L\n",
    "        dot_products = torch.bmm(proj_query,\n",
    "                                 self.proj_keys.permute(0, 2, 1))\n",
    "        scores = dot_products / self.d_k ** 0.5 \n",
    "        return scores\n",
    "\n",
    "    def forward(self, query, mask=None):\n",
    "        # Query is batch-first N, 1, H\n",
    "        scores = self.score_function(query) # N, 1, L 1 \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) \n",
    "        alphas = F.softmax(scores, dim=-1) # N, 1, L 2 \n",
    "        self.alphas = alphas.detach()\n",
    "        # N, 1, L x N, L, H -> N, 1, H\n",
    "        context = torch.bmm(alphas, self.values)\n",
    "        return context        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} 2 & 2 & 1 \\\\ 1 & 0 & 2 \\\\ 2 & 1 & 1\\end{bmatrix}$ $\\begin{bmatrix} 1 & 1 & 2 \\\\ 2 & 0 & 0 \\\\ 1 & 2 & 2\\end{bmatrix}$ $\\begin{bmatrix} 0 & 0 & 2 \\\\ 1 & 0 & 2 \\\\ 2 & 0 & 0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = torch.tensor([1, 0, 0])\n",
    "w2 = torch.tensor([0, 1, 0])\n",
    "w3 = torch.tensor([1, 1, 0])\n",
    "w4 = torch.tensor([0, 0, 1])\n",
    "\n",
    "Wq = torch.randint(3, (3, 3))\n",
    "Wk = torch.randint(3, (3, 3))\n",
    "Wv = torch.randint(3, (3, 3))\n",
    "look(Wq, Wk, Wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} 1 & 5 & 8 & 2 & 6 & 3 & 7 & 4\\end{bmatrix}$ $\\begin{bmatrix} 3 & 2 & 2 & 1\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} 1 & 5 & 8 & 2 & 6 & 3 & 7 & 4\\end{bmatrix}$ $\\begin{bmatrix} 3 & 2 & 2 & 1\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = [len(x_i) for x_i in seq]\n",
    "packed_batch_first = pack_padded_sequence(padded_batch_first, lengths, batch_first=True)\n",
    "packed_batch_second = pack_padded_sequence(padded_batch_second, lengths, batch_first=False)\n",
    "\n",
    "\n",
    "look(packed_batch_first.data, packed_batch_first.batch_sizes)\n",
    "look(packed_batch_second.data, packed_batch_second.batch_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 5 & 6 & 7 & 0 \\\\ 8 & 0 & 0 & 0\\end{bmatrix}$ $\\begin{bmatrix} 4 & 3 & 1\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unpacked_first, batch_first_sizes = pad_packed_sequence(packed_batch_first, batch_first=True)\n",
    "\n",
    "look(unpacked_first, batch_first_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$\\begin{bmatrix} 1 & 5 & 8 \\\\ 2 & 6 & 0 \\\\ 3 & 7 & 0 \\\\ 4 & 0 & 0\\end{bmatrix}$ $\\begin{bmatrix} 4 & 3 & 1\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unpacked_second, batch_second_sizes = pad_packed_sequence(packed_batch_second, batch_first=False)\n",
    "\n",
    "look(unpacked_second, batch_second_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Attention\n",
    "\n",
    "```\n",
    "torch.nn.MultiheadAttention(embed_dim, \n",
    "                            num_heads, \n",
    "                            dropout=0.0, \n",
    "                            bias=True, \n",
    "                            add_bias_kv=False, \n",
    "                            add_zero_attn=False, \n",
    "                            kdim=None, \n",
    "                            vdim=None, \n",
    "                            batch_first=False, \n",
    "                            device=None, \n",
    "                            dtype=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Model parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$W_{in}=$ $\\begin{bmatrix} -0.422 & 0.509 \\\\ 0.763 & -0.635 \\\\ 0.753 & 0.162 \\\\ 0.64 & 0.117 \\\\ 0.418 & -0.122 \\\\ 0.668 & 0.128\\end{bmatrix}$ $b_{in}=$ $\\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$W_{out}=$ $\\begin{bmatrix} 0.541 & 0.587 \\\\ -0.166 & 0.65\\end{bmatrix}$ $b_{out}=$ $\\begin{bmatrix} 0.0 & 0.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$W_q=$ $\\begin{bmatrix} -0.422 & 0.509 \\\\ 0.763 & -0.635\\end{bmatrix}$ $b_q=$ $\\begin{bmatrix} 0.0 & 0.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$W_k=$ $\\begin{bmatrix} 0.753 & 0.162 \\\\ 0.64 & 0.117\\end{bmatrix}$ $b_k=$ $\\begin{bmatrix} 0.0 & 0.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$W_v=$ $\\begin{bmatrix} 0.418 & -0.122 \\\\ 0.668 & 0.128\\end{bmatrix}$ $b_v=$ $\\begin{bmatrix} 0.0 & 0.0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$q=$ $\\begin{bmatrix} 0.267 & 0.627 \\\\ 0.27 & 0.441 \\\\ 0.297 & 0.832\\end{bmatrix}$ $k=$ $\\begin{bmatrix} 0.105 & 0.269 \\\\ 0.359 & 0.199 \\\\ 0.547 & 0.00616\\end{bmatrix}$ $v=$ $\\begin{bmatrix} 0.952 & 0.0753 \\\\ 0.886 & 0.583 \\\\ 0.338 & 0.809\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Process"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "embed_dim = 2\n",
    "mhe = torch.nn.MultiheadAttention(embed_dim=embed_dim, num_heads=1)\n",
    "\n",
    "look(\"## Model parameters\")\n",
    "in_proj_weight = mhe.in_proj_weight.data.detach().clone()\n",
    "in_proj_bias = mhe.in_proj_bias.data.detach().clone()\n",
    "look(\"$W_{in}=$\", in_proj_weight, \"$b_{in}=$\", in_proj_bias)\n",
    "\n",
    "out_proj_weight = mhe.out_proj.weight.detach().clone()\n",
    "out_proj_bias = mhe.out_proj.bias.detach().clone()\n",
    "look(\"$W_{out}=$\", out_proj_weight, \"$b_{out}=$\", out_proj_bias)\n",
    "look(\"<hr>\")\n",
    "\n",
    "Wq, Wk, Wv = torch.chunk(in_proj_weight, 3, dim=0)\n",
    "bq, bk, bv = torch.chunk(in_proj_bias, 3, dim=0)\n",
    "look(\"$W_q=$\", Wq, \"$b_q=$\", bq)\n",
    "look(\"$W_k=$\", Wk, \"$b_k=$\", bk)\n",
    "look(\"$W_v=$\", Wv, \"$b_v=$\", bv)\n",
    "\n",
    "\n",
    "look(\"## Data\")\n",
    "L = 3\n",
    "q = torch.rand(L, embed_dim)\n",
    "k = torch.rand(L, embed_dim)\n",
    "v = torch.rand(L, embed_dim)\n",
    "look(\"$q=$\", q, \"$k=$\", k, \"$v=$\", v)\n",
    "\n",
    "look(\"## Process\")\n",
    "# pq = torch.sum(Wq * q, dim=1) + bq\n",
    "# pk = torch.sum(Wk * k, dim=1) + bk\n",
    "# pv = torch.sum(Wv * v, dim=1) + bv\n",
    "# look(pq, pk, pv)\n",
    "\n",
    "# torch.bmm(pq, self.proj_keys.permute(0, 2, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4518, 0.3147],\n",
       "         [0.4517, 0.3146],\n",
       "         [0.4518, 0.3147]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.3319, 0.3336, 0.3345],\n",
       "         [0.3316, 0.3336, 0.3348],\n",
       "         [0.3320, 0.3336, 0.3344]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhe.forward(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:16:26) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
