{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url_path = \"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a05/\"\n",
    "downloads = {\"surnames_with_splits.csv.bz2\": \"surnames_with_splits.csv\",\n",
    "             \"nlp.py\": None}\n",
    "\n",
    "for download_name, extract_name in downloads.items():\n",
    "    if extract_name and os.path.exists(extract_name):\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(download_name):\n",
    "        import requests\n",
    "        response = requests.get(f\"{url_path}{download_name}\")\n",
    "        with open(download_name, \"wb\") as fp:\n",
    "            fp.write(response.content)\n",
    "        response.close()\n",
    "\n",
    "    if not extract_name:\n",
    "        continue\n",
    "\n",
    "    _, ext = os.path.splitext(download_name)\n",
    "    if ext == \".bz2\":    \n",
    "        import bz2\n",
    "        with open(download_name, 'rb') as bzf, open(extract_name, 'wb') as fp:\n",
    "            fp.write(bz2.decompress(bzf.read()))\n",
    "    elif ext == \".zip\":\n",
    "        from zipfile import ZipFile\n",
    "        with ZipFile(download_name) as zf:\n",
    "            zf.extractall(path=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying Surnames with a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nlp import StepByStep, Vectorizer, Vocabulary\n",
    "from itertools import chain\n",
    "\n",
    "def allclose(a, b, atol=1e-4):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "    return torch.allclose(a, b, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_df = pd.read_csv(\"surnames_with_splits.csv\")\n",
    "\n",
    "train_df = surname_df[surname_df.split == \"train\"]\n",
    "val_df = surname_df[surname_df.split == \"val\"]\n",
    "test_df = surname_df[surname_df.split == \"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "üëç\n",
    "Create variables:\n",
    "* `nationality_vocab` as instance of `Vocabulary` filled with nationalities from `train_df`\n",
    "* `surname_vocab` as instance of `Vocabulary` which is filled with all letters from all surnames in `train_df` and whose special tokens are:\n",
    "    * pad token = `\" \"` (space),\n",
    "    * begin token = `\"<\"`\n",
    "    * end token = `\">\"` \n",
    "    * unknown token = `\"@\"`.\n",
    "* `max_surname_size` is length of the longest surname in `train_df` increases by **2** (since begin and end tokens are added to padded surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test\n",
    "assert max_surname_size == 19\n",
    "assert torch\n",
    "assert allclose(surname_vocab.pad(\"johnson\", max_surname_size), [1, 43, 48, 41, 47, 52, 48, 47, 2, \n",
    "                                                                 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "For X values surnames are padded accordint to `max_surname_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "x_train = surname_vocab.pad_many(train_df.surname, size=max_surname_size)\n",
    "y_train = nationality_vocab.vocabularize(train_df.nationality)\n",
    "\n",
    "x_val = surname_vocab.pad_many(val_df.surname, size=max_surname_size)\n",
    "y_val = nationality_vocab.vocabularize(val_df.nationality)\n",
    "\n",
    "x_test = surname_vocab.pad_many(test_df.surname, size=max_surname_size)\n",
    "y_test = nationality_vocab.vocabularize(test_df.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weights\n",
    "\n",
    "üëç\n",
    "Create variable `class_counts` as dictionary whose keys are countries and values are frequencies of countries in `train_df` dataframe.\n",
    "Order counts by order of their appearance in `nationality_vocab` vocabulary.\n",
    "\n",
    "Create variable `y_weight_tensor` whose value is \n",
    "$$ \\frac{1}{log(freq(nationality))} $$\n",
    "where $ freq(country) $ is frequency of country in `train_df` dataframe.\n",
    "\n",
    "Order weights by order of their appearance in `nationality_vocab` vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test\n",
    "assert class_counts == {'Arabic': 1122, 'Chinese': 154, 'Czech': 289, 'Dutch': 165, 'English': 2080, 'French': 160, 'German': 403, 'Greek': 109, 'Irish': 128, 'Italian': 420, 'Japanese': 542, 'Korean': 53, 'Polish': 84, 'Portuguese': 38, 'Russian': 1661, 'Scottish': 52, 'Spanish': 180, 'Vietnamese': 40}\n",
    "assert y_weight_tensor.shape == (18,)\n",
    "assert allclose(y_weight_tensor[:5], [0.1424, 0.1985, 0.1765, 0.1959, 0.1309])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting frequencies and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Class frequencies\")\n",
    "plt.barh(list(class_counts), class_counts.values())\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Class weights\")\n",
    "plt.barh(list(class_counts), y_weight_tensor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model overview\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Vocabulary\n",
    "\n",
    "Let vocabulary $V$ contain 25 letters a, b, c, ..., z, and 4 special tokens:\n",
    "* `'.'` - pad token\n",
    "* `'<'` - begin of sequence token\n",
    "* `'>'` - end of token sequence\n",
    "* `'*'` - unknown token\n",
    "\n",
    "$V=\\{.,<,>,a,b,c,...,z,*\\}$\n",
    "\n",
    "$v_{size} = |V|=25 + 4 = 29$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Vocabulary(pad_tok=\".\", bgn_tok=\"<\", end_tok=\">\", unk_tok=\"@\")\n",
    "V.fill(\"abcdefghijklmnopqrstuvxyz\")\n",
    "v_size = len(V)\n",
    "\n",
    "print(\"V =\", V)\n",
    "print(\"v_size =\", v_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Vectorization with fixed size\n",
    "\n",
    "Let maximal number of letters in surname $s_{size}$ is 10.\n",
    "\n",
    "$s_{size} = 10$\n",
    "\n",
    "Vectorization of the surname 'adams' with padding is\n",
    "\n",
    "$x = V(adams) = [1, 4, 7, 4, 16, 22, 2, 0, 0, 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname = \"adams\"\n",
    "s_size = 10\n",
    "x = V.pad(surname, size=s_size)\n",
    "\n",
    "print(\"surname =\", surname)\n",
    "print(\"s_size =\", s_size)\n",
    "print(\"x =\", x.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Embedding\n",
    "\n",
    "Let $E$ be embedding layer having shape $v_{size} \\times e_{dim}$ where $e_{dim}$ is 15.\n",
    "\n",
    "Then $e_x = E(x)$ is embedding vector of the surname 'adams' and shape of $e_x$ is $s_{size} \\times e_{dim}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dim = 15\n",
    "E = nn.Embedding(num_embeddings=v_size, embedding_dim=e_dim, padding_idx=0)\n",
    "e_x = E(x)\n",
    "\n",
    "print(\"E shape =\", tuple(E.weight.shape))\n",
    "print(\"e_x shape =\", tuple(e_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: RNN\n",
    "\n",
    "Let RNN has input dimension of $e_{dim}$ and hidden dimension $h_{dim}$ = 4.\n",
    "\n",
    "Forward propagation of $x$ and first hidden vector $h_0 = [0, 0, 0, 0]$ is\n",
    "\n",
    "$h_{all}, h_{s_{size}}=RNN(x, h_0)$\n",
    "\n",
    "where $h_{all}$ contains all activations in time including the last activation $h_{s_{size}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 4\n",
    "h_0 = torch.zeros(1, h_dim) # torch requires that h_0 is in batch\n",
    "R = nn.RNN(input_size=e_dim, hidden_size=4, batch_first=True)\n",
    "h_all, h_s_size = R(e_x, h_0)\n",
    "\n",
    "print(\"h_dim =\", h_dim)\n",
    "print(\"h_0 =\", h_0.tolist())\n",
    "\n",
    "print(\"h_all shape =\", tuple(h_all.shape))\n",
    "print(\"h_s_size shape =\", tuple(h_s_size.shape))\n",
    "print(\"last row in h_all =\", h_all[-1].tolist())\n",
    "print(\"h_s_size =\", h_s_size.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Two Linear layers\n",
    "\n",
    "Let $FC1$ is linear layer of shape $h_{dim} \\times h_{dim}$.\n",
    "\n",
    "Let $c_{dim}$ be an size of an output (number of classes)\n",
    "\n",
    "Let $FC2$ is linear layer of shape $h_{dim} \\times c_{dim}$.\n",
    "\n",
    "Then \n",
    "\n",
    "$\\hat{y} = FC2(ReLU(FC1(h_{s_{size}})))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dim = 3\n",
    "FC1 = nn.Linear(in_features=h_dim, out_features=h_dim)\n",
    "FC2 = nn.Linear(in_features=h_dim, out_features=c_dim)\n",
    "\n",
    "y_hat = FC2(F.relu(FC1(h_s_size.squeeze())))\n",
    "\n",
    "print(\"y_hat shape =\", tuple(y_hat.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: SurnameClassifier\n",
    "\n",
    "üëç Create `SurnameClassifier` class as RNN width following methods:\n",
    "\n",
    "Method `__init__(self, num_emb, emb_size, rnn_hidden_size, num_classes, batch_first=True, padding_idx=0)` initializes classifier with\n",
    "* `num_emb` - number of embeddings is $v_{size}$\n",
    "* `emb_size` - dimension of embedding is $e_{dim}$\n",
    "* `rnn_hidden_size` - dimension of hidden layer of RNN is $h_{dim}$\n",
    "* `num_classes` - dimension of output (number of classes) is $c_{dim}$\n",
    "\n",
    "Crete layers according to Model overview presented in the cells above.\n",
    "\n",
    "Method `forward(self, x, apply_softmax=False)` for forward propagation according to Model overciew presented in the cells above.\n",
    "\n",
    "NOTE: Use dropout with $p=0.5$ for all inputs into each fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_emb, emb_size,\n",
    "                 rnn_hidden_size,\n",
    "                 num_classes, \n",
    "                 batch_first=True, padding_idx=0):\n",
    "                 \n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        return\n",
    "\n",
    "\n",
    "# test\n",
    "StepByStep.set_seed(96)\n",
    "model = SurnameClassifier(num_emb=len(surname_vocab), \n",
    "                          emb_size=100,\n",
    "                          rnn_hidden_size=64,\n",
    "                          num_classes=len(nationality_vocab),\n",
    "                          padding_idx=surname_vocab[surname_vocab.pad_tok], batch_first=False)\n",
    "layers = list(model.children())\n",
    "assert len(layers) == 4\n",
    "assert isinstance(layers[0], nn.Embedding)\n",
    "assert isinstance(layers[1], nn.RNN)\n",
    "assert isinstance(layers[2], nn.Linear)\n",
    "assert isinstance(layers[3], nn.Linear)\n",
    "dataset = SurnameDataset(x_train, y_train)\n",
    "loader = DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "x, y = next(iter(loader))\n",
    "y_hat = model(x)\n",
    "assert y_hat.shape == (3, 18)\n",
    "assert allclose(y_hat.mean(), -0.0136)\n",
    "assert allclose(y_hat.sum(dim=1), [-0.4314,  0.0830, -0.3875])\n",
    "y_hat = model(x, apply_softmax=True)\n",
    "assert y_hat.shape == (3, 18)\n",
    "assert allclose(y_hat.sum(dim=1), [1., 1., 1.])\n",
    "del layers, dataset, loader, x, y, y_hat, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.set_seed(96)\n",
    "\n",
    "model = SurnameClassifier(num_emb=len(surname_vocab), \n",
    "                          emb_size=100,\n",
    "                          rnn_hidden_size=64,\n",
    "                          num_classes=len(nationality_vocab),\n",
    "                          padding_idx=surname_vocab[surname_vocab.pad_tok], batch_first=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(y_weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_loader = DataLoader(SurnameDataset(x_train, y_train), batch_size=64, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(SurnameDataset(x_val, y_val), batch_size=64, drop_last=True, shuffle=True)\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer, scheduler)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "\n",
    "sbs.train_by_loss_change(1e-3)\n",
    "\n",
    "sbs.plot_losses(ylog=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "(sbs.predict(x_test).argmax(dim=1) == y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sbs.model(x_test, apply_softmax=True).argmax(dim=1)\n",
    "\n",
    "labels = list(nationality_vocab)\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "conf_df = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "#conf_df[conf_df==0] = \"\"\n",
    "\n",
    "sns.heatmap(conf_df, annot=True, cbar=None, cmap=\"GnBu\", fmt=\"d\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\"), \n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()\n",
    "#confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, model, surname_vocab, nationality_vocab):\n",
    "    x = surname_vocab.pad(surname, max_surname_size).unsqueeze(0)\n",
    "    y_pred = model(x, apply_softmax=True)\n",
    "    i = y_pred.argmax(dim=1).item()\n",
    "    nationality = nationality_vocab.inv[i]\n",
    "    return nationality\n",
    "\n",
    "predict_nationality(\"Trump\", sbs.model, surname_vocab, nationality_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(surname, model, surname_vocab, nationality_vocab, k=5):\n",
    "    x = surname_vocab.pad(surname, max_surname_size).unsqueeze(0)\n",
    "    y_pred = model(x, apply_softmax=True)\n",
    "\n",
    "    probs, indices = torch.topk(y_pred, k=k)\n",
    "    probs = probs.squeeze().tolist()\n",
    "    indices = indices.squeeze().tolist()\n",
    "\n",
    "    nationalities = {}\n",
    "    print(f\"Top {k} predictions:\")\n",
    "    for i, p in zip(indices, probs):\n",
    "        nationality = nationality_vocab.inv[i]\n",
    "        nationalities[nationality] = p\n",
    "        print(f\"{surname} => {nationality} (p={p:.3f})\")\n",
    "\n",
    "    return nationalities\n",
    "\n",
    "predict_topk_nationality(\"Malkovich\", sbs.model, surname_vocab, nationality_vocab, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
