{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url_path = \"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a05/rnn\"\n",
    "downloads = {\"surnames_with_splits.csv\": None,\n",
    "             \"nlp.py\": None}\n",
    "\n",
    "for download_name, extract_name in downloads.items():\n",
    "    if extract_name and os.path.exists(extract_name):\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(download_name):\n",
    "        import requests\n",
    "        response = requests.get(f\"{url_path}{download_name}\")\n",
    "        with open(download_name, \"wb\") as fp:\n",
    "            fp.write(response.content)\n",
    "        response.close()\n",
    "\n",
    "    if not extract_name:\n",
    "        continue\n",
    "\n",
    "    _, ext = os.path.splitext(download_name)\n",
    "    if ext == \".bz2\":    \n",
    "        import bz2\n",
    "        with open(download_name, 'rb') as bzf, open(extract_name, 'wb') as fp:\n",
    "            fp.write(bz2.decompress(bzf.read()))\n",
    "    elif ext == \".zip\":\n",
    "        from zipfile import ZipFile\n",
    "        with ZipFile(download_name) as zf:\n",
    "            zf.extractall(path=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Surnames with a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from nlp import Vocabulary, StepByStep, allclose, mdprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation\n",
    "\n",
    "This code reads a CSV file named `\"surnames_with_splits.csv\"` into a DataFrame using pandas. It then splits the DataFrame into three subsets based on the value in the `split` column: \n",
    "\n",
    "- `train_df`: Contains rows where the `split` column equals `\"train\"`.\n",
    "- `val_df`: Contains rows where the `split` column equals `\"val\"`.\n",
    "- `test_df`: Contains rows where the `split` column equals `\"test\"`.\n",
    "\n",
    "Finally, it displays the `train_df` DataFrame, which contains the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_df = pd.read_csv(\"surnames_with_splits.csv\")\n",
    "\n",
    "train_df = surname_df[surname_df.split == \"train\"]\n",
    "val_df = surname_df[surname_df.split == \"val\"]\n",
    "test_df = surname_df[surname_df.split == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Maximal Size\n",
    "\n",
    "üëç  \n",
    "In this task, you need to implement a function called `get_max_length` that accepts a sequence (such as a list or array) and returns the length of the longest item in the sequence.\n",
    "- Define the function `get_max_length(sequence)` that takes a sequence as input.\n",
    "- The function should return the length of the longest item in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "def get_max_length(sequence):\n",
    "    return max(len(item) for item in sequence)\n",
    "\n",
    "max_surname_size = get_max_length(train_df.surname)\n",
    "print(max_surname_size)\n",
    "\n",
    "assert max_surname_size == 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Building Vocabulary\n",
    "\n",
    "This code initializes two vocabularies, `surname_vocab` and `nationality_vocab`, using a custom `Vocabulary` class.\n",
    "\n",
    "- `PAD_TOK` is set to a space (`\" \"`) and `UNK_TOK` is set to an \"@\" symbol, which are used as special tokens for padding and unknown words, respectively.\n",
    "- `surname_vocab` is created with these special tokens, and then the vocabulary is populated using the `fill()` method, which is called on the `surname` column from the `train_df` DataFrame.\n",
    "- `nationality_vocab` is created without any special tokens, and its vocabulary is populated with the unique nationalities from `train_df`, sorted by their frequency (from `nationality_counts`).\n",
    "\n",
    "Finally, it prints the contents of both `surname_vocab` and `nationality_vocab` to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname vocab: {' ': 0, '<': 1, '>': 2, '@': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, 'A': 9, 'b': 10, 'u': 11, 'd': 12, 'F': 13, 'k': 14, 'r': 15, 'y': 16, 'S': 17, 'e': 18, 'g': 19, 'C': 20, 'm': 21, 'H': 22, 'i': 23, 'K': 24, 'n': 25, 'W': 26, 's': 27, 'f': 28, 'G': 29, 'M': 30, 'l': 31, 'B': 32, 'z': 33, 'N': 34, 'I': 35, 'w': 36, 'D': 37, 'Q': 38, 'j': 39, 'E': 40, 'R': 41, 'Z': 42, 'c': 43, 'Y': 44, 'J': 45, 'L': 46, 'O': 47, '-': 48, 'P': 49, 'X': 50, 'p': 51, ':': 52, 'v': 53, 'U': 54, '1': 55, 'V': 56, 'x': 57, 'q': 58, '√©': 59, '√â': 60, \"'\": 61, '√ü': 62, '√∂': 63, '√§': 64, '√º': 65, '√∫': 66, '√†': 67, '√≤': 68, '√®': 69, '√≥': 70, '≈ö': 71, 'ƒÖ': 72, '≈Ñ': 73, '√°': 74, '≈º': 75, '√µ': 76, '√≠': 77, '√±': 78, '√Å': 79}\n",
      "Nationality vocab: {'Arabic': 0, 'Chinese': 1, 'Czech': 2, 'Dutch': 3, 'English': 4, 'French': 5, 'German': 6, 'Greek': 7, 'Irish': 8, 'Italian': 9, 'Japanese': 10, 'Korean': 11, 'Polish': 12, 'Portuguese': 13, 'Russian': 14, 'Scottish': 15, 'Spanish': 16, 'Vietnamese': 17}\n"
     ]
    }
   ],
   "source": [
    "PAD_TOK = \" \"\n",
    "BGN_TOK = \"<\"\n",
    "END_TOK = \">\"\n",
    "UNK_TOK = \"@\"\n",
    "\n",
    "surname_vocab = Vocabulary(pad_tok=PAD_TOK, bgn_tok=BGN_TOK, end_tok=END_TOK, unk_tok=UNK_TOK)\n",
    "surname_vocab.fill(train_df.surname)\n",
    "\n",
    "nationality_vocab = Vocabulary()\n",
    "nationality_vocab.fill([sorted(set(train_df.nationality))])\n",
    "\n",
    "print(f\"Surname vocab: {surname_vocab}\")\n",
    "print(f\"Nationality vocab: {nationality_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Surname Vectorization\n",
    "\n",
    "üëç  \n",
    "In this task, you need to implement a function called `vectorize` that converts a sequence (such as a string) into a fixed-size tensor of integers, based on the provided vocabulary. It optionally adds special markers for the beginning and end of the sequence and handles truncation or padding to ensure the output tensor matches the desired size.\n",
    "\n",
    "- Define the function `vectorize(sequence, vocab, max_size, mark_sequence=True)` that takes three inputs:\n",
    "  - `sequence`: A sequence (such as a string) to be vectorized.\n",
    "  - `vocab`: A dictionary mapping items in the sequence to integer indices.\n",
    "  - `max_size`: The maximum size of the output tensor.\n",
    "  - `mark_sequece`: If `True`, the function adds \"begin of sequence\" token at the start and \"end of token\" at the end of the sequence.\n",
    "- The function should return a tensor of size `max_size` (if `mark_sequence` is set to `False` or `max_size + 2` otherwise), where:\n",
    "  - Each position in the tensor corresponds to an index from the `vocab` for the corresponding item in the sequence.\n",
    "  - The sequence is truncated or padded to fit the `max_size`.\n",
    "  - Begin of sequence token and end of sequence token are accessible through vocabulary using `vocab.bgn_tok` and `vocab.end_tok`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Vectorization of 'Doe' with sequence marks is $\\begin{bmatrix} 1 & 37 & 5 & 18 & 2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Vectorization of 'Doe' without sequence marks is $\\begin{bmatrix} 37 & 5 & 18 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vectorize(seq, vocab, max_size, mark_sequence=True):\n",
    "    seq = list(seq)[:max_size]\n",
    "    if mark_sequence:\n",
    "        seq = [vocab.bgn_tok] + seq + [vocab.end_tok]\n",
    "        vec = torch.zeros(max_size + 2, dtype=int)\n",
    "    else:\n",
    "        seq = list(seq)\n",
    "        vec = torch.zeros(max_size, dtype=int)\n",
    "\n",
    "    for i, item in enumerate(seq):\n",
    "        vec[i] = vocab[item]\n",
    "    return vec\n",
    "\n",
    "\n",
    "sn = \"Doe\"\n",
    "max_size = 10\n",
    "# mark_sequence = True\n",
    "sn2vec = vectorize(sn, surname_vocab, max_size)\n",
    "assert isinstance(sn2vec, torch.Tensor) and sn2vec.dtype == torch.int64, \\\n",
    "    f\"vectorize('{sn}', max_size={max_size}) should return tensor containing integers\"\n",
    "assert sn2vec.shape == (max_size + 2,), \\\n",
    "    f\"return tensor of vectorize('{sn}', max_size={max_size}) shuld be of shape {(max_size+2, )}\"\n",
    "mdprint(f\"Vectorization of '{sn}' with sequence marks is\", sn2vec)\n",
    "vec2sn = \"\".join([surname_vocab.inverse[idx.item()] for idx in sn2vec])\n",
    "unsn = (\"<\" + sn + \">\").ljust(max_size + 2, \" \")\n",
    "assert vec2sn == unsn, \\\n",
    "    f\"Unvectorization of returned tensor of vectorize('{sn}', max_size={max_size}) must be '{unsn}'\"\n",
    "\n",
    "# mark_sequence = False\n",
    "sn2vec = vectorize(sn, surname_vocab, max_size, mark_sequence=False)\n",
    "assert isinstance(sn2vec, torch.Tensor) and sn2vec.dtype == torch.int64, \\\n",
    "    f\"vectorize('{sn}', max_size={max_size}) should return tensor containing integers\"\n",
    "assert sn2vec.shape == (max_size,), \\\n",
    "    f\"return tensor of vectorize('{sn}', max_size={max_size}) shuld be of shape {(max_size, )}\"\n",
    "mdprint(f\"Vectorization of '{sn}' without sequence marks is\", sn2vec)\n",
    "vec2sn = \"\".join([surname_vocab.inverse[idx.item()] for idx in sn2vec])\n",
    "unsn = sn.ljust(max_size, \" \")\n",
    "assert vec2sn == unsn, \\\n",
    "    f\"Unvectorization of returned tensor of vectorize('{sn}', max_size={max_size}) must be '{unsn}'\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Datasets\n",
    "\n",
    "This code defines a custom `Dataset` class, `SurnameDataset`, which is used for handling surname and nationality data in a format suitable for machine learning models. Dataset contains datapoints in following form: \n",
    "* x - tensor for padded and marked surname, for example `<Smith>     `\n",
    "* h - tensor for nationality used as context vector\n",
    "* y - tensor for shifted padded and marked surname for one letter, for example `Smith>      `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 5, 6, 7, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 0,\n",
       " tensor([4, 5, 6, 7, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, surname_vocab, nationality_vocab, max_surname_size):\n",
    "        self.data = []\n",
    "        y_pad = torch.tensor([surname_vocab.pad_idx])\n",
    "        for _, row in df.iterrows():\n",
    "            x = vectorize(row.surname, surname_vocab, max_surname_size, mark_sequence=True)\n",
    "            h = nationality_vocab[row.nationality]\n",
    "            y = torch.cat([x[1:], y_pad])\n",
    "\n",
    "            self.data.append((x, h, y))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = SurnameDataset(train_df, surname_vocab, nationality_vocab, max_surname_size)\n",
    "val_dataset = SurnameDataset(val_df, surname_vocab, nationality_vocab, max_surname_size)\n",
    "test_dataset = SurnameDataset(test_df, surname_vocab, nationality_vocab, max_surname_size)\n",
    "\n",
    "train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model\n",
    "\n",
    "## 2.1 Model initialization\n",
    "\n",
    "üëç  \n",
    "In this task, you will implement the `__init__` method of the `SurnameGeneratorRNN` class. This class defines a Recurrent Neural Network (RNN) for surname generation. The model predicts the next character in a surname sequence, given the previous characters and the nationality context. The model includes:\n",
    "* Character embedding for input surnames.\n",
    "* Nationality embedding for providing context.\n",
    "* An RNN layer for sequence modeling.\n",
    "* A fully connected layer for predicting the next character.\n",
    "* Dropout for regularization.\n",
    "\n",
    "\n",
    "#### **1. Input Character Embedding (`self.in_embedding`)**\n",
    "- Create an `nn.Embedding` layer for character inputs:\n",
    "  - **`num_embeddings`**: `len(surname_vocab)` ‚Äî number of unique characters in the vocabulary.\n",
    "  - **`embedding_dim`**: `in_embedding_dim` ‚Äî size of the embedding vector for each character.\n",
    "  - **`padding_idx`**: `surname_vocab.pad_idx` ‚Äî the index used for padding.\n",
    "\n",
    "The embedding layer maps input indices to dense vectors:\n",
    "$$\n",
    "\\mathbf{x}_t \\in \\mathbb{R}^{\\text{in\\_embedding\\_dim}}\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{x}_t$ is the embedding for the $t$-th character in the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Nationality Embedding (`self.h_embedding`)**\n",
    "- Create an `nn.Embedding` layer for nationality inputs:\n",
    "  - **`num_embeddings`**: `len(nationality_vocab)` ‚Äî number of nationalities.\n",
    "  - **`embedding_dim`**: `h_embedding_dim` ‚Äî size of the embedding vector for each nationality.\n",
    "\n",
    "This layer creates a dense representation for nationalities, which is used to initialize the RNN's hidden state:\n",
    "$$\n",
    "\\mathbf{h}_{\\text{init}} \\in \\mathbb{R}^{\\text{h\\_embedding\\_dim}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. RNN Layer (`self.rnn`)**\n",
    "- Use a `torch.nn.GRU` layer to process input sequences:\n",
    "  - **`input_size`**: `in_embedding_dim` ‚Äî size of the input embeddings.\n",
    "  - **`hidden_size`**: `h_embedding_dim` ‚Äî size of the hidden state.\n",
    "  - **`num_layers`**: `rnn_num_layers` ‚Äî number of stacked GRU layers.\n",
    "  - **`batch_first`**: `True` ‚Äî ensures input/output tensors are shaped as `(batch_size, seq_len, features)`.\n",
    "\n",
    "The RNN computes a sequence of hidden states:\n",
    "$$\n",
    "\\mathbf{h}_t = \\text{GRU}(\\mathbf{h}_{t-1}, \\mathbf{x}_t)\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{x}_t$ is the embedding for the $t$-th input character,\n",
    "- $\\mathbf{h}_t$ is the hidden state at time $t$.\n",
    "\n",
    "The output is a tensor of hidden states for all time steps:\n",
    "$$\n",
    "\\mathbf{H} \\in \\mathbb{R}^{\\text{batch\\_size}, \\text{seq\\_len}, \\text{h\\_embedding\\_dim}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Fully Connected Layer (`self.fc`)**\n",
    "- Use an `nn.Linear` layer to predict the next character:\n",
    "  - **`in_features`**: `h_embedding_dim` ‚Äî size of the hidden state.\n",
    "  - **`out_features`**: `len(surname_vocab)` ‚Äî size of the character vocabulary.\n",
    "\n",
    "For each hidden state $\\mathbf{h}_t$, this layer outputs a vector of logits:\n",
    "$$\n",
    "\\mathbf{y}_t = \\text{Linear}(\\mathbf{h}_t) \\quad \\mathbf{y}_t \\in \\mathbb{R}^{\\text{len(surname\\_vocab)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Dropout Layer (`self.dropout`)**\n",
    "- Add an `nn.Dropout` layer with a probability `dropout_p` to regularize the RNN's hidden states:\n",
    "  - Dropout randomly sets some hidden state values to zero during training:\n",
    "  $$ \n",
    "  \\mathbf{h}_t^{\\text{dropout}} = \\text{Dropout}(\\mathbf{h}_t)\n",
    "  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGeneratorRNN(nn.Module):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, in_embedding_dim=32, h_embedding_dim=64, rnn_num_layers=1, dropout_p=0.2):\n",
    "        super(SurnameGeneratorRNN, self).__init__()\n",
    "     \n",
    "        self.in_embedding = nn.Embedding(num_embeddings=len(surname_vocab), \n",
    "                                         embedding_dim=in_embedding_dim, \n",
    "                                         padding_idx=surname_vocab.pad_idx)\n",
    "    \n",
    "        self.h_embedding = nn.Embedding(num_embeddings=len(nationality_vocab), \n",
    "                                        embedding_dim=h_embedding_dim)\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(input_size=in_embedding_dim, \n",
    "                                hidden_size=h_embedding_dim,\n",
    "                                num_layers=rnn_num_layers,\n",
    "                                batch_first=True)\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=h_embedding_dim,\n",
    "                                  out_features=len(surname_vocab))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_p)       \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "model = SurnameGeneratorRNN(surname_vocab, nationality_vocab, in_embedding_dim=64, h_embedding_dim=32, rnn_num_layers=1, dropout_p=0.2)\n",
    "\n",
    "assert model.in_embedding.num_embeddings == len(surname_vocab), \\\n",
    "    f\"Embedding layer should have {len(surname_vocab)} embeddings, but got {model.in_embedding.num_embeddings}\"\n",
    "assert model.in_embedding.embedding_dim == 64, \\\n",
    "    f\"Embedding dimension should be 64, but got {model.in_embedding.embedding_dim}\"\n",
    "\n",
    "assert model.h_embedding.num_embeddings == len(nationality_vocab), \\\n",
    "    f\"Embedding layer should have {len(nationality_vocab)} embeddings, but got {model.h_embedding.num_embeddings}\"\n",
    "assert model.h_embedding.embedding_dim == 32, \\\n",
    "    f\"Embedding dimension should be 32, but got {model.h_embedding.embedding_dim}\"\n",
    "\n",
    "assert model.rnn.input_size == 64, \\\n",
    "    f\"RNN input size should be 64, but got {model.rnn.input_size}\"\n",
    "assert model.rnn.hidden_size == 32, \\\n",
    "    f\"RNN hidden size should be 32, but got {model.rnn.hidden_size}\"\n",
    "\n",
    "assert model.dropout.p == 0.2, \\\n",
    "    f\"Dropout probability should be 0.2, but got {model.dropout.p}\"\n",
    "\n",
    "assert model.fc.in_features == 32, \\\n",
    "    f\"Fully connected layer input size should be 32, but got {model.fc.in_features}\"\n",
    "assert model.fc.out_features == len(surname_vocab), \\\n",
    "    f\"Fully connected layer output size should be {len(surname_vocab)}, but got {model.fc.out_features}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Forward\n",
    "\n",
    "üëç \n",
    "In this task, you will implement the `forward` method of the `SurnameGeneratorRNN` class. This method defines how the model processes input data through its architecture, combining embeddings, an RNN layer, dropout, and a fully connected layer to produce predictions for the next character in a sequence.\n",
    "\n",
    "#### **1. Embedding the Input Sequence **\n",
    "- Use the `self.in_embedding` layer to embed the input sequence of character indices (`x`):\n",
    "  - Input shape: $(\\text{batch\\_size}, \\text{sequence\\_length})$\n",
    "  - Output shape: $(\\text{batch\\_size}, \\text{sequence\\_length}, \\text{in\\_embedding\\_dim})$\n",
    "\n",
    "The embeddings represent the input sequence as dense vectors:\n",
    "$$\n",
    "\\mathbf{x}_{\\text{emb}} = \\text{Embedding}(\\mathbf{x})\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{x}$ is the input sequence,\n",
    "- $\\mathbf{x}_{\\text{emb}}$ is the embedded representation.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Embedding the Nationality Context**\n",
    "- Check if the nationality context `h` is provided:\n",
    "  - If `h` is not `None`, embed it using `self.h_embedding`:\n",
    "    $$ \n",
    "    \\mathbf{h}_{\\text{emb}} = \\text{Embedding}(h)\n",
    "    $$\n",
    "  - Repeat the embedding for the number of RNN layers:\n",
    "    $$ \n",
    "    \\mathbf{h}_{\\text{emb\\_rep}} = \\mathbf{h}_{\\text{emb}} \\times \\text{(num layers)}\n",
    "    $$\n",
    "    Shape: $(\\text{rnn\\_num\\_layers}, \\text{batch\\_size}, \\text{h\\_embedding\\_dim})$\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Passing Data Through the RNN**\n",
    "- Pass the embedded input sequence and the initial hidden state to the RNN:\n",
    "  - Output hidden states for all time steps:\n",
    "    $$ \n",
    "    \\mathbf{y}_{\\text{emb}}, \\mathbf{h}_{\\text{last}} = \\text{RNN}(\\mathbf{x}_{\\text{emb}}, \\mathbf{h}_{\\text{emb}})\n",
    "    $$\n",
    "  - Shapes:\n",
    "    - $\\mathbf{y}_{\\text{emb}}$: $(\\text{batch\\_size}, \\text{sequence\\_length}, \\text{h\\_embedding\\_dim})$\n",
    "    - $\\mathbf{h}_{\\text{last}}$: $(\\text{rnn\\_num\\_layers}, \\text{batch\\_size}, \\text{h\\_embedding\\_dim})$\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Applying Dropout **\n",
    "- Use the `self.dropout` layer to apply dropout to the RNN's output:\n",
    "  $$ \n",
    "  \\mathbf{y}_{\\text{drop}} = \\text{Dropout}(\\mathbf{y}_{\\text{emb}})\n",
    "  $$\n",
    "  Shape: $(\\text{batch\\_size}, \\text{sequence\\_length}, \\text{h\\_embedding\\_dim})$\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Predicting the Next Character**\n",
    "- Pass the dropout output through the fully connected layer:\n",
    "  $$ \n",
    "  \\mathbf{y}_{\\text{hat}} = \\text{Linear}(\\mathbf{y}_{\\text{drop}})\n",
    "  $$\n",
    "  Shape: $(\\text{batch\\_size}, \\text{sequence\\_length}, \\text{len(surname\\_vocab)})$\n",
    "\n",
    "This gives the logits for the next character at each time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, h=None):\n",
    "    # Embedding the input sequence of characters\n",
    "    batch_size, _ = x.shape\n",
    "    x_emb = self.in_embedding(x)\n",
    "\n",
    "    if h is not None:\n",
    "        h_emb = self.h_embedding(h).repeat((self.rnn.num_layers, 1, 1))\n",
    "    else:\n",
    "        h_emb = None\n",
    "    \n",
    "    y_emb, last_hidden = self.rnn(x_emb, h_emb)\n",
    "    \n",
    "    y_drop = self.dropout(y_emb)\n",
    "    y_hat = self.fc(y_drop)\n",
    "    \n",
    "        \n",
    "    return y_hat\n",
    "\n",
    "SurnameGeneratorRNN.forward = forward\n",
    "\n",
    "model = SurnameGeneratorRNN(surname_vocab, nationality_vocab, in_embedding_dim=64, h_embedding_dim=32, rnn_num_layers=2, dropout_p=0.2)\n",
    "\n",
    "# Test input tensor dimensions for the surname \"Smith\"\n",
    "x = vectorize(\"Smith\", surname_vocab, max_surname_size)\n",
    "assert x.shape == (max_surname_size + 2,), f\"Expected input shape: ({max_surname_size + 2},), but got: {x.shape}\"\n",
    "\n",
    "# Ensure that input is correctly unsqueezed to match batch dimension\n",
    "x_batch = x.unsqueeze(0)\n",
    "assert x_batch.shape == (1, max_surname_size + 2), \\\n",
    "    f\"Expected input shape: (1, {max_surname_size + 2}), but got: {x_batch.shape}\"\n",
    "\n",
    "# Test nationality context embedding for a specific nationality (\"American\")\n",
    "h = nationality_vocab['English']  # Example nationality index\n",
    "h_emb = model.h_embedding(torch.tensor([h]))  # Embedding the nationality\n",
    "assert h_emb.shape == (1, model.h_embedding.embedding_dim), \\\n",
    "    f\"Expected h_emb shape: (1, {model.h_embedding.embedding_dim}), but got: {h_emb.shape}\"\n",
    "\n",
    "# Ensure nationality embedding is repeated across the number of RNN layers\n",
    "h_emb_repeated = h_emb.repeat((model.rnn.num_layers, 1, 1))\n",
    "assert h_emb_repeated.shape == (model.rnn.num_layers, 1, model.h_embedding.embedding_dim), \\\n",
    "    f\"Expected h_emb shape: ({model.rnn.num_layers}, 1, {model.h_embedding.embedding_dim}), but got: {h_emb_repeated.shape}\"\n",
    "\n",
    "# Ensure model output has the correct shape\n",
    "y_hat = model(x_batch, h=torch.tensor([h]))\n",
    "assert y_hat.shape == (1, max_surname_size + 2, len(surname_vocab)), \\\n",
    "    f\"Expected output shape: (1, {max_surname_size + 2}, {len(surname_vocab)}), but got: {y_hat.shape}\"\n",
    "\n",
    "# Check that y_hat contains valid numerical values (logits)\n",
    "assert torch.is_tensor(y_hat), \\\n",
    "    f\"Expected y_hat to be a tensor, but got: {type(y_hat)}\"\n",
    "assert y_hat.dtype == torch.float32, \\\n",
    "    f\"Expected output dtype: float32, but got: {y_hat.dtype}\"\n",
    "\n",
    "# Ensure that dropout does not produce NaN values\n",
    "assert not torch.isnan(y_hat).any(), \\\n",
    "    \"Output contains NaN values\"\n",
    "\n",
    "# Check that dropout is applied (not all values should be the same)\n",
    "y_hat_no_dropout = model.fc(model.rnn(model.in_embedding(x_batch))[0][:, -1, :])\n",
    "assert not torch.allclose(y_hat, y_hat_no_dropout), \\\n",
    "    \"Dropout is not being applied (y_hat is identical without dropout)\"\n",
    "\n",
    "# Ensure that the RNN produces hidden states for each time step\n",
    "y_emb, last_hidden = model.rnn(model.in_embedding(x_batch), h_emb_repeated)\n",
    "assert y_emb.shape == (1, max_surname_size + 2, model.h_embedding.embedding_dim), \\\n",
    "    f\"Expected RNN output shape: (1, {max_surname_size}, {model.h_embedding.embedding_dim}), but got: {y_emb.shape}\"\n",
    "\n",
    "# Ensure that the last hidden state has the correct shape\n",
    "assert last_hidden.shape == (model.rnn.num_layers, 1, model.h_embedding.embedding_dim), \\\n",
    "    f\"Expected last_hidden shape: ({model.rnn.num_layers}, 1, {model.h_embedding.embedding_dim}), but got: {last_hidden.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Loss Function\n",
    "\n",
    "üëç \n",
    "In this task, you will implement a custom loss function for training a neural network model designed to predict nationalities from surnames. The function, `batch_nll_loss_fn`, calculates the **batch-wise Negative Log-Likelihood (NLL) loss** between the predicted values and the target values.\n",
    "\n",
    "1. **Batch-wise Loss Calculation**: The function should calculate the NLL loss for each batch element individually, considering the target and predicted probabilities for each class.\n",
    "2. **Softmax and Log Transformation**: The predictions (`y_hat`) will go through a `F.log_softmax` transformation before the NLL loss is applied.\n",
    "3. **Padding Handling**: The function should correctly handle padding indices (i.e., avoid calculating loss for padded tokens).\n",
    "\n",
    "- The function should receive two inputs:\n",
    "  - `y_hat`: The model's output, a tensor of shape `(batch_size, seq_len)` containing propability of the next word for sequence.\n",
    "  - `y`: The ground truth targets, a tensor of shape `(batch_size, seq_len)` containing the target indices for each sequence.\n",
    "\n",
    "- The function should return the **mean loss** across all sequences in the batch.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Log-Softmax Transformation**:\n",
    "   First, the predicted logits ($\\hat{y}$) are passed through a `F.log_softmax` function to convert the raw logits into log probabilities. This transformation is done to enable the calculation of the Negative Log-Likelihood loss, as the loss function expects log-probabilities instead of raw probabilities.\n",
    "\n",
    "   $$ p_{\\hat{y}} = \\text{log\\_softmax}(\\hat{y}) = \\log \\left( \\frac{\\exp(\\hat{y}_i)}{\\sum_{i} \\exp(\\hat{y}_i)} \\right) $$\n",
    "\n",
    "#### 2. **Negative Log-Likelihood (NLL) Loss**:\n",
    "   For each sequence in the batch, the NLL loss is computed between the log probabilities of the predicted tokens and the ground truth target tokens (`y`). The `ignore_index` is used to avoid considering the padded tokens when calculating the loss.\n",
    "\n",
    "   $$ \\mathcal{L}_\\text{NLL} = - \\sum_{i=1}^{T} y_i \\log(p_{\\hat{y}_i}) $$\n",
    "\n",
    "   where:\n",
    "   - $ T $ is the sequence length,\n",
    "   - $ y_i $ is the sequence token at position $ i $,\n",
    "   - $ p_{\\hat{y}_i} $ is the predicted probability for the actual token at position $ i $.\n",
    "\n",
    "#### 3. **Handling Padding**:\n",
    "   The function makes use of the `ignore_index` parameter in `nll_loss` to ignore the padded tokens, ensuring they do not contribute to the overall loss calculation.\n",
    "\n",
    "#### 4. **Final Loss Calculation**:\n",
    "   The losses for each sequence in the batch are computed individually, and then the mean loss across the entire batch is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$\\hat{y}=$ $\\begin{bmatrix} \\begin{bmatrix} 2.46 & 1.265 & 0.2663 & -0.9015 & -0.3897 \\\\ 2.658 & 0.6629 & -1.201 & -0.6137 & 0.7392 \\\\ 1.872 & -0.3575 & -0.5704 & -0.1768 & 1.006\\end{bmatrix} & \\begin{bmatrix} 0.5722 & -0.6895 & -1.77 & -0.4635 & 1.385 \\\\ 0.1051 & -0.2648 & -1.048 & 0.9095 & -0.4891 \\\\ 0.7013 & 0.492 & -0.7662 & -0.6436 & 0.6103\\end{bmatrix}\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$y=$ $\\begin{bmatrix} 1 & 0 & 2 \\\\ 2 & 0 & 2\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$batch_nll_loss(\\hat{y}, y)=$ 2.741"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch_nll_loss_fn(y_hat, y):\n",
    "    log_softmax_y_hat = F.log_softmax(y_hat, dim=-1)\n",
    "    losses = []\n",
    "    for b_y_hat, b_y in zip(log_softmax_y_hat, y):\n",
    "        lv = F.nll_loss(b_y_hat, b_y, ignore_index=surname_vocab.pad_idx)\n",
    "        losses.append(lv)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "StepByStep.set_seed(96)\n",
    "\n",
    "y_hat = torch.randn(2, 3, 5)  # 4 sequences, length 5, 10 possible classes\n",
    "y = torch.randint(0, 5, (2, 3))\n",
    "\n",
    "mdprint(\"$\\hat{y}=$\", y_hat)\n",
    "mdprint(\"$y=$\", y)\n",
    "\n",
    "loss = batch_nll_loss_fn(y_hat, y)\n",
    "assert loss > 0, \"Loss should be greater than 0\"\n",
    "mdprint(\"$batch_nll_loss(\\hat{y}, y)=$\", loss)\n",
    "assert allclose(loss, 2.741, 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n",
    "\n",
    "### 3.1 Training setup\n",
    "\n",
    "The training setup initializes the model, loss function, optimizer, learning rate scheduler, and data loaders with the following parameters:\n",
    "\n",
    "- **Model**: The `SurnameGeneratorRNN` is initialized with an input embedding dimension of 32 and a hidden embedding dimension of 16. It uses a single layer (`rnn_num_layers=1`) for the RNN, and includes a dropout probability of 0.25 (`dropout_p=0.25`) to prevent overfitting. The model is designed to generate surnames by learning from a vocabulary of surnames (`surname_vocab`) and nationalities (`nationality_vocab`).\n",
    "  \n",
    "- **Loss function**: The loss function used is `batch_nll_loss_fn`, a custom negative log-likelihood loss function applied to batches of data. It is designed to compute the loss over a batch, likely for multi-class classification or generative tasks such as surname prediction.\n",
    "  \n",
    "- **Optimizer**: The `RMSprop` optimizer is selected with a learning rate of 0.001. This adaptive learning rate optimizer adjusts the learning rate for each parameter based on the squared gradient, helping to stabilize training especially when dealing with noisy gradients or non-stationary objectives.\n",
    "  \n",
    "- **Learning rate scheduler**: The learning rate scheduler `ReduceLROnPlateau` is used to decrease the learning rate by a factor of 0.5 if the validation loss does not improve after 1 epoch. This helps the model fine-tune the learning process and avoid overshooting the optimal parameters as training progresses.\n",
    "  \n",
    "- **Data loaders**: The `train_loader`, `val_loader`, and `test_loader` are configured with a batch size of 64, shuffling the data to ensure randomness in training. The `drop_last=True` argument ensures that incomplete batches (if any) are discarded. These loaders provide efficient and organized data flow during training, validation, and testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.set_seed(96)\n",
    "\n",
    "model = SurnameGeneratorRNN(surname_vocab, nationality_vocab, \n",
    "                            in_embedding_dim=32, h_embedding_dim=16,\n",
    "                            rnn_num_layers=1, dropout_p=0.25)\n",
    "\n",
    "loss_fn = batch_nll_loss_fn\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, drop_last=True, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116 , train loss: 2.36414, val loss: 2.28044, lr: 1.52587890625e-08, loss change: 7.557868957519531e-05"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer, scheduler)\n",
    "\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train_by_loss_change(1e-4)\n",
    "#sbs.train_by_n_epochs(120)\n",
    "\n",
    "sbs.plot_losses(ylog=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic\n",
      " - Chawa\n",
      " - Magbyer\n",
      " - Qerose\n",
      " - Iskum\n",
      " - Nuiwrouy\n",
      "Chinese\n",
      " - Atte\n",
      " - Du\n",
      " - Adwts\n",
      " - Zarer\n",
      " - Fan\n",
      "Czech\n",
      " - Irimriz\n",
      " - Heavtak\n",
      " - Prardis\n",
      " - Hames\n",
      " - Dupda\n",
      "Dutch\n",
      " - Rirc\n",
      " - Damhthasle\n",
      " - Swapidod\n",
      " - Skiyl\n",
      " - Stasnady\n",
      "English\n",
      " - Hutclerl\n",
      " - Steoig\n",
      " - Horan\n",
      " - Mascher\n",
      " - Jouns\n",
      "French\n",
      " - Mwary\n",
      " - Lanersoan\n",
      " - Sbolnmayson\n",
      " - Iskir\n",
      " - Jean\n",
      "German\n",
      " - Alkus\n",
      " - Snordla\n",
      " - Kelwart\n",
      " - Houstister\n",
      " - Aonnay\n",
      "Greek\n",
      " - Jeralanko\n",
      " - Hosithe\n",
      " - Kyatnetka\n",
      " - Mikiureb\n",
      " - Azrekertages\n",
      "Irish\n",
      " - Nannidm\n",
      " - Dullotovt\n",
      " - Telr\n",
      " - Cint√üor\n",
      " - Merta\n",
      "Italian\n",
      " - Odaenno\n",
      " - Latzeo\n",
      " - Netes\n",
      " - Alpamelyl\n",
      " - Senatra\n",
      "Japanese\n",
      " - Abrosuri\n",
      " - Umatoka\n",
      " - Fusaiun\n",
      " - Jakanu\n",
      " - Ikushao\n",
      "Korean\n",
      " - Yan\n",
      " - Husull\n",
      " - Kam\n",
      " - Pibn\n",
      " - Wago\n",
      "Polish\n",
      " - Bashnure\n",
      " - Jrakit\n",
      " - Howay\n",
      " - Gcjarilyn\n",
      " - Hanathraky\n",
      "Portuguese\n",
      " - Sborien\n",
      " - Molae\n",
      " - Gac\n",
      " - Kacnabis\n",
      " - Andose\n",
      "Russian\n",
      " - Naviertin\n",
      " - Damerta\n",
      " - Sovers\n",
      " - Enzr\n",
      " - Ralak\n",
      "Scottish\n",
      " - Pmuwem\n",
      " - Busken\n",
      " - Laseson\n",
      " - Piislal\n",
      " - Shtory\n",
      "Spanish\n",
      " - Vue\n",
      " - Anettaina\n",
      " - Atelr\n",
      " - Sesos\n",
      " - Heyure\n",
      "Vietnamese\n",
      " - Noch\n",
      " - Jidk\n",
      " - Shr\n",
      " - Strak\n",
      " - Sru\n"
     ]
    }
   ],
   "source": [
    "# StepByStep.set_seed(96)\n",
    "\n",
    "def vectorize_without_startend_toks(seq, vocab):\n",
    "    seq = list(seq)\n",
    "    vec = torch.zeros(len(seq), dtype=int)\n",
    "    for i, item in enumerate(seq):\n",
    "        vec[i] = vocab[item]\n",
    "    return vec\n",
    "\n",
    "def generate_samples_for_nationalities(model, surname_vocab, nationality_vocab, max_surname_size, num_samples=5):\n",
    "\n",
    "    def generate_sample(nationality):\n",
    "        h = torch.tensor([nationality_vocab[nationality]])\n",
    "        sample = surname_vocab.bgn_tok\n",
    "        while sample[-1] not in (surname_vocab.end_tok, surname_vocab.pad_tok) and len(sample) < max_surname_size:\n",
    "            x = vectorize_without_startend_toks(sample, surname_vocab).unsqueeze(0)\n",
    "            y_hat = model(x, h)\n",
    "            probs = F.softmax(y_hat[-1][-1], dim=-1).detach()\n",
    "            next_char_i = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_char = surname_vocab.inverse[next_char_i]\n",
    "            sample += next_char\n",
    "            # if next_char in (surname_vocab.bgn_tok, surname_vocab.end_tok) :\n",
    "            #     break\n",
    "        return sample.strip(surname_vocab.bgn_tok + surname_vocab.end_tok + surname_vocab.pad_tok)\n",
    "    \n",
    "    model.eval()\n",
    "    for nationality in nationality_vocab:\n",
    "        print(nationality)\n",
    "        for _ in range(num_samples):\n",
    "            sample = generate_sample(nationality)\n",
    "            print(f\" - {sample}\")\n",
    "\n",
    "\n",
    "generate_samples_for_nationalities(model, surname_vocab, nationality_vocab, max_surname_size, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
