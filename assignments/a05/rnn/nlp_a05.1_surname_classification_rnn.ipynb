{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url_path = \"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a05/rnn/\"\n",
    "downloads = {\"surnames_with_splits.csv\": None,\n",
    "             \"nlp.py\": None}\n",
    "\n",
    "for download_name, extract_name in downloads.items():\n",
    "    if extract_name and os.path.exists(extract_name):\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(download_name):\n",
    "        import requests\n",
    "        response = requests.get(f\"{url_path}{download_name}\")\n",
    "        with open(download_name, \"wb\") as fp:\n",
    "            fp.write(response.content)\n",
    "        response.close()\n",
    "\n",
    "    if not extract_name:\n",
    "        continue\n",
    "\n",
    "    _, ext = os.path.splitext(download_name)\n",
    "    if ext == \".bz2\":    \n",
    "        import bz2\n",
    "        with open(download_name, 'rb') as bzf, open(extract_name, 'wb') as fp:\n",
    "            fp.write(bz2.decompress(bzf.read()))\n",
    "    elif ext == \".zip\":\n",
    "        from zipfile import ZipFile\n",
    "        with ZipFile(download_name) as zf:\n",
    "            zf.extractall(path=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Surnames with a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from nlp import Vocabulary, StepByStep, allclose, mdprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation\n",
    "\n",
    "This code reads a CSV file named `\"surnames_with_splits.csv\"` into a DataFrame using pandas. It then splits the DataFrame into three subsets based on the value in the `split` column: \n",
    "\n",
    "- `train_df`: Contains rows where the `split` column equals `\"train\"`.\n",
    "- `val_df`: Contains rows where the `split` column equals `\"val\"`.\n",
    "- `test_df`: Contains rows where the `split` column equals `\"test\"`.\n",
    "\n",
    "Finally, it displays the `train_df` DataFrame, which contains the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_df = pd.read_csv(\"surnames_with_splits.csv\")\n",
    "\n",
    "train_df = surname_df[surname_df.split == \"train\"]\n",
    "val_df = surname_df[surname_df.split == \"val\"]\n",
    "test_df = surname_df[surname_df.split == \"test\"]\n",
    "\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Nationality Counts\n",
    "\n",
    "ðŸ‘  \n",
    "In this task, you need to implement a function called `build_counts` that accepts a sequence (such as a list or array) and returns a dictionary with the count of occurrences for each unique item in the sequence.\n",
    "- Define the function `build_counts(sequence)` that takes a sequence as input.\n",
    "- The function should return a dictionary where the keys are the unique items in the sequence, and the values are the counts of each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_counts(sequence):\n",
    "    count = {}\n",
    "    for item in sequence:\n",
    "        count[item] = count.get(item, 0) + 1\n",
    "    return count\n",
    "\n",
    "nationality_counts = build_counts(train_df.nationality)\n",
    "print(nationality_counts)\n",
    "\n",
    "assert len(nationality_counts) == 18\n",
    "assert nationality_counts[\"English\"] == 2080\n",
    "assert nationality_counts[\"Portuguese\"] == 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Maximal Size\n",
    "\n",
    "ðŸ‘  \n",
    "In this task, you need to implement a function called `get_max_length` that accepts a sequence (such as a list or array) and returns the length of the longest item in the sequence.\n",
    "- Define the function `get_max_length(sequence)` that takes a sequence as input.\n",
    "- The function should return the length of the longest item in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(sequence):\n",
    "    return max(len(item) for item in sequence)\n",
    "\n",
    "max_surname_size = get_max_length(train_df.surname)\n",
    "print(max_surname_size)\n",
    "\n",
    "assert max_surname_size == 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Building Vocabulary\n",
    "\n",
    "This code initializes two vocabularies, `surname_vocab` and `nationality_vocab`, using a custom `Vocabulary` class.\n",
    "\n",
    "- `PAD_TOK` is set to a space (`\" \"`) and `UNK_TOK` is set to an \"@\" symbol, which are used as special tokens for padding and unknown words, respectively.\n",
    "- `surname_vocab` is created with these special tokens, and then the vocabulary is populated using the `fill()` method, which is called on the `surname` column from the `train_df` DataFrame.\n",
    "- `nationality_vocab` is created without any special tokens, and its vocabulary is populated with the unique nationalities from `train_df`, sorted by their frequency (from `nationality_counts`).\n",
    "\n",
    "Finally, it prints the contents of both `surname_vocab` and `nationality_vocab` to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOK = \" \"\n",
    "UNK_TOK = \"@\"\n",
    "\n",
    "surname_vocab = Vocabulary(pad_tok=PAD_TOK, unk_tok=UNK_TOK)\n",
    "surname_vocab.fill(train_df.surname)\n",
    "\n",
    "nationality_vocab = Vocabulary()\n",
    "nationality_vocab.fill([sorted(set(train_df.nationality), key=nationality_counts.get, reverse=True)])\n",
    "\n",
    "print(f\"Surname vocab: {surname_vocab}\")\n",
    "print(f\"Nationality vocab: {nationality_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Nationality Weights\n",
    "\n",
    "ðŸ‘  \n",
    "In this task, you need to implement a function called `build_weights` that calculates weights based on the counts of items and their corresponding indices in a vocabulary.\n",
    "\n",
    "- Define the function `build_weights(counts, vocab)` that takes two inputs:\n",
    "  - `counts`: A dictionary containing the counts of each item.\n",
    "  - `vocab`: A dictionary mapping items to their indices.\n",
    "- The function should return a tensor of weights, where each weight is calculated as the inverse of the logarithm of the corresponding count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weights(counts, vocab):\n",
    "    weights = torch.zeros(len(vocab))\n",
    "    for item in sorted(counts, key=vocab.get):\n",
    "        weights[vocab[item]] = counts[item]\n",
    "    weights = 1 / torch.log(weights)\n",
    "    return weights\n",
    "\n",
    "nationality_weights = build_weights(nationality_counts, nationality_vocab)\n",
    "\n",
    "assert allclose(nationality_weights[:5], [0.1309, 0.1349, 0.1424, 0.1588, 0.1656])\n",
    "assert nationality_weights.shape == (18,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Plotting frequencies and weights\n",
    "\n",
    "This code generates two horizontal bar charts using matplotlib to visualize the frequency and weights of nationalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Class frequencies\")\n",
    "plt.barh(list(nationality_vocab), [nationality_counts[nation] for nation in nationality_vocab])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Class weights\")\n",
    "plt.barh(list(nationality_vocab), nationality_weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Surname Vectorization\n",
    "\n",
    "ðŸ‘  \n",
    "In this task, you need to implement a function called `vectorize` that converts a sequence (such as a string) into a fixed-size tensor of integers, based on the provided vocabulary.\n",
    "\n",
    "- Define the function `vectorize(seq, vocab, max_size)` that takes three inputs:\n",
    "  - `seq`: A sequence (such as a string) to be vectorized.\n",
    "  - `vocab`: A dictionary mapping items in the sequence to integer indices.\n",
    "  - `max_size`: The maximum size of the output tensor.\n",
    "- The function should return a tensor of size `max_size`, where:\n",
    "  - Each position in the tensor corresponds to an index from the `vocab` for the corresponding item in the sequence.\n",
    "  - The sequence is truncated or padded to fit the `max_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(seq, vocab, max_size):\n",
    "    seq = list(seq)[:max_size]\n",
    "    vec = torch.zeros(max_size, dtype=int)\n",
    "    for i, item in enumerate(seq):\n",
    "        vec[i] = vocab[item]\n",
    "    return vec\n",
    "\n",
    "\n",
    "sn = \"Johnson\"\n",
    "max_size = 10\n",
    "sn2vec = vectorize(sn, surname_vocab, max_size)\n",
    "assert isinstance(sn2vec, torch.Tensor) and sn2vec.dtype == torch.int64, f\"vectorize('{sn}', max_size={max_size}) should return tensor containing integers\"\n",
    "assert sn2vec.shape == (max_size,), f\"return tensor of vectorize('{sn}', max_size={max_size}) shuld be of shape {(max_size, )}\"\n",
    "\n",
    "\n",
    "mdprint(f\"Vectorization of '{sn}' is\", sn2vec)\n",
    "\n",
    "vec2sn = \"\".join([surname_vocab.inverse[idx.item()] for idx in sn2vec])\n",
    "assert vec2sn == sn.ljust(10, \" \"), f\"Unvectorization of returned tensor of vectorize('{sn}', max_size={max_size}) must be '{sn.ljust(10, ' ')}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Datasets\n",
    "\n",
    "This code defines a custom `Dataset` class, `SurnameDataset`, which is used for handling surname and nationality data in a format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, surname_vocab, nationality_vocab, max_surname_size):\n",
    "        self.data = []\n",
    "        for _, row in df.iterrows():\n",
    "            x = vectorize(row.surname, surname_vocab, max_surname_size)\n",
    "            y = nationality_vocab[row.nationality]\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = SurnameDataset(train_df, surname_vocab, nationality_vocab, max_surname_size)\n",
    "val_dataset = SurnameDataset(val_df, surname_vocab, nationality_vocab, max_surname_size)\n",
    "test_dataset = SurnameDataset(test_df, surname_vocab, nationality_vocab, max_surname_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model\n",
    "\n",
    "## 2.1 Model initialization\n",
    "\n",
    "ðŸ‘  \n",
    "In this task, you are required to implement the `__init__` method for a class `SurnameRNN` that will define the architecture of a Recurrent Neural Network (RNN) for surname classification. The model will predict the nationality based on a given surname.\n",
    "\n",
    "#### 1. **Embedding Layer** `self.embedding`  \n",
    "Initialize an `nn.Embedding` layer:\n",
    "  - **Input size**: `len(surname_vocab)` â€” the size of the vocabulary (number of unique characters).\n",
    "  - **Output size**: `embedding_dim` â€” the size of the embedding vectors (dense representation of characters).\n",
    "  - **Padding index**: `surname_vocab.pad_idx` â€” the index used for padding tokens.\n",
    "  \n",
    "The embedding layer takes a sequence of indices representing characters and outputs their corresponding embedding vectors. Given an input sequence of indices $\\mathbf{x} = [x_1, x_2, ..., x_T]$ where $T$ is the sequence length, the output is a sequence of embedding vectors:\n",
    "$$\n",
    "\\mathbf{E}(\\mathbf{x}) = [\\mathbf{e}_1, \\mathbf{e}_2, ..., \\mathbf{e}_T] \\quad \\text{where} \\quad \\mathbf{e}_t \\in \\mathbb{R}^{\\text{embedding\\_dim}}\n",
    "$$\n",
    "So the shape of the output is $(\\text{batch\\_size}, T, \\text{embedding\\_dim})$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **RNN Layer** `self.rnn`\n",
    "Initialize an `nn.RNN` layer:\n",
    "  - **Input size**: `embedding_dim` â€” each input token is an embedding of size `embedding_dim`.\n",
    "  - **Hidden size**: `hidden_dim` â€” the number of hidden units in the RNN.\n",
    "  - **Batch first**: `True` â€” ensures that the input and output have shape `(batch_size, seq_len, features)`.\n",
    "\n",
    "The RNN takes as input a sequence of embeddings and computes hidden states over time:\n",
    "$$\n",
    "\\mathbf{h}_t = \\text{RNN}(\\mathbf{h}_{t-1}, \\mathbf{x}_t)\n",
    "$$\n",
    "where:\n",
    "  - $\\mathbf{x}_t \\in \\mathbb{R}^{\\text{embedding\\_dim}}$ is the input at time step $t$,\n",
    "  - $\\mathbf{h}_t \\in \\mathbb{R}^{\\text{hidden\\_dim}}$ is the hidden state at time step $t$.\n",
    "\n",
    "After processing the entire sequence, the output tensor shape from the RNN will be:\n",
    "$$\n",
    "  \\mathbf{H} = [\\mathbf{h}_1, \\mathbf{h}_2, ..., \\mathbf{h}_T] \\quad \\text{where} \\quad \\mathbf{H} \\in \\mathbb{R}^{\\text{batch\\_size}, T, \\text{hidden\\_dim}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Dropout Layer** `self.dropout`  \n",
    "Initialize a `torch.nn.Dropout` layer with a probability `dropout_p` for regularization.\n",
    "\n",
    "The dropout operation applies a mask to the hidden states to randomly set some of the elements to zero:\n",
    "$$\n",
    "\\mathbf{h}_{\\text{dropout}} = \\text{Dropout}(\\mathbf{h}) \\quad \\text{with probability} \\, p\n",
    "$$\n",
    "where $\\mathbf{h}$ is the hidden state from the RNN. The shape of the output is the same as $\\mathbf{h}$, i.e., $\\mathbb{R}^{\\text{batch\\_size}, T, \\text{hidden\\_dim}}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Fully Connected (FC) Layer** `self.fc`  \n",
    "Initialize an `nn.Linear` layer with:\n",
    "  - **Input size**: `hidden_dim` â€” the size of the hidden state from the RNN.\n",
    "  - **Output size**: `len(nationality_vocab)` â€” the number of possible nationalities.\n",
    "\n",
    "The fully connected layer takes the final hidden state (after dropout) and produces a set of logits representing the predicted nationalities:\n",
    "$$\n",
    "\\mathbf{y} = \\text{Linear}(\\mathbf{h}_{\\text{dropout}})\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{h}_{\\text{dropout}} \\in \\mathbb{R}^{\\text{batch\\_size}, \\text{hidden\\_dim}}$ is the output from the RNN after dropout,\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{\\text{batch\\_size}, \\text{len(nationality\\_vocab)}}$ is the output logits (predicted nationalities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameRNN(nn.Module):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, embedding_dim=32, hidden_dim=64, dropout_p=0.2):\n",
    "        super(SurnameRNN, self).__init__()\n",
    "     \n",
    "        self.embedding = nn.Embedding(num_embeddings=len(surname_vocab), \n",
    "                                      embedding_dim=embedding_dim, \n",
    "                                      padding_idx=surname_vocab.pad_idx)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_p)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(in_features=hidden_dim,\n",
    "                                  out_features=len(nationality_vocab))\n",
    "        \n",
    "\n",
    "\n",
    "model = SurnameRNN(surname_vocab, nationality_vocab, dropout_p=0.2)\n",
    "\n",
    "assert model.embedding.num_embeddings == len(surname_vocab), \\\n",
    "    f\"Embedding layer should have {len(surname_vocab)} embeddings, but got {model.embedding.num_embeddings}\"\n",
    "assert model.embedding.embedding_dim == 32, \\\n",
    "    f\"Embedding dimension should be 32, but got {model.embedding.embedding_dim}\"\n",
    "\n",
    "assert model.rnn.input_size == 32, \\\n",
    "    f\"RNN input size should be 32, but got {model.rnn.input_size}\"\n",
    "assert model.rnn.hidden_size == 64, \\\n",
    "    f\"RNN hidden size should be 64, but got {model.rnn.hidden_size}\"\n",
    "\n",
    "assert model.dropout.p == 0.2, \\\n",
    "    f\"Dropout probability should be 0.2, but got {model.dropout.p}\"\n",
    "\n",
    "assert model.fc.in_features == 64, \\\n",
    "    f\"Fully connected layer input size should be 64, but got {model.fc.in_features}\"\n",
    "assert model.fc.out_features == len(nationality_vocab), \\\n",
    "    f\"Fully connected layer output size should be {len(nationality_vocab)}, but got {model.fc.out_features}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Forward\n",
    "\n",
    "Implement the `forward` Method for `SurnameRNN`**\n",
    "\n",
    "#### 1. **Embed the Input Sequence**\n",
    "- **Step**: Use the `self.embedding` layer to convert the input sequence `x` into embedding vectors.\n",
    "- **Input**: A tensor `x` with dimensions $(\\text{batch\\_size}, T)$, where $T$ is the sequence length.\n",
    "- **Output**: A tensor `x_emb` with dimensions $(\\text{batch\\_size}, T, \\text{embedding\\_dim})$, representing the embedded input sequence.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\mathbf{E}(\\mathbf{x}) = \\text{embedding}(\\mathbf{x})\n",
    "  $$\n",
    "  where each $x_t$ is mapped to an embedding vector of size `embedding_dim`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Initialize the Hidden State for the RNN**\n",
    "- **Step**: Initialize the hidden state `zero_hidden` as a tensor of zeros. The size should match the batch size and the hidden size of the RNN.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\mathbf{h}_0 = \\text{zeros}(1, \\text{batch\\_size}, \\text{hidden\\_dim})\n",
    "  $$\n",
    "  where $\\mathbf{h}_0$ is the initial hidden state, and $\\text{hidden\\_dim}$ is the number of hidden units in the RNN.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Process the Sequence with the RNN**\n",
    "- **Step**: Pass the embedded sequence `x_emb` through the RNN using the initial hidden state `zero_hidden`.\n",
    "- **Output**: A tensor `rnn_out` with dimensions $(\\text{batch\\_size}, T, \\text{hidden\\_dim})$ representing the hidden states at each time step.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\mathbf{h}_t = \\text{RNN}(\\mathbf{h}_{t-1}, \\mathbf{x}_t)\n",
    "  $$\n",
    "  where each hidden state $\\mathbf{h}_t \\in \\mathbb{R}^{\\text{hidden\\_dim}}$ is updated at each time step.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Extract the Last Hidden State**\n",
    "- **Step**: Extract the last hidden state from the RNN output (`rnn_out`). This will be used for the final prediction.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\mathbf{h}_{\\text{last}} = \\mathbf{h}_T \\quad \\text{(last time step)}\n",
    "  $$\n",
    "  where `h_last` has dimensions $(\\text{batch\\_size}, \\text{hidden\\_dim})$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Apply Dropout**\n",
    "- **Step**: Apply dropout to the last hidden state `h_last` using the `self.dropout` layer to regularize the model.\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\mathbf{h}_{\\text{dropout}} = \\text{Dropout}(\\mathbf{h}_{\\text{last}})\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Fully Connected Layer**\n",
    "- **Step**: Pass the dropout-applied last hidden state `h_dropout` through the fully connected layer (`self.fc`) to get the output logits for the nationalities.\n",
    "- **Output**: A tensor `y_hat` with dimensions $(\\text{batch\\_size}, \\text{len(nationality\\_vocab)})$, representing the predicted nationalities (logits).\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\mathbf{y} = \\text{Linear}(\\mathbf{h}_{\\text{dropout}})\n",
    "  $$\n",
    "  where $\\mathbf{y}$ contains the predicted logits for each nationality.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Return the Output**\n",
    "- **Step**: Return the output tensor `y_hat`, which contains the predicted logits for the nationalities.\n",
    "  \n",
    "---\n",
    "\n",
    "#### Summary of Dimensions:\n",
    "1. **Input tensor `x`**: $(\\text{batch\\_size}, T)$\n",
    "2. **Embedded input `x_emb`**: $(\\text{batch\\_size}, T, \\text{embedding\\_dim})$\n",
    "3. **RNN output `rnn_out`**: $(\\text{batch\\_size}, T, \\text{hidden\\_dim})$\n",
    "4. **Last hidden state `h_last`**: $(\\text{batch\\_size}, \\text{hidden\\_dim})$\n",
    "5. **Dropout-applied `h_dropout`**: $(\\text{batch\\_size}, \\text{hidden\\_dim})$\n",
    "6. **Final output `y_hat`**: $(\\text{batch\\_size}, \\text{len(nationality\\_vocab)})$\n",
    "\n",
    "---\n",
    "\n",
    "This assignment helps you understand how the forward pass flows through the network, from embedding the input, passing it through the RNN, applying dropout, and finally using the fully connected layer for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    # Embedding the input sequence of characters\n",
    "    batch_size, _ = x.shape\n",
    "    x_emb = self.embedding(x)\n",
    "    \n",
    "    zero_hidden = torch.zeros(1, batch_size, self.rnn.hidden_size)\n",
    "    rnn_out, _ = self.rnn(x_emb, zero_hidden)\n",
    "\n",
    "    last_hidden_state = rnn_out[:,-1,:]\n",
    "    \n",
    "    # Fully connected layer\n",
    "    y_hat = self.fc(self.dropout(last_hidden_state))\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "# Test input tensor dimensions\n",
    "x = vectorize(\"Johnson\", surname_vocab, max_surname_size)\n",
    "assert x.shape == (max_surname_size,), f\"Expected input shape: ({max_surname_size},), but got: {x.shape}\"\n",
    "\n",
    "# Ensure that input is correctly unsqueezed to match batch dimension\n",
    "x_batch = x.unsqueeze(0)\n",
    "assert x_batch.shape == (1, max_surname_size), \\\n",
    "f\"Expected input shape: (1, {max_surname_size}), but got: {x_batch.shape}\"\n",
    "\n",
    "# Ensure model output has the correct shape\n",
    "SurnameRNN.forward = forward\n",
    "y_hat = model(x_batch)\n",
    "assert y_hat.shape == (1, len(nationality_vocab)), \\\n",
    "f\"Expected output shape: (1, {len(nationality_vocab)}), but got: {y_hat.shape}\"\n",
    "\n",
    "# Check that y_hat contains valid numerical values (logits)\n",
    "assert torch.is_tensor(y_hat), \\\n",
    "f\"Expected y_hat to be a tensor, but got: {type(y_hat)}\"\n",
    "assert y_hat.dtype == torch.float32, \\\n",
    "f\"Expected output dtype: float32, but got: {y_hat.dtype}\"\n",
    "\n",
    "# Ensure that dropout does not produce NaN values\n",
    "assert not torch.isnan(y_hat).any(), \\\n",
    "    \"Output contains NaN values\"\n",
    "\n",
    "# Check that the dropout layer is being applied (not all values should be the same)\n",
    "y_hat_no_dropout = model.fc(model.rnn(model.embedding(x_batch))[0][:, -1, :])\n",
    "assert not torch.allclose(y_hat, y_hat_no_dropout), \\\n",
    "\"Dropout is not being applied (y_hat is identical without dropout)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n",
    "\n",
    "### 3.1 Training setup\n",
    "\n",
    "The training setup initializes the model, loss function, optimizer, and learning rate scheduler with the following parameters:\n",
    "\n",
    "- **Model**: The `SurnameRNN` is initialized with an embedding dimension of 50 and a hidden state dimension of 64. This defines the size of the character embeddings and the hidden states in the RNN.\n",
    "- **Loss function**: The loss function used is `nn.CrossEntropyLoss`, which computes the categorical cross-entropy loss for multi-class classification, weighted by the `nationality_weights` to account for class imbalances.\n",
    "- **Optimizer**: The `Adam` optimizer is used with a learning rate of 0.001 to update the modelâ€™s parameters during training. This optimizer adapts the learning rate for each parameter based on past gradients.\n",
    "- **Learning rate scheduler**: `ReduceLROnPlateau` is employed to reduce the learning rate by a factor of 0.1 if the validation loss doesn't improve after 1 epoch, helping to fine-tune the model towards convergence.\n",
    "- **Data loaders**: The `train_loader` and `val_loader` are configured with a batch size of 64, shuffling the data for training and dropping the last incomplete batch, ensuring efficient and stable training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = SurnameRNN(surname_vocab, nationality_vocab, embedding_dim=embedding_dim, hidden_dim=hidden_dim)\n",
    "loss_fn = nn.CrossEntropyLoss(nationality_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer, scheduler)\n",
    "\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train_by_loss_change(1e-3)\n",
    "\n",
    "sbs.plot_losses(ylog=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "### 4.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "x_test, y_test = list(test_loader)[0]\n",
    "model.eval()\n",
    "acc = (model(x_test).argmax(dim=1) == y_test).sum() / x_test.shape[0]\n",
    "print(\"Accuracy:\",  acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_pred = F.softmax(model(x_test), dim=1).argmax(dim=1)\n",
    "\n",
    "labels = list(nationality_vocab)\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "conf_df = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "#conf_df[conf_df==0] = \"\"\n",
    "\n",
    "sns.heatmap(conf_df, annot=True, cbar=None, cmap=\"GnBu\", fmt=\"d\", mask=conf_df==0)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()\n",
    "#confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, model, surname_vocab, nationality_vocab):\n",
    "    model.eval()\n",
    "    x = vectorize(surname, surname_vocab, max_surname_size)\n",
    "    y_pred = model(x.unsqueeze(0))\n",
    "    i = y_pred.argmax(dim=1).item()\n",
    "    nationality = nationality_vocab.inverse[i]\n",
    "    return nationality\n",
    "\n",
    "predict_nationality(\"Trump\", model, surname_vocab, nationality_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Top-K Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationality(surname, model, surname_vocab, nationality_vocab, k=5):\n",
    "    model.eval()\n",
    "    x = vectorize(surname, surname_vocab, max_surname_size)\n",
    "    \n",
    "    y_hat = model(x.unsqueeze(0))\n",
    "    y_pred = F.softmax(y_hat, dim=1)\n",
    "\n",
    "    probs, indices = torch.topk(y_pred, k=k)\n",
    "\n",
    "    probs = probs.squeeze().tolist()\n",
    "    indices = indices.squeeze().tolist()\n",
    "\n",
    "    nationalities = {}\n",
    "    print(f\"Top {k} predictions:\")\n",
    "    for i, p in zip(indices, probs):\n",
    "        nationality = nationality_vocab.inverse[i]\n",
    "        nationalities[nationality] = p\n",
    "        print(f\"{surname} => {nationality} (p={p:.3f})\")\n",
    "\n",
    "\n",
    "predict_topk_nationality(\"Trump\", model, surname_vocab, nationality_vocab, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
