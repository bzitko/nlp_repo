{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url_path = \"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a05/\"\n",
    "downloads = {\"surnames_with_splits.csv.bz2\": \"surnames_with_splits.csv\",\n",
    "             \"nlp.py\": None}\n",
    "\n",
    "for download_name, extract_name in downloads.items():\n",
    "    if extract_name and os.path.exists(extract_name):\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(download_name):\n",
    "        import requests\n",
    "        response = requests.get(f\"{url_path}{download_name}\")\n",
    "        with open(download_name, \"wb\") as fp:\n",
    "            fp.write(response.content)\n",
    "        response.close()\n",
    "\n",
    "    if not extract_name:\n",
    "        continue\n",
    "\n",
    "    _, ext = os.path.splitext(download_name)\n",
    "    if ext == \".bz2\":    \n",
    "        import bz2\n",
    "        with open(download_name, 'rb') as bzf, open(extract_name, 'wb') as fp:\n",
    "            fp.write(bz2.decompress(bzf.read()))\n",
    "    elif ext == \".zip\":\n",
    "        from zipfile import ZipFile\n",
    "        with ZipFile(download_name) as zf:\n",
    "            zf.extractall(path=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Surnames with a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nlp import StepByStep, Vocabulary, mdprint\n",
    "from itertools import chain\n",
    "\n",
    "def allclose(a, b, atol=1e-4):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "    return torch.allclose(a, b, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_df = pd.read_csv(\"surnames_with_splits.csv\").drop([\"nationality_index\"], axis=1)\n",
    "\n",
    "train_df = surname_df[surname_df.split == \"train\"]\n",
    "val_df = surname_df[surname_df.split == \"val\"]\n",
    "test_df = surname_df[surname_df.split == \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "üëç\n",
    "Create variables:\n",
    "* `nationality_vocab` as instance of `Vocabulary` filled with nationalities from `train_df`\n",
    "* `surname_vocab` as instance of `Vocabulary` which is filled with all letters from all surnames in `train_df` and whose special tokens are:\n",
    "    * pad_token = `.`,\n",
    "    * begin token = `<`\n",
    "    * end token = `>` \n",
    "    * unknown token = `@`.\n",
    "* `max_surname_size` is length of the longest surname in `train_df` increases by **2** (since begin and end tokens are added to padded surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test\n",
    "assert max_surname_size == 18, f\"your max_surname_size is {max_surname_size}, not 18 \"\n",
    "assert torch\n",
    "assert allclose(surname_vocab.pad(\"johnson\", 10), [1, 43, 48, 41, 47, 52, 48, 47,  2,  0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "Make function `vectorize(split_df, surname_vocab, nationality_vocab, max_surname_size)` where \n",
    "* `split_df` - dataframe containing surname and nationality columns\n",
    "* `surname_vocab` - vocabulary of surnames\n",
    "* `nationality_vocab` - vocabulary of nationalities\n",
    "* `max_surname_size` - maximal length of surname\n",
    "which produces:\n",
    "* `x` - input vector of all surnames (call method `.pad_many` on `surname_vocab`)\n",
    "* `h` - hidden vector of all nationalities (call method `.vocabularize` on `nationality_vocab`)\n",
    "* `y` - is made of `x` by shifting all `x` values one place to the left and filling the empty space with `surname_vocab`'s index of the pad token.\n",
    " \n",
    "and returns triple `(x, h, y)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "\n",
    "def vectorize(split_df, x_vocab, h_vocab, max_x_size):\n",
    "\n",
    "    return \n",
    "\n",
    "# test\n",
    "item = vectorize(train_df[:1], surname_vocab, nationality_vocab, max_surname_size)\n",
    "assert len(item) == 3\n",
    "x, h, y = item\n",
    "assert allclose(x, [[ 1, 27, 48, 53, 34, 41,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
    "assert allclose(h, [0])\n",
    "assert allclose(y, [[ 27, 48, 53, 34, 41,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 0]])\n",
    "del x, h, y, item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, h_train, y_train = vectorize(train_df, surname_vocab, nationality_vocab, max_surname_size)        \n",
    "x_val, h_val, y_val = vectorize(val_df, surname_vocab, nationality_vocab, max_surname_size)        \n",
    "x_test, h_test, y_test = vectorize(test_df, surname_vocab, nationality_vocab, max_surname_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_tensor, h_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.h = h_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.h[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model overview\n",
    "\n",
    "### Vocabularization\n",
    "\n",
    "Surname\n",
    "\n",
    "* Surname vocabulary $|V^\\text{sur}| = v^\\text{sur}_{size}$\n",
    "* Surname $\\text{surname} = \\begin{bmatrix} s_1 & s_2 & ... & s_n \\end{bmatrix}$\n",
    "* Vocabularization of surname $\\vec{x} = V^\\text{sur}(\\text{surname}) = \\begin{bmatrix} V^\\text{sur}(s_1) & V^\\text{sur}(s_2) & ... & V^\\text{sur}(s_n) \\end{bmatrix} = \\begin{bmatrix} x_1 & x_2 & ... & x_n \\end{bmatrix}$\n",
    "\n",
    "Nationality\n",
    "* Nationality vocabulary $|V^\\text{nat}| = v^\\text{nat}_{size}$\n",
    "* Nationality $\\text{nationality}$\n",
    "\n",
    "### Tensorification\n",
    "\n",
    "* Input vector $ \\vec{x} = \\begin{bmatrix} x_1 & x_2 & ... & x_{n-1} & x_n \\end{bmatrix} $\n",
    "+ Output vector $ y = \\begin{bmatrix} x_2 & x_3 & ... & x_{n-1} & 0\\end{bmatrix} $ where 0 at the end is padding index.\n",
    "\n",
    "### Forward\n",
    "\n",
    "* Surname embedding $ E^x$ with embedding dimension $e^x_{dim}$.\n",
    "* Nationality embedding $ E^h $ with embedding dimension $e^h_{dim}$.\n",
    "* Embedding of the input $ \\vec{e_x} = E^x(\\vec{x}) $\n",
    "* Embedding of the hidden $ \\vec{h_x} = E^h(h) $\n",
    "* Forward through RNN $ \\hat{e_y} = GRU(e_x, e_h) $\n",
    "* Forward through FC $ \\hat{y} = FC(e_y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: SurnameGenerator\n",
    "üëç Create `SurnameGenerator` class as RNN width following methods:\n",
    "\n",
    "Method `__init__(self, in_num_emb, in_emb_size, h_num_emb, h_emb_size, rnn_num_layers, batch_first=True, padding_idx=0)` initializes classifier with\n",
    "* `in_num_emb` - number of embeddings is $v^{sur}_{size}$\n",
    "* `in_emb_size` - dimension of embedding is $e^{sur}_{dim}$\n",
    "* `h_num_emb` - number of embeddings is $v^{nat}_{size}$\n",
    "* `h_emb_size` - dimension of embedding is $e^{nat}_{dim}$\n",
    "* `batch_first` - first dimension of RNN output is for batch\n",
    "* `padding_idx` - index of pad token\n",
    "* `dropout_p` - dropout after full connected layer\n",
    "\n",
    "Crete layers according to Model overview presented in the cells above (make Dropout also as layer)\n",
    "\n",
    "Method `forward(self, x, h=None)` for forward propagation according to **Model overview** presented in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGenerator(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_num_emb, in_emb_size,\n",
    "                 h_num_emb, h_emb_size,\n",
    "                 rnn_num_layers=1,\n",
    "                 batch_first=True, padding_idx=0, dropout_p=0.25):\n",
    "                 \n",
    "        super(SurnameGenerator, self).__init__()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        \n",
    "        return \n",
    "\n",
    "# test\n",
    "StepByStep.set_seed(96)\n",
    "model = SurnameGenerator(in_num_emb=len(surname_vocab), in_emb_size=100,\n",
    "                         h_num_emb=len(nationality_vocab), h_emb_size=64,\n",
    "                         rnn_num_layers=2,\n",
    "                         padding_idx=surname_vocab[surname_vocab.pad_tok], batch_first=True,\n",
    "                         dropout_p=0.25)\n",
    "\n",
    "\n",
    "# test init\n",
    "layers = list(map(type, model.children()))\n",
    "assert len(layers) == 5\n",
    "assert layers.count(nn.Embedding) == 2\n",
    "assert layers.count(nn.GRU) == 1\n",
    "assert layers.count(nn.Linear) == 1\n",
    "assert layers.count(nn.Dropout) == 1\n",
    "emb_shapes = {layer.weight.shape for layer in model.children() if isinstance(layer, nn.Embedding)} \n",
    "assert emb_shapes == {(80, 100), (18, 64)}\n",
    "gru = next(layer for layer in model.children() if isinstance(layer, nn.GRU))\n",
    "fc = next(layer for layer in model.children() if isinstance(layer, nn.Linear))\n",
    "drop = next(layer for layer in model.children() if isinstance(layer, nn.Dropout))\n",
    "assert gru.weight_ih_l0.shape == (192, 100)\n",
    "assert gru.weight_hh_l0.shape == (192, 64)\n",
    "assert drop.p == 0.25\n",
    "\n",
    "# test init two layers\n",
    "assert gru.weight_ih_l1.shape == gru.weight_hh_l1.shape == (192, 64)\n",
    "del layers, emb_shapes, gru, fc, drop\n",
    "\n",
    "# test forward\n",
    "dataset = SurnameDataset(x_train, h_train, y_train)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "x, h, _ = next(iter(loader))\n",
    "model.train()\n",
    "y = model(x, h)\n",
    "assert y.shape == (2, 18, 80)\n",
    "assert allclose(y.mean(), -0.0094)\n",
    "# test forward in evaluation mode\n",
    "model.eval()\n",
    "y_eval = model(x, h)\n",
    "assert len((y == y_eval).nonzero()) == 0\n",
    "assert allclose(y_eval.mean(), -0.0070)\n",
    "del dataset, loader, x, h, y, y_eval, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Calculated by negative log loss (NLL). First prediction is softmaxed and logaritmized for each output letter in a surname. Then NLL is used to calculate loss for each letter. Finally the mean of all NLL losses for each letter in a surname is used for finall loss of the whole surname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_nll_loss_fn(y_hat, y):\n",
    "    y_hat = F.log_softmax(y_hat, dim=-1)\n",
    "    losses = []\n",
    "    for b_y_hat, b_y in zip(y_hat, y):\n",
    "        lv = F.nll_loss(b_y_hat, b_y, ignore_index=0)\n",
    "        losses.append(lv)\n",
    "    return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.set_seed(96)\n",
    "\n",
    "model = SurnameGenerator(in_num_emb=len(surname_vocab), in_emb_size=32,\n",
    "                         h_num_emb=len(nationality_vocab), h_emb_size=16,\n",
    "                         rnn_num_layers=1,\n",
    "                         padding_idx=surname_vocab[surname_vocab.pad_tok], batch_first=True,\n",
    "                         dropout_p=0.25)\n",
    "\n",
    "loss_fn = batch_nll_loss_fn\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_loader = DataLoader(SurnameDataset(x_train, h_train, y_train), batch_size=64, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(SurnameDataset(x_val, h_val, y_val), batch_size=64, drop_last=True, shuffle=True)\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer, scheduler)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "\n",
    "sbs.train_by_loss_change(1e-4)\n",
    "\n",
    "sbs.plot_losses(ylog=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.set_seed(96)\n",
    "\n",
    "def generate_samples_for_nationalities(model, surname_vocab, nationality_vocab, max_surname_size, num_samples=5):\n",
    "\n",
    "    def generate_sample(nationality):\n",
    "        h = torch.tensor([nationality_vocab[nationality]])\n",
    "        sample = surname_vocab.bgn_tok\n",
    "        while sample[-1] not in (surname_vocab.end_tok, surname_vocab.pad_tok) and len(sample) < max_surname_size:\n",
    "            x = surname_vocab.vocabularize(sample).unsqueeze(0)\n",
    "            y_hat = model(x, h)\n",
    "            probs = F.softmax(y_hat[-1][-1], dim=-1).detach()\n",
    "            next_char_i = torch.multinomial(probs, num_samples=1).item()\n",
    "            next_char = surname_vocab.inverse[next_char_i]\n",
    "            sample += next_char\n",
    "            if next_char in (surname_vocab.bgn_tok, surname_vocab.end_tok) :\n",
    "                break\n",
    "        return sample.strip(surname_vocab.bgn_tok + surname_vocab.end_tok + surname_vocab.pad_tok)\n",
    "    \n",
    "    model.eval()\n",
    "    for nationality in nationality_vocab:\n",
    "        print(nationality)\n",
    "        for _ in range(num_samples):\n",
    "            sample = generate_sample(nationality)\n",
    "            print(f\" - {sample}\")\n",
    "\n",
    "generate_samples_for_nationalities(model, surname_vocab, nationality_vocab, max_surname_size, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
