{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION: Surname generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"surnames_with_splits.csv.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a05/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "        \n",
    "name = \"surnames_with_splits.csv\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    surname_csv=\"surnames_with_splits.csv\",\n",
    "    model_filename=\"model.pth\",\n",
    "    # Model hyper parameter\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    seed=1337,\n",
    "    early_stop=10,\n",
    "    # Runtime hyper parameter\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>Totah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>Abboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>Fakhoury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>Srour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>Sayegh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>test</td>\n",
       "      <td>Dinh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>test</td>\n",
       "      <td>Phung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>test</td>\n",
       "      <td>Quang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>test</td>\n",
       "      <td>Vu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>test</td>\n",
       "      <td>Ha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10980 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  split   surname\n",
       "0          Arabic  train     Totah\n",
       "1          Arabic  train    Abboud\n",
       "2          Arabic  train  Fakhoury\n",
       "3          Arabic  train     Srour\n",
       "4          Arabic  train    Sayegh\n",
       "...           ...    ...       ...\n",
       "10975  Vietnamese   test      Dinh\n",
       "10976  Vietnamese   test     Phung\n",
       "10977  Vietnamese   test     Quang\n",
       "10978  Vietnamese   test        Vu\n",
       "10979  Vietnamese   test        Ha\n",
       "\n",
       "[10980 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(args.surname_csv).drop([\"nationality_index\"], axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Generalized vocabulary can have:\n",
    "* padding token - to fill up empty space\n",
    "* unknown token - token for out-of-vocabulary tokens\n",
    "* begin sequence - token for start of a sequence\n",
    "* end sequence - token for end of a sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, tokens=None, pad_token=None, unk_token=None, begin_seq_token=None, end_seq_token=None):\n",
    "        self._tok2idx = {}\n",
    "        self._idx2tok = {}\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_idx = None\n",
    "        if pad_token is not None:\n",
    "            self.pad_idx = self.add_token(pad_token)\n",
    "        \n",
    "        self.unk_token = unk_token\n",
    "        self.unk_idx = None\n",
    "        if unk_token is not None:\n",
    "            self.unk_idx = self.add_token(unk_token)\n",
    "\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.begin_seq_idx = None\n",
    "        if begin_seq_token is not None:\n",
    "            self.begin_seq_idx = self.add_token(begin_seq_token)\n",
    "\n",
    "        self.end_seq_token = end_seq_token\n",
    "        self.end_seq_idx = None\n",
    "        if end_seq_token is not None:\n",
    "            self.end_seq_idx = self.add_token(end_seq_token)\n",
    "\n",
    "        if tokens is not None:\n",
    "            self.add_tokens(tokens)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self._tok2idx:\n",
    "            idx = len(self._tok2idx)\n",
    "            self._tok2idx[token] = idx\n",
    "            self._idx2tok[idx] = token\n",
    "            return idx\n",
    "        return self._tok2idx[token]\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def ordered_indices(self):\n",
    "        return sorted(self._idx2tok)\n",
    "\n",
    "    def ordered_tokens(self):\n",
    "        for i in sorted(self._idx2tok):\n",
    "            yield self._idx2tok[i]\n",
    "\n",
    "    def __getitem__(self, token_or_idx):\n",
    "        if isinstance(token_or_idx, str):\n",
    "            return self._tok2idx.get(token_or_idx, self.unk_idx)\n",
    "        if isinstance(token_or_idx, int):\n",
    "            return self._idx2tok.get(token_or_idx, self.unk_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tok2idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in sorted(self._idx2tok):\n",
    "            yield self._idx2tok[i]\n",
    "\n",
    "    def info(self):\n",
    "        txt = f\"Vocabulary size:{len(self)}\"\n",
    "        for i in range(min(4, len(self))):\n",
    "            txt += f\" {self[i]}:{i}\"\n",
    "        txt += \" ...\"\n",
    "        print(txt)\n",
    "\n",
    "chars = {ch for surname in df[df.split == \"train\"].surname for ch in surname}\n",
    "surname_vocab = Vocab(sorted(chars), pad_token=\".\", unk_token=\"@\", begin_seq_token=\"<\", end_seq_token=\">\")\n",
    "nationality_vocab = Vocab(sorted(df.nationality))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "\n",
    "* `vectorizer(tokens)` should return long tensor (vector). Vector values corresponds to tokens. Vector should be filled with padding indexes to satisfy vector maximal size.  \n",
    "\n",
    "* üëç  method `vectorize(tokens, seq=True)` receives \n",
    "    * `tokens` - a list of vocabulary entities, and\n",
    "    * `seq` - if set to true, then resulting vector represents a sequence.\n",
    "\n",
    "Let 0 is padding index, 2 is begin of sequence index and 3 is end of sequence index and maximal size is 10. Then for tokens whose indices are, for example, 56, 96 41, a resulting vector should be `[2 56 96 41 3 0 0 0 0]`.  \n",
    "If `seq` is set to false, resulting vector should be `[56 96 41 0 0 0 0 0 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "\n",
    "    def __init__(self, vocabulary, max_size=-1):\n",
    "        self.vocab = vocabulary\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def vectorize(self, tokens, seq=True):\n",
    "        indices = [self.vocab[tok] for tok in tokens]\n",
    "        if seq:\n",
    "            indices = [self.vocab.begin_seq_idx] + indices + [self.vocab.end_seq_idx]\n",
    "            \n",
    "        if self.max_size >= 0:\n",
    "            indices = indices[:self.max_size]\n",
    "            indices += [self.vocab.pad_idx] * (self.max_size - len(indices))\n",
    "        return torch.LongTensor(indices)    \n",
    "\n",
    "\n",
    "surname_max_size = max(len(surname) for surname in df.surname)\n",
    "surname_vectorizer = Vectorizer(surname_vocab, surname_max_size + 2)\n",
    "nationality_vectorizer = Vectorizer(nationality_vocab)\n",
    "\n",
    "assert nationality_vectorizer.vectorize([\"English\"], seq=False).tolist() == [4]\n",
    "assert surname_vectorizer.vectorize(\"johnson\", seq=True).tolist() == [ 2, 43, 48, 41, 47, 52, 48, 47,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "`SurnameDataset` class inherits `torch.utils.data.Dataset`.  \n",
    "Implemented methods are:\n",
    "* `__init__(df, vectorizer_x, vectorizer_h, nationalities)` initialization receives dataframe `df`, `vectorizer_x` vectorizer for surnames, and `vectorizer_h` for nationalities.\n",
    "* `set_split()` for setting current data split\n",
    "* üëç `__getitem__(idx)` should return triple of vectors: x, y, h where \n",
    "    * x is vectorized surname, for example `[2 56 96 41 3 0 0 0 0]`\n",
    "    * y is x moved to left, for example `[56 96 41 3 0 0 0 0 0]`\n",
    "    * h is vector for nationality\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, vectorizer_x, vectorizer_h, nationalities=None):\n",
    "        if nationalities is None:\n",
    "            self.df = df\n",
    "        elif isinstance(nationalities, str):\n",
    "            self.df = df[df.nationality == nationalities]\n",
    "        else:\n",
    "            self.df = df[df.nationality.isin(nationalities)]\n",
    "        self.vectorizer_x = vectorizer_x\n",
    "        self.vectorizer_h = vectorizer_h\n",
    "        self._lookup = {split: self.df[self.df.split == split] for split in set(self.df.split)}\n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self._target_split = split\n",
    "        self._target_df = self._lookup[split]\n",
    "\n",
    "    def vectorize_x(self, surname):\n",
    "        return self.vectorizer_x.vectorize(surname, seq=True)\n",
    "\n",
    "    def vectorize_y(self, nationality):\n",
    "        return self.vectorizer_h.vectorize([nationality], seq=False).squeeze()\n",
    "\n",
    "    def vectorize(self, surname, nationality):\n",
    "        indices_x = self.vectorizer_x.vectorize(surname, seq=True)\n",
    "        indices_y = torch.cat([indices_x[1:], torch.zeros(1, dtype=torch.long)])\n",
    "        index_h = self.vectorizer_h.vectorize([nationality], seq=False)\n",
    "        return indices_x, indices_y, index_h\n",
    "\n",
    "    def frequency_x(self):\n",
    "        return torch.tensor([len(self.df[self.df.surname==tok]) for tok in self.vectorizer_x.vocab.ordered_tokens()])\n",
    "\n",
    "    def frequency_y(self):\n",
    "        return torch.tensor([len(self.df[self.df.nationality==tok]) for tok in self.vectorizer_h.vocab.ordered_tokens()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        datapoint = self._target_df.iloc[idx]\n",
    "        return self.vectorize(datapoint.surname, datapoint.nationality)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True):\n",
    "    for x1, x2, y in torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle):\n",
    "        yield x1.to(args.device), x2.to(args.device), y.to(args.device)\n",
    "\n",
    "dataset = SurnameDataset(df, surname_vectorizer, nationality_vectorizer)\n",
    "\n",
    "assert len(dataset) == 7680\n",
    "assert len(dataset[0]) == 3\n",
    "\n",
    "dataset = SurnameDataset(df, surname_vectorizer, nationality_vectorizer, nationalities=\"English\")\n",
    "assert len(dataset) == 2080\n",
    "assert len(dataset[0]) == 3\n",
    "\n",
    "dataset = SurnameDataset(df, surname_vectorizer, nationality_vectorizer, nationalities=\"Russian\")\n",
    "assert len(dataset) == 1661\n",
    "assert len(dataset[0]) == 3\n",
    "\n",
    "dataset = SurnameDataset(df, surname_vectorizer, nationality_vectorizer, nationalities=[\"English\", \"Russian\"])\n",
    "assert len(dataset) == 3741\n",
    "assert len(dataset[0]) == 3\n",
    "\n",
    "\n",
    "dataset = SurnameDataset(df, surname_vectorizer, nationality_vectorizer)\n",
    "x, y, h = dataset[0]\n",
    "assert x.shape == y.shape\n",
    "assert x.tolist() == [2, 27, 48, 53, 34, 41, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "assert y.tolist()[:-1] == x.tolist()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "üëç  \n",
    "`SurnameGenerator` initialization receives \n",
    "* `embedding_size` dimension of embedding vector (for surnames)\n",
    "* `num_embeddings` size of surname vocabulary\n",
    "* `rnn_hidden_size` dimension of hidden RNN layer\n",
    "* `num_rnn_hidden_embedding` size of nationality vocabulary\n",
    "* `dropout_p` probability of dropout\n",
    "\n",
    "Model will consist of: \n",
    "* $E_s$ - embedding layer for surnames, \n",
    "* $E_n$ - embedding layer for nationalities,\n",
    "* GRU - gated reccurent unit\n",
    "* FC - fully connected layer with dropout\n",
    "\n",
    "Forward receives \n",
    "* $x$ indicies of surnames\n",
    "* $h$ indicies of nationalityes\n",
    "\n",
    "then $\\hat{y} = FC(GRU(E_s(x), E_n(h)))$.\n",
    "\n",
    "Apply softmax if `apply_softmax` is set to true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameGenerator(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, rnn_hidden_size, \n",
    "                 num_rnn_hidden_embedding=None, batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        super(SurnameGenerator, self).__init__()\n",
    "\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                      embedding_dim=embedding_size,\n",
    "                                      padding_idx=padding_idx)\n",
    "\n",
    "        if num_rnn_hidden_embedding:\n",
    "            self.hidden_emb = torch.nn.Embedding(num_embeddings=num_rnn_hidden_embedding,\n",
    "                                                 embedding_dim=embedding_size)\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(input_size=embedding_size, \n",
    "                                hidden_size=rnn_hidden_size,\n",
    "                                batch_first=batch_first)\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=rnn_hidden_size, \n",
    "                                  out_features=num_embeddings)\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "\n",
    "    def forward(self, x, h=None, apply_softmax=False):\n",
    "        x_emb = self.emb(x)\n",
    "        h_emb = self.hidden_emb(h).permute(1, 0, 2) if h is not None else None\n",
    "        \n",
    "        y_hat, _ = self.rnn(x_emb, h_emb)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_hat.shape\n",
    "        #y_hat = y_hat.contiguous().view(batch_size * seq_size, feat_size)        \n",
    "\n",
    "        y_hat = self.fc(F.dropout(y_hat, p=self.dropout_p))\n",
    "\n",
    "        if apply_softmax:\n",
    "            return F.softmax(y_hat, dim=-1)\n",
    "    \n",
    "        #new_feat_size = y_hat.shape[-1]\n",
    "        #y_hat = y_hat.view(batch_size, seq_size, new_feat_size)\n",
    "\n",
    "        #return y_hat\n",
    "        return y_hat#.log_softmax(dim=-1)\n",
    "        \n",
    "\n",
    "generator = SurnameGenerator(embedding_size=args.char_embedding_size, \n",
    "                             num_embeddings=len(dataset.vectorizer_x.vocab), \n",
    "                             rnn_hidden_size=args.rnn_hidden_size,\n",
    "                             num_rnn_hidden_embedding=len(dataset.vectorizer_h.vocab),\n",
    "                             batch_first=True,\n",
    "                             padding_idx=dataset.vectorizer_x.vocab.pad_idx)\n",
    "\n",
    "x1, x2, h = next(generate_batches(dataset, batch_size=3))\n",
    "y_hat = generator(x1)\n",
    "assert y_hat.shape == (3, 19, 80)\n",
    "\n",
    "y_hat = generator(x1, h)\n",
    "assert y_hat.shape == (3, 19, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def compute_accuracy(y_hat, y):\n",
    "    _, y_hat_indices = y_hat.max(dim=1)\n",
    "    n_correct = torch.eq(y_hat_indices, y).sum().item()\n",
    "    return n_correct / len(y_hat_indices) * 100\n",
    "\n",
    "# early stopping\n",
    "def early_stop(train_state, model):\n",
    "    val_loss = train_state[\"val_loss\"]\n",
    "    if len(val_loss) < 2:\n",
    "        torch.save(model.state_dict(), args.model_filename)\n",
    "        return False\n",
    "    \n",
    "    if val_loss[-1] < val_loss[-2]:\n",
    "        torch.save(model.state_dict(), args.model_filename)\n",
    "    \n",
    "    if len(val_loss) >= args.early_stop:\n",
    "        val_loss =  val_loss[-args.early_stop:]\n",
    "        return all(val_loss[i] < val_loss[i + 1] \n",
    "                   for i in range(args.early_stop - 1))\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining loss function\n",
    "\n",
    "For \n",
    "* $N$ - batch size\n",
    "* $C$ - sequence size\n",
    "* $V$ - vocabulary size\n",
    "\n",
    "let $\\hat{y}$ be a prediction tensor of shape $N \\times C \\times V$ and $y$ be a target tensor of shape $N \\times C$.  \n",
    "Function `compute_loss(y_hat, y)` is responsible for computing negative log-likelihood loss for each datapoint in the batch.\n",
    "\n",
    "Before applying pyTorch's NLLLoss, each sequence in the batch $\\hat{y}$ has to be turned into log of probabilities, i.e. $log(softmax(\\hat{y}_i))$ for $i=1...N$. After calculating all $N$ losses by $NLLLoss(log(softmax(\\hat{y}_i)), y_i)$ `compute_loss()` returns their mean.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "def compute_loss(y_hat, y):\n",
    "    y_hat = F.log_softmax(y_hat, dim=-1)\n",
    "    losses = []\n",
    "    for b_y_hat, b_y in zip(y_hat, y):\n",
    "        lv = loss_func(b_y_hat, b_y)\n",
    "        losses.append(lv)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "batch_size = 3\n",
    "seq_size = 2\n",
    "vocab_size = 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "y_hat = torch.rand(batch_size, seq_size, vocab_size)\n",
    "y = torch.tensor([[0, 1], [2, 1], [3, 0]])\n",
    "loss = compute_loss(y_hat, y)\n",
    "assert torch.all(loss == torch.tensor(1.33540785))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5c8b945ffe43a1802899bca1ce5512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4048b9cce5bc41ceb969748604679a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2517b770e20d4889b47bdb8eb2055383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting loop\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_hat, y):\n",
    "    _, y_hat_indices = y_hat.max(dim=-1)\n",
    "    y_hat_indices = y_hat_indices.ravel()\n",
    "    y = y.ravel()\n",
    "    n_correct = torch.eq(y_hat_indices, y).sum().item()\n",
    "    return n_correct / len(y_hat_indices) * 100    \n",
    "\n",
    "# generator\n",
    "\n",
    "generator = SurnameGenerator(embedding_size=args.char_embedding_size, \n",
    "                             num_embeddings=len(dataset.vectorizer_x.vocab), \n",
    "                             rnn_hidden_size=args.rnn_hidden_size,\n",
    "                             num_rnn_hidden_embedding=len(dataset.vectorizer_h.vocab),\n",
    "                             batch_first=True,\n",
    "                             padding_idx=dataset.vectorizer_x.vocab.pad_idx)\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "# loss, optimizer, scheduler\n",
    "loss_func = torch.nn.NLLLoss(ignore_index=dataset.vectorizer_x.vocab.pad_idx)\n",
    "optimizer = torch.optim.Adam(generator.parameters(), lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# progress bars\n",
    "epoch_bar = tqdm(desc='epochs', total=args.num_epochs, position=0)\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "# train state tracker\n",
    "train_state = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": [],}\n",
    "\n",
    "\n",
    "generator = generator.to(args.device)\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = running_acc = 0.0\n",
    "        \n",
    "        generator.train()\n",
    "        for batch_index, (x, y, h) in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = generator(x, h)\n",
    "            loss = compute_loss(y_hat, y)\n",
    "            # loss = loss_func(y_hat, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_t = compute_accuracy(y_hat, y)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)        \n",
    "\n",
    "        # Iterate over val dataset\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = running_acc = 0.0\n",
    "        \n",
    "        generator.eval()\n",
    "        for batch_index, (x, y, h) in enumerate(batch_generator):\n",
    "            y_hat =  generator(x, h)\n",
    "\n",
    "            loss = compute_loss(y_hat, y)\n",
    "            # loss = loss_func(y_hat, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_hat, y)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)   \n",
    "\n",
    "        if early_stop(train_state, generator):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff4c3499490>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_state[\"val_loss\"])\n",
    "plt.plot(train_state[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 8.2533\n",
      "Test Accuracy: 11.3492\n"
     ]
    }
   ],
   "source": [
    "generator.load_state_dict(torch.load(args.model_filename))\n",
    "\n",
    "generator = generator.to(args.device)\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "generator.eval()\n",
    "for batch_index, (x, y, h) in enumerate(batch_generator):\n",
    "    y_hat =  generator(x, h)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = compute_loss(y_hat, y)\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_hat, y)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "print(f\"Test loss: {running_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {running_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Function `sample_from_model(model, vectorizer, num_samples=10, nationality_idx=None)` must generate `num_samples` surnames. If `nationality_idx` is set to some nationality index, then generated surnames belong to specific nationality. Nationality is represented as first hidden input $h_0$ to GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic\n",
      " - Schhoz\n",
      " - Diamem\n",
      " - Aklrrilha\n",
      "Chinese\n",
      " - Hue\n",
      " - Karn\n",
      " - Bar\n",
      "Czech\n",
      " - Kvimen\n",
      " - Yusfonko\n",
      " - Hokibtan\n",
      "Dutch\n",
      " - Bboad\n",
      " - Hatred\n",
      " - Trame\n",
      "English\n",
      " - Lbags\n",
      " - Hir√≤n\n",
      " - Mihdo\n",
      "French\n",
      " - Laclas\n",
      " - Foh\n",
      " - Rralg\n",
      "German\n",
      " - Mura\n",
      " - Bias\n",
      " - Ur√†sr\n",
      "Greek\n",
      " - Bamoshonka\n",
      " - Thediak\n",
      " - Tkensei\n",
      "Irish\n",
      " - Smarot\n",
      " - Jitki\n",
      " - Dhantsov\n",
      "Italian\n",
      " - Cidsatsi\n",
      " - Saltiig\n",
      " - Tod\n",
      "Japanese\n",
      " - Khanaa\n",
      " - Adosta\n",
      " - Tusakuev\n",
      "Korean\n",
      " - Heug\n",
      " - Sby\n",
      " - Sonesh\n",
      "Polish\n",
      " - Qadov\n",
      " - Revko\n",
      " - Beula\n",
      "Portuguese\n",
      " - Nanami\n",
      " - Barbimirla\n",
      " - Zadcson\n",
      "Russian\n",
      " - Semichan\n",
      " - Tyago\n",
      " - Daninon\n",
      "Scottish\n",
      " - Huluhsh\n",
      " - Muen\n",
      " - Kuwen\n",
      "Spanish\n",
      " - Suema\n",
      " - Smaec\n",
      " - Cost\n",
      "Vietnamese\n",
      " - Zou\n",
      " - Lor\n",
      " - Ter\n"
     ]
    }
   ],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, nationality_idx=None):\n",
    "    # encode\n",
    "    x_0 = torch.tensor([vectorizer.vocab.begin_seq_idx for _ in range(num_samples)])\n",
    "    samples = [x_0]\n",
    "\n",
    "    if nationality_idx:\n",
    "        h = torch.tensor([nationality_idx for _ in range(num_samples)])\n",
    "        h_t = model.hidden_emb(h).unsqueeze(0)\n",
    "    else:\n",
    "        h_t = None\n",
    "\n",
    "    \n",
    "    for t in range(vectorizer.max_size):\n",
    "        x_t = samples[t].unsqueeze(1)\n",
    "        x_emb = model.emb(x_t)\n",
    "        y_t, h_t = model.rnn(x_emb, h_t)\n",
    "        y_t = model.fc(y_t)\n",
    "\n",
    "        pred = y_t.squeeze(1)\n",
    "        prob = F.softmax(pred, dim=-1)\n",
    "        indices = torch.multinomial(prob, num_samples=1).squeeze()\n",
    "        samples.append(indices)\n",
    "    \n",
    "    samples = torch.stack(samples, dim=1)\n",
    "    \n",
    "    # decode\n",
    "    surnames = []\n",
    "    for sample in samples:\n",
    "        surname = \"\"\n",
    "        for num in sample.tolist():\n",
    "            if num == vectorizer.vocab.begin_seq_idx:\n",
    "                continue\n",
    "            if num == vectorizer.vocab.end_seq_idx:\n",
    "                break\n",
    "            surname += vectorizer.vocab[num]\n",
    "        surnames.append(surname)\n",
    "    \n",
    "    return surnames\n",
    "\n",
    "for nationality in nationality_vocab:\n",
    "    print(nationality)\n",
    "    samples = sample_from_model(generator, surname_vectorizer, num_samples=3, nationality_idx=nationality_vocab[nationality])\n",
    "    for sample in samples:\n",
    "        print(\" -\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "emb = generator.emb.weight.data[1:,:]\n",
    "labels = list(surname_vocab.ordered_tokens())[1:]\n",
    "x = torch.tensor(pca.fit_transform(emb))\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.axis([torch.min(x).item(), torch.max(x).item(), torch.min(x).item(), torch.max(x).item()])\n",
    "for (xi, yi), lbl in zip(x, labels):\n",
    "    plt.text(xi, yi, lbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:16:26) \n[Clang 12.0.0 ]"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
