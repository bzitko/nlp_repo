{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surname generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"eng_fra_simplest.csv.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a05/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "        \n",
    "name = \"eng_fra_simplest.csv\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    dataset_csv=\"eng_fra_simplest.csv\",\n",
    "    model_filename=\"model.pth\",\n",
    "    # Model hyper parameter\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=100,\n",
    "    learning_rate=5e-4,\n",
    "    batch_size=64,\n",
    "    seed=1337,\n",
    "    early_stop=5,\n",
    "    source_embedding_size=64,\n",
    "    target_embedding_size=64,\n",
    "    envoding_size=64\n",
    "    # Runtime hyper parameter\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source_language</th>\n",
       "      <th>split</th>\n",
       "      <th>target_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>he 's the cutest boy in town .</td>\n",
       "      <td>train</td>\n",
       "      <td>c'est le garçon le plus mignon en ville .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>he 's a nonsmoker .</td>\n",
       "      <td>train</td>\n",
       "      <td>il est non-fumeur .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>he 's smarter than me .</td>\n",
       "      <td>train</td>\n",
       "      <td>il est plus intelligent que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>he 's a lovely young man .</td>\n",
       "      <td>train</td>\n",
       "      <td>c'est un adorable jeune homme .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>he 's three years older than me .</td>\n",
       "      <td>train</td>\n",
       "      <td>il a trois ans de plus que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13057</th>\n",
       "      <td>13057</td>\n",
       "      <td>you are n't invited .</td>\n",
       "      <td>test</td>\n",
       "      <td>vous n'êtes pas invités .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058</th>\n",
       "      <td>13058</td>\n",
       "      <td>you are always watching tv .</td>\n",
       "      <td>test</td>\n",
       "      <td>tu regardes tout le temps la télé .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13059</th>\n",
       "      <td>13059</td>\n",
       "      <td>you are trusted by every one of us .</td>\n",
       "      <td>test</td>\n",
       "      <td>chacun de nous te fait confiance .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13060</th>\n",
       "      <td>13060</td>\n",
       "      <td>you are blinded by love .</td>\n",
       "      <td>test</td>\n",
       "      <td>vous êtes aveuglé par l'amour .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13061</th>\n",
       "      <td>13061</td>\n",
       "      <td>you are a good person .</td>\n",
       "      <td>test</td>\n",
       "      <td>tu es une chouette gonzesse .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13062 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                       source_language  split  \\\n",
       "0               0        he 's the cutest boy in town .  train   \n",
       "1               1                   he 's a nonsmoker .  train   \n",
       "2               2               he 's smarter than me .  train   \n",
       "3               3            he 's a lovely young man .  train   \n",
       "4               4     he 's three years older than me .  train   \n",
       "...           ...                                   ...    ...   \n",
       "13057       13057                 you are n't invited .   test   \n",
       "13058       13058          you are always watching tv .   test   \n",
       "13059       13059  you are trusted by every one of us .   test   \n",
       "13060       13060             you are blinded by love .   test   \n",
       "13061       13061               you are a good person .   test   \n",
       "\n",
       "                                 target_language  \n",
       "0      c'est le garçon le plus mignon en ville .  \n",
       "1                            il est non-fumeur .  \n",
       "2              il est plus intelligent que moi .  \n",
       "3                c'est un adorable jeune homme .  \n",
       "4               il a trois ans de plus que moi .  \n",
       "...                                          ...  \n",
       "13057                  vous n'êtes pas invités .  \n",
       "13058        tu regardes tout le temps la télé .  \n",
       "13059         chacun de nous te fait confiance .  \n",
       "13060            vous êtes aveuglé par l'amour .  \n",
       "13061              tu es une chouette gonzesse .  \n",
       "\n",
       "[13062 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(args.dataset_csv)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Generalized vocabulary can have:\n",
    "* padding token - to fill up empty space\n",
    "* unknown token - token for out-of-vocabulary tokens\n",
    "* begin sequence - token for start of a sequence\n",
    "* end sequence - token for end of a sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " '<BOS>': 2,\n",
       " '<EOS>': 3,\n",
       " '!': 4,\n",
       " \"'\": 5,\n",
       " \"''\": 6,\n",
       " \"'ll\": 7,\n",
       " \"'m\": 8,\n",
       " \"'re\": 9,\n",
       " \"'s\": 10,\n",
       " \"'ve\": 11,\n",
       " ',': 12,\n",
       " '--': 13,\n",
       " '.': 14,\n",
       " '10': 15,\n",
       " '100': 16,\n",
       " '18': 17,\n",
       " '19': 18,\n",
       " '1:00': 19,\n",
       " '2': 20,\n",
       " '20': 21,\n",
       " '229': 22,\n",
       " '25': 23,\n",
       " '35': 24,\n",
       " '5': 25,\n",
       " '6': 26,\n",
       " '80': 27,\n",
       " '?': 28,\n",
       " '``': 29,\n",
       " 'a': 30,\n",
       " 'ability': 31,\n",
       " 'able': 32,\n",
       " 'about': 33,\n",
       " 'above': 34,\n",
       " 'abroad': 35,\n",
       " 'absent': 36,\n",
       " 'absent-minded': 37,\n",
       " 'absolute': 38,\n",
       " 'absolutely': 39,\n",
       " 'absorbed': 40,\n",
       " 'abuse': 41,\n",
       " 'abusing': 42,\n",
       " 'accept': 43,\n",
       " 'accident': 44,\n",
       " 'account': 45,\n",
       " 'accredited': 46,\n",
       " 'accusations': 47,\n",
       " 'accustomed': 48,\n",
       " 'accustoming': 49,\n",
       " 'acquaintance': 50,\n",
       " 'acquainted': 51,\n",
       " 'across': 52,\n",
       " 'act': 53,\n",
       " 'acting': 54,\n",
       " 'active': 55,\n",
       " 'actor': 56,\n",
       " 'actress': 57,\n",
       " 'actresses': 58,\n",
       " 'actually': 59,\n",
       " 'adamant': 60,\n",
       " 'adapt': 61,\n",
       " 'adaptable': 62,\n",
       " 'addict': 63,\n",
       " 'addicted': 64,\n",
       " 'adding': 65,\n",
       " 'address': 66,\n",
       " 'adequate': 67,\n",
       " 'admit': 68,\n",
       " 'admitting': 69,\n",
       " 'adorable': 70,\n",
       " 'adult': 71,\n",
       " 'advantage': 72,\n",
       " 'adventure': 73,\n",
       " 'advertisement': 74,\n",
       " 'advice': 75,\n",
       " 'advises': 76,\n",
       " 'advocate': 77,\n",
       " 'advocating': 78,\n",
       " 'affair': 79,\n",
       " 'afraid': 80,\n",
       " 'after': 81,\n",
       " 'afternoon': 82,\n",
       " 'again': 83,\n",
       " 'against': 84,\n",
       " 'age': 85,\n",
       " 'agent': 86,\n",
       " 'aggressive': 87,\n",
       " 'ago': 88,\n",
       " 'agree': 89,\n",
       " 'agreed': 90,\n",
       " 'agreement': 91,\n",
       " 'agrees': 92,\n",
       " 'ahead': 93,\n",
       " 'aids': 94,\n",
       " 'air': 95,\n",
       " 'airmail': 96,\n",
       " 'airplane': 97,\n",
       " 'airport': 98,\n",
       " 'alarmed': 99,\n",
       " 'alcoholic': 100,\n",
       " 'alert': 101,\n",
       " 'alike': 102,\n",
       " 'alive': 103,\n",
       " 'all': 104,\n",
       " 'allergic': 105,\n",
       " 'allow': 106,\n",
       " 'allowance': 107,\n",
       " 'allowed': 108,\n",
       " 'allowing': 109,\n",
       " 'almost': 110,\n",
       " 'alone': 111,\n",
       " 'along': 112,\n",
       " 'already': 113,\n",
       " 'also': 114,\n",
       " 'altogether': 115,\n",
       " 'always': 116,\n",
       " 'am': 117,\n",
       " 'amazed': 118,\n",
       " 'amazing': 119,\n",
       " 'ambidextrous': 120,\n",
       " 'ambition': 121,\n",
       " 'ambitious': 122,\n",
       " 'ambulance': 123,\n",
       " 'amends': 124,\n",
       " 'america': 125,\n",
       " 'american': 126,\n",
       " 'americans': 127,\n",
       " 'ammo': 128,\n",
       " 'ammunition': 129,\n",
       " 'among': 130,\n",
       " 'amused': 131,\n",
       " 'amusing': 132,\n",
       " 'an': 133,\n",
       " 'and': 134,\n",
       " 'android': 135,\n",
       " 'anger': 136,\n",
       " 'angry': 137,\n",
       " 'animal': 138,\n",
       " 'animals': 139,\n",
       " 'annoying': 140,\n",
       " 'another': 141,\n",
       " 'answer': 142,\n",
       " 'answering': 143,\n",
       " 'antiques': 144,\n",
       " 'anxious': 145,\n",
       " 'any': 146,\n",
       " 'anybody': 147,\n",
       " 'anymore': 148,\n",
       " 'anyone': 149,\n",
       " 'anything': 150,\n",
       " 'anywhere': 151,\n",
       " 'apartment': 152,\n",
       " 'apologize': 153,\n",
       " 'appalling': 154,\n",
       " 'apparently': 155,\n",
       " 'appealing': 156,\n",
       " 'appearing': 157,\n",
       " 'apple': 158,\n",
       " 'apples': 159,\n",
       " 'application': 160,\n",
       " 'appointment': 161,\n",
       " 'appointments': 162,\n",
       " 'appreciated': 163,\n",
       " 'approaching': 164,\n",
       " 'apt': 165,\n",
       " 'arabs': 166,\n",
       " 'archeologist': 167,\n",
       " 'are': 168,\n",
       " 'argue': 169,\n",
       " 'arguing': 170,\n",
       " 'aristocrat': 171,\n",
       " 'arithmetic': 172,\n",
       " 'arm': 173,\n",
       " 'armed': 174,\n",
       " 'arms': 175,\n",
       " 'army': 176,\n",
       " 'around': 177,\n",
       " 'arrest': 178,\n",
       " 'arrived': 179,\n",
       " 'arrogant': 180,\n",
       " 'art': 181,\n",
       " 'article': 182,\n",
       " 'articulate': 183,\n",
       " 'artist': 184,\n",
       " 'artists': 185,\n",
       " 'as': 186,\n",
       " 'ashamed': 187,\n",
       " 'asian': 188,\n",
       " 'aside': 189,\n",
       " 'ask': 190,\n",
       " 'asked': 191,\n",
       " 'asking': 192,\n",
       " 'asks': 193,\n",
       " 'asleep': 194,\n",
       " 'aspirations': 195,\n",
       " 'assertive': 196,\n",
       " 'assistance': 197,\n",
       " 'assistant': 198,\n",
       " 'associate': 199,\n",
       " 'assumed': 200,\n",
       " 'assuming': 201,\n",
       " 'astonished': 202,\n",
       " 'astronomy': 203,\n",
       " 'astute': 204,\n",
       " 'at': 205,\n",
       " 'athletic': 206,\n",
       " 'atomic': 207,\n",
       " 'atrocious': 208,\n",
       " 'attached': 209,\n",
       " 'attaching': 210,\n",
       " 'attack': 211,\n",
       " 'attacked': 212,\n",
       " 'attend': 213,\n",
       " 'attention': 214,\n",
       " 'attentive': 215,\n",
       " 'attic': 216,\n",
       " 'attitude': 217,\n",
       " 'attracted': 218,\n",
       " 'attractive': 219,\n",
       " 'audacity': 220,\n",
       " 'august': 221,\n",
       " 'australia': 222,\n",
       " 'australian': 223,\n",
       " 'australians': 224,\n",
       " 'author': 225,\n",
       " 'authority': 226,\n",
       " 'auto': 227,\n",
       " 'available': 228,\n",
       " 'avenue': 229,\n",
       " 'average': 230,\n",
       " 'avoiding': 231,\n",
       " 'awaits': 232,\n",
       " 'awake': 233,\n",
       " 'aware': 234,\n",
       " 'away': 235,\n",
       " 'awesome': 236,\n",
       " 'awfully': 237,\n",
       " 'awkward': 238,\n",
       " 'babies': 239,\n",
       " 'baby': 240,\n",
       " 'bachelor': 241,\n",
       " 'back': 242,\n",
       " 'bad': 243,\n",
       " 'bad-mannered': 244,\n",
       " 'badly': 245,\n",
       " 'baffled': 246,\n",
       " 'bag': 247,\n",
       " 'bags': 248,\n",
       " 'baker': 249,\n",
       " 'baking': 250,\n",
       " 'bald': 251,\n",
       " 'bank': 252,\n",
       " 'barbarians': 253,\n",
       " 'barefoot': 254,\n",
       " 'barely': 255,\n",
       " 'barking': 256,\n",
       " 'bars': 257,\n",
       " 'bartender': 258,\n",
       " 'baseball': 259,\n",
       " 'basics': 260,\n",
       " 'basketball': 261,\n",
       " 'bat': 262,\n",
       " 'bath': 263,\n",
       " 'bathing': 264,\n",
       " 'bathroom': 265,\n",
       " 'bathtub': 266,\n",
       " 'bats': 267,\n",
       " 'batteries': 268,\n",
       " 'battle': 269,\n",
       " 'bawling': 270,\n",
       " 'be': 271,\n",
       " 'beach': 272,\n",
       " 'beans': 273,\n",
       " 'bear': 274,\n",
       " 'beard': 275,\n",
       " 'beat': 276,\n",
       " 'beautiful': 277,\n",
       " 'beauty': 278,\n",
       " 'because': 279,\n",
       " 'become': 280,\n",
       " 'becoming': 281,\n",
       " 'bed': 282,\n",
       " 'bedrooms': 283,\n",
       " 'beds': 284,\n",
       " 'bedtime': 285,\n",
       " 'bee': 286,\n",
       " 'been': 287,\n",
       " 'beer': 288,\n",
       " 'before': 289,\n",
       " 'beggar': 290,\n",
       " 'begging': 291,\n",
       " 'begin': 292,\n",
       " 'beginner': 293,\n",
       " 'beginning': 294,\n",
       " 'behave': 295,\n",
       " 'behaving': 296,\n",
       " 'behind': 297,\n",
       " 'being': 298,\n",
       " 'beliefs': 299,\n",
       " 'believe': 300,\n",
       " 'belly': 301,\n",
       " 'belongings': 302,\n",
       " 'bench': 303,\n",
       " 'bender': 304,\n",
       " 'beside': 305,\n",
       " 'besides': 306,\n",
       " 'best': 307,\n",
       " 'better': 308,\n",
       " 'better-looking': 309,\n",
       " 'between': 310,\n",
       " 'beyond': 311,\n",
       " 'biased': 312,\n",
       " 'bicycle': 313,\n",
       " 'big': 314,\n",
       " 'bigger': 315,\n",
       " 'biggest': 316,\n",
       " 'bigot': 317,\n",
       " 'bike': 318,\n",
       " 'bill': 319,\n",
       " 'bills': 320,\n",
       " 'biologist': 321,\n",
       " 'biology': 322,\n",
       " 'birth': 323,\n",
       " 'birthday': 324,\n",
       " 'bit': 325,\n",
       " 'bite': 326,\n",
       " 'bitter': 327,\n",
       " 'björk': 328,\n",
       " 'black': 329,\n",
       " 'blackmailed': 330,\n",
       " 'blackmailing': 331,\n",
       " 'blame': 332,\n",
       " 'blast': 333,\n",
       " 'bleeding': 334,\n",
       " 'blessed': 335,\n",
       " 'blind': 336,\n",
       " 'blinded': 337,\n",
       " 'blocking': 338,\n",
       " 'blonde': 339,\n",
       " 'blood': 340,\n",
       " 'bloomer': 341,\n",
       " 'blue': 342,\n",
       " 'bluffing': 343,\n",
       " 'blushing': 344,\n",
       " 'boat': 345,\n",
       " 'body': 346,\n",
       " 'boiling': 347,\n",
       " 'book': 348,\n",
       " 'booked': 349,\n",
       " 'books': 350,\n",
       " 'bookshop': 351,\n",
       " 'booze': 352,\n",
       " 'bored': 353,\n",
       " 'boring': 354,\n",
       " 'born': 355,\n",
       " 'boss': 356,\n",
       " 'bossy': 357,\n",
       " 'boston': 358,\n",
       " 'both': 359,\n",
       " 'bother': 360,\n",
       " 'bothered': 361,\n",
       " 'bothering': 362,\n",
       " 'bottom': 363,\n",
       " 'bound': 364,\n",
       " 'bowl': 365,\n",
       " 'box': 366,\n",
       " 'boy': 367,\n",
       " 'boy-next-door': 368,\n",
       " 'boyfriend': 369,\n",
       " 'boys': 370,\n",
       " 'bragging': 371,\n",
       " 'brains': 372,\n",
       " 'brave': 373,\n",
       " 'brazil': 374,\n",
       " 'bread': 375,\n",
       " 'break': 376,\n",
       " 'breakfast': 377,\n",
       " 'breaking': 378,\n",
       " 'breast-feeding': 379,\n",
       " 'breath': 380,\n",
       " 'breathing': 381,\n",
       " 'brick': 382,\n",
       " 'bride': 383,\n",
       " 'bright': 384,\n",
       " 'brighter': 385,\n",
       " 'brightest': 386,\n",
       " 'bring': 387,\n",
       " 'bringing': 388,\n",
       " 'broadway': 389,\n",
       " 'broke': 390,\n",
       " 'brother': 391,\n",
       " 'brothers': 392,\n",
       " 'brought': 393,\n",
       " 'brown': 394,\n",
       " 'bruised': 395,\n",
       " 'brushing': 396,\n",
       " 'buck': 397,\n",
       " 'buddy': 398,\n",
       " 'budget': 399,\n",
       " 'buff': 400,\n",
       " 'bug': 401,\n",
       " 'build': 402,\n",
       " 'building': 403,\n",
       " 'built': 404,\n",
       " 'bulgaria': 405,\n",
       " 'bulked': 406,\n",
       " 'bullets': 407,\n",
       " 'bum': 408,\n",
       " 'bus': 409,\n",
       " 'business': 410,\n",
       " 'businessman': 411,\n",
       " 'businesswoman': 412,\n",
       " 'busy': 413,\n",
       " 'but': 414,\n",
       " 'butterfly': 415,\n",
       " 'buy': 416,\n",
       " 'buying': 417,\n",
       " 'by': 418,\n",
       " 'ca': 419,\n",
       " 'cafeteria': 420,\n",
       " 'cake': 421,\n",
       " 'call': 422,\n",
       " 'calling': 423,\n",
       " 'callous': 424,\n",
       " 'calls': 425,\n",
       " 'calm': 426,\n",
       " 'came': 427,\n",
       " 'camera': 428,\n",
       " 'can': 429,\n",
       " 'canada': 430,\n",
       " 'canadian': 431,\n",
       " 'canadians': 432,\n",
       " 'canary': 433,\n",
       " 'cancel': 434,\n",
       " 'cancer': 435,\n",
       " 'candidates': 436,\n",
       " 'cane': 437,\n",
       " 'cannibals': 438,\n",
       " 'cantankerous': 439,\n",
       " 'capable': 440,\n",
       " 'captain': 441,\n",
       " 'car': 442,\n",
       " 'carbon': 443,\n",
       " 'card': 444,\n",
       " 'cardiologist': 445,\n",
       " 'cards': 446,\n",
       " 'care': 447,\n",
       " 'career': 448,\n",
       " 'careful': 449,\n",
       " 'carefully': 450,\n",
       " 'careless': 451,\n",
       " 'carnations': 452,\n",
       " 'carpenter': 453,\n",
       " 'carrying': 454,\n",
       " 'case': 455,\n",
       " 'cash': 456,\n",
       " 'castle': 457,\n",
       " 'cat': 458,\n",
       " 'catch': 459,\n",
       " 'catching': 460,\n",
       " 'cats': 461,\n",
       " 'caught': 462,\n",
       " 'cause': 463,\n",
       " 'caused': 464,\n",
       " 'celebrating': 465,\n",
       " 'celebrity': 466,\n",
       " 'cellphone': 467,\n",
       " 'ceo': 468,\n",
       " 'ceremonies': 469,\n",
       " 'certain': 470,\n",
       " 'certainly': 471,\n",
       " 'chair': 472,\n",
       " 'chairman': 473,\n",
       " 'challenge': 474,\n",
       " 'champion': 475,\n",
       " 'chance': 476,\n",
       " 'chances': 477,\n",
       " 'change': 478,\n",
       " 'changing': 479,\n",
       " 'channel': 480,\n",
       " 'chapter': 481,\n",
       " 'character': 482,\n",
       " 'charge': 483,\n",
       " 'charges': 484,\n",
       " 'charity': 485,\n",
       " 'charmed': 486,\n",
       " 'charming': 487,\n",
       " 'chasing': 488,\n",
       " 'chatterbox': 489,\n",
       " 'cheat': 490,\n",
       " 'cheating': 491,\n",
       " 'checking': 492,\n",
       " 'checks': 493,\n",
       " 'cheerful': 494,\n",
       " 'chemistry': 495,\n",
       " 'chess': 496,\n",
       " 'chicago': 497,\n",
       " 'chicken': 498,\n",
       " 'child': 499,\n",
       " 'childish': 500,\n",
       " 'children': 501,\n",
       " 'china': 502,\n",
       " 'chinese': 503,\n",
       " 'chiseling': 504,\n",
       " 'choices': 505,\n",
       " 'choir': 506,\n",
       " 'choosing': 507,\n",
       " 'chop': 508,\n",
       " 'chores': 509,\n",
       " 'chosen': 510,\n",
       " 'christians': 511,\n",
       " 'christmas': 512,\n",
       " 'chubby': 513,\n",
       " 'church': 514,\n",
       " 'circulation': 515,\n",
       " 'citizen': 516,\n",
       " 'city': 517,\n",
       " 'class': 518,\n",
       " 'classifying': 519,\n",
       " 'classmate': 520,\n",
       " 'classmates': 521,\n",
       " 'classroom': 522,\n",
       " 'clean': 523,\n",
       " 'cleaning': 524,\n",
       " 'cleared': 525,\n",
       " 'clearly': 526,\n",
       " 'clerk': 527,\n",
       " 'clever': 528,\n",
       " 'cleverness': 529,\n",
       " 'climate': 530,\n",
       " 'climb': 531,\n",
       " 'clinging': 532,\n",
       " 'close': 533,\n",
       " 'closed': 534,\n",
       " 'closer': 535,\n",
       " 'closest': 536,\n",
       " 'closet': 537,\n",
       " 'closing': 538,\n",
       " 'clothes': 539,\n",
       " 'clown': 540,\n",
       " 'club': 541,\n",
       " 'coast': 542,\n",
       " 'coat': 543,\n",
       " 'cocaine': 544,\n",
       " 'coffee': 545,\n",
       " 'coke': 546,\n",
       " 'cold': 547,\n",
       " 'colleague': 548,\n",
       " 'colleagues': 549,\n",
       " 'collecting': 550,\n",
       " 'collection': 551,\n",
       " 'college': 552,\n",
       " 'colombia': 553,\n",
       " 'come': 554,\n",
       " 'comedian': 555,\n",
       " 'comedians': 556,\n",
       " 'comes': 557,\n",
       " 'comfortable': 558,\n",
       " 'comics': 559,\n",
       " 'coming': 560,\n",
       " 'command': 561,\n",
       " 'committee': 562,\n",
       " 'common': 563,\n",
       " 'company': 564,\n",
       " 'compared': 565,\n",
       " 'comparing': 566,\n",
       " 'compatible': 567,\n",
       " 'compete': 568,\n",
       " 'competitors': 569,\n",
       " 'compiling': 570,\n",
       " 'complaining': 571,\n",
       " 'complaints': 572,\n",
       " 'complete': 573,\n",
       " 'completely': 574,\n",
       " 'complicated': 575,\n",
       " 'compliment': 576,\n",
       " 'compliments': 577,\n",
       " 'compromise': 578,\n",
       " 'compulsive': 579,\n",
       " 'computer': 580,\n",
       " 'computer-savvy': 581,\n",
       " 'conceited': 582,\n",
       " 'concentrate': 583,\n",
       " 'concentrating': 584,\n",
       " 'concerned': 585,\n",
       " 'concerns': 586,\n",
       " 'concert': 587,\n",
       " 'condition': 588,\n",
       " 'conduct': 589,\n",
       " 'conference': 590,\n",
       " 'conferences': 591,\n",
       " 'confidence': 592,\n",
       " 'confident': 593,\n",
       " 'confronted': 594,\n",
       " 'confused': 595,\n",
       " 'conscientious': 596,\n",
       " 'consequence': 597,\n",
       " 'consequences': 598,\n",
       " 'conservative': 599,\n",
       " 'considerate': 600,\n",
       " 'considered': 601,\n",
       " 'considering': 602,\n",
       " 'constantly': 603,\n",
       " 'contagious': 604,\n",
       " 'contemplating': 605,\n",
       " 'content': 606,\n",
       " 'contented': 607,\n",
       " 'contest': 608,\n",
       " 'contributions': 609,\n",
       " 'control': 610,\n",
       " 'conversation': 611,\n",
       " 'convince': 612,\n",
       " 'convinced': 613,\n",
       " 'cook': 614,\n",
       " 'cookies': 615,\n",
       " 'cooking': 616,\n",
       " 'cool': 617,\n",
       " 'cooperating': 618,\n",
       " 'cop': 619,\n",
       " 'cope': 620,\n",
       " 'cops': 621,\n",
       " 'copy': 622,\n",
       " 'corn': 623,\n",
       " 'corner': 624,\n",
       " 'correct': 625,\n",
       " 'could': 626,\n",
       " 'count': 627,\n",
       " 'counting': 628,\n",
       " 'countries': 629,\n",
       " 'country': 630,\n",
       " 'counts': 631,\n",
       " 'couple': 632,\n",
       " 'courageous': 633,\n",
       " 'course': 634,\n",
       " 'courteous': 635,\n",
       " 'cousin': 636,\n",
       " 'cousins': 637,\n",
       " 'cow': 638,\n",
       " 'coward': 639,\n",
       " 'cowards': 640,\n",
       " 'cracker': 641,\n",
       " 'cracking': 642,\n",
       " 'crafty': 643,\n",
       " 'cranky': 644,\n",
       " 'crazy': 645,\n",
       " 'cream': 646,\n",
       " 'creative': 647,\n",
       " 'credit': 648,\n",
       " 'creek': 649,\n",
       " 'creepy': 650,\n",
       " 'crew': 651,\n",
       " 'crime': 652,\n",
       " 'criminal': 653,\n",
       " 'criminals': 654,\n",
       " 'critic': 655,\n",
       " 'critical': 656,\n",
       " 'criticism': 657,\n",
       " 'criticized': 658,\n",
       " 'criticizing': 659,\n",
       " 'croatia': 660,\n",
       " 'crop': 661,\n",
       " 'cross': 662,\n",
       " 'cross-legged': 663,\n",
       " 'cruel': 664,\n",
       " 'cry': 665,\n",
       " 'crybaby': 666,\n",
       " 'crying': 667,\n",
       " 'cultured': 668,\n",
       " 'cunning': 669,\n",
       " 'cup': 670,\n",
       " 'cured': 671,\n",
       " 'curious': 672,\n",
       " 'currently': 673,\n",
       " 'curse': 674,\n",
       " 'custody': 675,\n",
       " 'custom': 676,\n",
       " 'customer': 677,\n",
       " 'customs': 678,\n",
       " 'cut': 679,\n",
       " 'cute': 680,\n",
       " 'cuter': 681,\n",
       " 'cutest': 682,\n",
       " 'cutie': 683,\n",
       " 'cynical': 684,\n",
       " 'dad': 685,\n",
       " 'dance': 686,\n",
       " 'dancer': 687,\n",
       " 'dancing': 688,\n",
       " 'danger': 689,\n",
       " 'dangerous': 690,\n",
       " 'daredevil': 691,\n",
       " 'dark': 692,\n",
       " 'data': 693,\n",
       " 'date': 694,\n",
       " 'dating': 695,\n",
       " 'daughter': 696,\n",
       " 'day': 697,\n",
       " 'day-dreaming': 698,\n",
       " 'days': 699,\n",
       " 'dead': 700,\n",
       " 'deadline': 701,\n",
       " 'deaf': 702,\n",
       " 'deal': 703,\n",
       " 'dealing': 704,\n",
       " 'dear': 705,\n",
       " 'death': 706,\n",
       " 'deaths': 707,\n",
       " 'debt': 708,\n",
       " 'deceiving': 709,\n",
       " 'decided': 710,\n",
       " 'decision': 711,\n",
       " 'decisions': 712,\n",
       " 'decorating': 713,\n",
       " 'dedicated': 714,\n",
       " 'deep': 715,\n",
       " 'deeply': 716,\n",
       " 'defend': 717,\n",
       " 'defenseless': 718,\n",
       " 'definitely': 719,\n",
       " 'degree': 720,\n",
       " 'dehydrated': 721,\n",
       " 'delighted': 722,\n",
       " 'deliver': 723,\n",
       " 'deluding': 724,\n",
       " 'delusional': 725,\n",
       " 'demands': 726,\n",
       " 'demented': 727,\n",
       " 'democrat': 728,\n",
       " 'denial': 729,\n",
       " 'dentist': 730,\n",
       " 'denying': 731,\n",
       " 'departing': 732,\n",
       " 'department': 733,\n",
       " 'depend': 734,\n",
       " 'dependable': 735,\n",
       " 'dependent': 736,\n",
       " 'depending': 737,\n",
       " 'depressed': 738,\n",
       " 'depth': 739,\n",
       " 'design': 740,\n",
       " 'desk': 741,\n",
       " 'desperate': 742,\n",
       " 'dessert': 743,\n",
       " 'detail': 744,\n",
       " 'detective': 745,\n",
       " 'determined': 746,\n",
       " 'devastated': 747,\n",
       " 'develop': 748,\n",
       " 'developing': 749,\n",
       " 'devoid': 750,\n",
       " 'devoted': 751,\n",
       " 'diabetic': 752,\n",
       " 'diary': 753,\n",
       " 'dictionary': 754,\n",
       " 'did': 755,\n",
       " 'die': 756,\n",
       " 'died': 757,\n",
       " 'diet': 758,\n",
       " 'dieting': 759,\n",
       " 'different': 760,\n",
       " 'difficult': 761,\n",
       " 'difficulties': 762,\n",
       " 'digging': 763,\n",
       " 'dining': 764,\n",
       " 'dinner': 765,\n",
       " 'diplomat': 766,\n",
       " 'diplomatic': 767,\n",
       " 'direct': 768,\n",
       " 'direction': 769,\n",
       " 'director': 770,\n",
       " 'dirty': 771,\n",
       " 'disagreeing': 772,\n",
       " 'disappoint': 773,\n",
       " 'disappointed': 774,\n",
       " 'disappointment': 775,\n",
       " 'disbeliever': 776,\n",
       " 'discounting': 777,\n",
       " 'discouraged': 778,\n",
       " 'discuss': 779,\n",
       " 'discussing': 780,\n",
       " 'discussion': 781,\n",
       " 'disease': 782,\n",
       " 'disguise': 783,\n",
       " 'disgusted': 784,\n",
       " 'disgusting': 785,\n",
       " 'dishes': 786,\n",
       " 'disloyal': 787,\n",
       " 'disobeying': 788,\n",
       " 'disposable': 789,\n",
       " 'disposal': 790,\n",
       " 'dissatisfied': 791,\n",
       " 'distances': 792,\n",
       " 'distantly': 793,\n",
       " 'distracted': 794,\n",
       " 'disturb': 795,\n",
       " 'disturbed': 796,\n",
       " 'ditch': 797,\n",
       " 'divorced': 798,\n",
       " 'divulge': 799,\n",
       " 'dizzy': 800,\n",
       " 'dj': 801,\n",
       " 'do': 802,\n",
       " 'doctor': 803,\n",
       " 'doctors': 804,\n",
       " 'documentary': 805,\n",
       " 'does': 806,\n",
       " 'dog': 807,\n",
       " 'dogs': 808,\n",
       " 'doing': 809,\n",
       " 'doll': 810,\n",
       " 'dollars': 811,\n",
       " 'done': 812,\n",
       " 'doomed': 813,\n",
       " 'door': 814,\n",
       " 'double-parked': 815,\n",
       " 'doubt': 816,\n",
       " 'doubtful': 817,\n",
       " 'down': 818,\n",
       " 'downstairs': 819,\n",
       " 'downtown': 820,\n",
       " 'dozing': 821,\n",
       " 'dragged': 822,\n",
       " 'drained': 823,\n",
       " 'drama': 824,\n",
       " 'dramatist': 825,\n",
       " 'drawing': 826,\n",
       " 'drawn': 827,\n",
       " 'dreading': 828,\n",
       " 'dream': 829,\n",
       " 'dreamer': 830,\n",
       " 'dreaming': 831,\n",
       " 'dreams': 832,\n",
       " 'dress': 833,\n",
       " 'dressed': 834,\n",
       " 'drink': 835,\n",
       " 'drinking': 836,\n",
       " 'dripping': 837,\n",
       " 'drive': 838,\n",
       " 'driver': 839,\n",
       " 'driving': 840,\n",
       " 'drop': 841,\n",
       " 'drowning': 842,\n",
       " 'drug': 843,\n",
       " 'drugs': 844,\n",
       " 'drunk': 845,\n",
       " 'drunkard': 846,\n",
       " 'dumb': 847,\n",
       " 'dumbfounded': 848,\n",
       " 'during': 849,\n",
       " 'duty': 850,\n",
       " 'dwelling': 851,\n",
       " 'dye': 852,\n",
       " 'dying': 853,\n",
       " 'each': 854,\n",
       " 'eager': 855,\n",
       " 'ear': 856,\n",
       " 'earlier': 857,\n",
       " 'early': 858,\n",
       " 'earning': 859,\n",
       " 'ears': 860,\n",
       " 'earth': 861,\n",
       " 'earthquakes': 862,\n",
       " 'ease': 863,\n",
       " 'easily': 864,\n",
       " 'east': 865,\n",
       " 'easter': 866,\n",
       " 'easy': 867,\n",
       " 'easy-to-read': 868,\n",
       " 'easygoing': 869,\n",
       " 'eat': 870,\n",
       " 'eaten': 871,\n",
       " 'eater': 872,\n",
       " 'eating': 873,\n",
       " 'economics': 874,\n",
       " 'editor': 875,\n",
       " 'editor-in-chief': 876,\n",
       " 'educated': 877,\n",
       " 'education': 878,\n",
       " 'efficient': 879,\n",
       " 'effort': 880,\n",
       " 'egg': 881,\n",
       " 'eggs': 882,\n",
       " 'egypt': 883,\n",
       " 'eight': 884,\n",
       " 'eighteen': 885,\n",
       " 'either': 886,\n",
       " 'electrician': 887,\n",
       " 'else': 888,\n",
       " 'elusive': 889,\n",
       " 'email': 890,\n",
       " 'embarking': 891,\n",
       " 'embarrassed': 892,\n",
       " 'embarrasses': 893,\n",
       " 'embarrassing': 894,\n",
       " 'embassy': 895,\n",
       " 'emergencies': 896,\n",
       " 'emergency': 897,\n",
       " 'emotional': 898,\n",
       " 'emotionally': 899,\n",
       " 'employed': 900,\n",
       " 'employees': 901,\n",
       " 'end': 902,\n",
       " 'endangering': 903,\n",
       " 'enemies': 904,\n",
       " 'enemy': 905,\n",
       " 'energy': 906,\n",
       " 'engaged': 907,\n",
       " 'engagement': 908,\n",
       " 'engineer': 909,\n",
       " 'england': 910,\n",
       " 'english': 911,\n",
       " 'englishman': 912,\n",
       " 'enjoy': 913,\n",
       " 'enjoyed': 914,\n",
       " 'enjoying': 915,\n",
       " 'enlisted': 916,\n",
       " 'enough': 917,\n",
       " 'enter': 918,\n",
       " 'entertaining': 919,\n",
       " 'entire': 920,\n",
       " 'entirely': 921,\n",
       " 'entitled': 922,\n",
       " 'entrance': 923,\n",
       " 'envious': 924,\n",
       " 'environment': 925,\n",
       " 'environmentalist': 926,\n",
       " 'equal': 927,\n",
       " 'era': 928,\n",
       " 'errands': 929,\n",
       " 'estimate': 930,\n",
       " 'europe': 931,\n",
       " 'evacuating': 932,\n",
       " 'even': 933,\n",
       " 'evening': 934,\n",
       " 'ever': 935,\n",
       " 'every': 936,\n",
       " 'everybody': 937,\n",
       " 'everyone': 938,\n",
       " 'everything': 939,\n",
       " 'evicted': 940,\n",
       " 'evil': 941,\n",
       " 'ex-girlfriend': 942,\n",
       " 'exact': 943,\n",
       " 'exactly': 944,\n",
       " 'exaggerate': 945,\n",
       " 'exam': 946,\n",
       " 'examination': 947,\n",
       " 'examinations': 948,\n",
       " 'exams': 949,\n",
       " 'excellent': 950,\n",
       " 'exchange': 951,\n",
       " 'excited': 952,\n",
       " 'excuses': 953,\n",
       " 'executive': 954,\n",
       " 'exercising': 955,\n",
       " 'exhausted': 956,\n",
       " 'expected': 957,\n",
       " 'expecting': 958,\n",
       " 'expenses': 959,\n",
       " 'expensive': 960,\n",
       " 'experience': 961,\n",
       " 'experienced': 962,\n",
       " 'experiences': 963,\n",
       " 'experiencing': 964,\n",
       " 'experiments': 965,\n",
       " 'expert': 966,\n",
       " 'explain': 967,\n",
       " 'extremely': 968,\n",
       " 'extrovert': 969,\n",
       " 'extroverted': 970,\n",
       " 'eye': 971,\n",
       " 'eyebrows': 972,\n",
       " 'eyes': 973,\n",
       " 'face': 974,\n",
       " 'faced': 975,\n",
       " 'faces': 976,\n",
       " 'facing': 977,\n",
       " 'fact': 978,\n",
       " 'facts': 979,\n",
       " 'fail': 980,\n",
       " 'failing': 981,\n",
       " 'failure': 982,\n",
       " 'fair': 983,\n",
       " 'fairly': 984,\n",
       " 'faithful': 985,\n",
       " 'fake': 986,\n",
       " 'fallen': 987,\n",
       " 'falling': 988,\n",
       " 'familiar': 989,\n",
       " 'family': 990,\n",
       " 'famous': 991,\n",
       " 'fan': 992,\n",
       " 'fantasy': 993,\n",
       " 'far': 994,\n",
       " 'farmer': 995,\n",
       " 'farsi': 996,\n",
       " 'fascinated': 997,\n",
       " 'fascinating': 998,\n",
       " 'fashionable': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, tokens=None, pad_token=None, unk_token=None, begin_seq_token=None, end_seq_token=None):\n",
    "        self._tok2idx = {}\n",
    "        self._idx2tok = {}\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_idx = None\n",
    "        if pad_token is not None:\n",
    "            self.pad_idx = self.add_token(pad_token)\n",
    "        \n",
    "        self.unk_token = unk_token\n",
    "        self.unk_idx = None\n",
    "        if unk_token is not None:\n",
    "            self.unk_idx = self.add_token(unk_token)\n",
    "\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.begin_seq_idx = None\n",
    "        if begin_seq_token is not None:\n",
    "            self.begin_seq_idx = self.add_token(begin_seq_token)\n",
    "\n",
    "        self.end_seq_token = end_seq_token\n",
    "        self.end_seq_idx = None\n",
    "        if end_seq_token is not None:\n",
    "            self.end_seq_idx = self.add_token(end_seq_token)\n",
    "\n",
    "        if tokens is not None:\n",
    "            self.add_tokens(tokens)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self._tok2idx:\n",
    "            idx = len(self._tok2idx)\n",
    "            self._tok2idx[token] = idx\n",
    "            self._idx2tok[idx] = token\n",
    "            return idx\n",
    "        return self._tok2idx[token]\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def ordered_indices(self):\n",
    "        return sorted(self._idx2tok)\n",
    "\n",
    "    def ordered_tokens(self):\n",
    "        for i in sorted(self._idx2tok):\n",
    "            yield self._idx2tok[i]\n",
    "\n",
    "    def __getitem__(self, token_or_idx):\n",
    "        if isinstance(token_or_idx, str):\n",
    "            return self._tok2idx.get(token_or_idx, self.unk_idx)\n",
    "        if isinstance(token_or_idx, int):\n",
    "            return self._idx2tok.get(token_or_idx, self.unk_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tok2idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in sorted(self._idx2tok):\n",
    "            yield self._idx2tok[i]\n",
    "\n",
    "    def info(self):\n",
    "        txt = f\"Vocabulary size:{len(self)}\"\n",
    "        for i in range(min(4, len(self))):\n",
    "            txt += f\" {self[i]}:{i}\"\n",
    "        txt += \" ...\"\n",
    "        print(txt)\n",
    "\n",
    "source_words = {w for sent in df[df.split == \"train\"].source_language for w in sent.split()}\n",
    "source_vocab = Vocab(sorted(source_words), pad_token=\"<PAD>\", unk_token=\"<UNK>\", begin_seq_token=\"<BOS>\", end_seq_token=\"<EOS>\")\n",
    "\n",
    "target_words = {w for sent in df[df.split == \"train\"].target_language for w in sent.split()}\n",
    "target_vocab = Vocab(sorted(target_words), pad_token=\"<PAD>\", unk_token=\"<UNK>\", begin_seq_token=\"<BOS>\", end_seq_token=\"<EOS>\")\n",
    "\n",
    "source_vocab._tok2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "\n",
    "* `vectorizer(tokens)` should return long tensor (vector). Vector values corresponds to tokens. Vector should be filled with padding indexes to satisfy vector maximal size.  \n",
    "\n",
    "* 👍  method `vectorize(tokens, seq=True)` receives \n",
    "    * `tokens` - a list of vocabulary entities, and\n",
    "    * `seq` - if set to true, then resulting vector represents a sequence.\n",
    "\n",
    "Let 0 is padding index, 2 is begin of sequence index and 3 is end of sequence index and maximal size is 10. Then for tokens whose indices are, for example, 56, 96 41, a resulting vector should be `[2 56 96 41 3 0 0 0 0]`.  \n",
    "If `seq` is set to false, resulting vector should be `[56 96 41 0 0 0 0 0 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "\n",
    "    def __init__(self, vocabulary, max_size=-1):\n",
    "        self.vocab = vocabulary\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def vectorize(self, tokens, seq=True):\n",
    "        indices = [self.vocab[tok] for tok in tokens]\n",
    "        if seq:\n",
    "            indices = [self.vocab.begin_seq_idx] + indices + [self.vocab.end_seq_idx]\n",
    "        \n",
    "        if self.max_size >= 0:\n",
    "            indices = indices[:self.max_size]\n",
    "            indices += [self.vocab.pad_idx] * (self.max_size - len(indices))\n",
    "        return torch.LongTensor(indices)    \n",
    "\n",
    "source_max_size = max(len(sent.split()) for sent in df.source_language)\n",
    "source_vectorizer = Vectorizer(source_vocab, source_max_size + 2)\n",
    "\n",
    "target_max_size = max(len(sent.split()) for sent in df.target_language)\n",
    "target_vectorizer = Vectorizer(target_vocab, target_max_size + 2)\n",
    "\n",
    "assert source_vectorizer.vectorize(\"i was there before you was .\".split(), seq=True).tolist() == [2, 1371, 2901, 2682, 289, 3015, 2901, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "assert target_vectorizer.vectorize(\"j'étais là avant toi .\".split(), seq=True).tolist() == [2, 2264, 2581, 325, 4482, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "`NMTDataset` class inherits `torch.utils.data.Dataset`.  \n",
    "Implemented methods are:\n",
    "* `__init__(df, vectorizer_x, vectorizer_h, nationalities)` initialization receives dataframe `df`, `vectorizer_x` vectorizer for surnames, and `vectorizer_h` for nationalities.\n",
    "* `set_split()` for setting current data split\n",
    "* 👍 `__getitem__(idx)` should return triple of vectors: x, y, h where \n",
    "    * x is vectorized surname, for example `[2 56 96 41 3 0 0 0 0]`\n",
    "    * y is x moved to left, for example `[56 96 41 3 0 0 0 0 0]`\n",
    "    * h is vector for nationality\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, source_vectorizer, target_vectorizer):\n",
    "        self.df = df\n",
    "        self.source_vectorizer = source_vectorizer\n",
    "        self.target_vectorizer = target_vectorizer\n",
    "        self._lookup = {split: self.df[self.df.split == split] for split in set(self.df.split)}\n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self._target_split = split\n",
    "        self._target_df = self._lookup[split]\n",
    "\n",
    "    def vectorize_source(self, sent):\n",
    "        return self.source_vectorizer.vectorize(sent.split(), seq=True)\n",
    "\n",
    "    def vectorize_target(self, sent):\n",
    "        return self.target_vectorizer.vectorize(sent.split(), seq=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.df.iloc[idx]\n",
    "        return self.vectorize_source(data.source_language), self.vectorize_target(data.target_language)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True):\n",
    "    for x, y in torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle):\n",
    "        yield x.to(args.device), y.to(args.device)\n",
    "\n",
    "dataset = NMTDataset(df, source_vectorizer, target_vectorizer)\n",
    "\n",
    "assert len(dataset) == 9138\n",
    "assert len(dataset[0]) == 2\n",
    "\n",
    "x, y = dataset[0]\n",
    "assert x.tolist() == [2, 1274, 10, 2676, 682, 367, 1396, 2754, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "assert y.tolist() == [2, 510, 2498, 1874, 2498, 3396, 2814, 1505, 4682, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "👍  \n",
    "`SurnameGenerator` initialization receives \n",
    "* `embedding_size` dimension of embedding vector (for surnames)\n",
    "* `num_embeddings` size of surname vocabulary\n",
    "* `rnn_hidden_size` dimension of hidden RNN layer\n",
    "* `num_rnn_hidden_embedding` size of nationality vocabulary\n",
    "* `dropout_p` probability of dropout\n",
    "\n",
    "Model will consist of: \n",
    "* $E_s$ - embedding layer for surnames, \n",
    "* $E_n$ - embedding layer for nationalities,\n",
    "* GRU - gated reccurent unit\n",
    "* FC - fully connected layer with dropout\n",
    "\n",
    "Forward receives \n",
    "* $x$ indicies of surnames\n",
    "* $h$ indicies of nationalityes\n",
    "\n",
    "then $\\hat{y} = FC(GRU(E_s(x), E_n(h)))$.\n",
    "\n",
    "Apply softmax if `apply_softmax` is set to true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NMTEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_size,\n",
    "                                            embedding_dim=hidden_size)\n",
    "\n",
    "        self.rnn = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        input = self.embedding(input)\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "encoder = NMTEncoder(input_size=len(dataset.source_vectorizer.vocab),\n",
    "                     hidden_size=args.source_embedding_size)\n",
    "\n",
    "batch_size = 3\n",
    "source_vec_max_size = dataset.source_vectorizer.max_size\n",
    "x, y = next(generate_batches(dataset, batch_size=batch_size))\n",
    "y_hat, h = encoder(x)\n",
    "\n",
    "assert y_hat.shape == (batch_size, source_vec_max_size, args.source_embedding_size)\n",
    "\n",
    "# y_hat[:,-1,:] == h.squeeze()  # zadnji output je zapravo izlazni hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "👍 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 26, 64])\n",
      "torch.Size([3, 26, 64])\n",
      "torch.Size([3, 26, 64]) torch.Size([1, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_size, \n",
    "                                      embedding_dim=hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, \n",
    "                            out_features=output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden=None, apply_softmax=False):\n",
    "        output = self.embedding(input)\n",
    "        print(output.shape)\n",
    "        output = F.relu(output)\n",
    "        print(output.shape)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        print(output.shape, hidden.shape)\n",
    "        output = self.softmax(self.fc(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "decoder = NMTDecoder(hidden_size=args.target_embedding_size,\n",
    "                     output_size=len(dataset.target_vectorizer.vocab))\n",
    "\n",
    "batch_size = 3\n",
    "target_vec_max_size = dataset.target_vectorizer.max_size\n",
    "x, y = next(generate_batches(dataset, batch_size=batch_size))\n",
    "y_hat, h = decoder(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def compute_accuracy(y_hat, y):\n",
    "    _, y_hat_indices = y_hat.max(dim=1)\n",
    "    n_correct = torch.eq(y_hat_indices, y).sum().item()\n",
    "    return n_correct / len(y_hat_indices) * 100\n",
    "\n",
    "# early stopping\n",
    "def early_stop(train_state, model):\n",
    "    val_loss = train_state[\"val_loss\"]\n",
    "    if len(val_loss) < 2:\n",
    "        torch.save(model.state_dict(), args.model_filename)\n",
    "        return False\n",
    "    \n",
    "    if val_loss[-1] < val_loss[-2]:\n",
    "        torch.save(model.state_dict(), args.model_filename)\n",
    "    \n",
    "    if len(val_loss) >= args.early_stop:\n",
    "        val_loss =  val_loss[-args.early_stop:]\n",
    "        return all(val_loss[i] < val_loss[i + 1] \n",
    "                   for i in range(args.early_stop - 1))\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining loss function\n",
    "\n",
    "For \n",
    "* $N$ - batch size\n",
    "* $C$ - sequence size\n",
    "* $V$ - vocabulary size\n",
    "\n",
    "let $\\hat{y}$ be a prediction tensor of shape $N \\times C \\times V$ and $y$ be a target tensor of shape $N \\times C$.  \n",
    "Function `compute_loss(y_hat, y)` is responsible for computing negative log-likelihood loss for each datapoint in the batch.\n",
    "\n",
    "Before applying pyTorch's NLLLoss, each sequence in the batch $\\hat{y}$ has to be turned into log of probabilities, i.e. $log(softmax(\\hat{y}_i))$ for $i=1...N$. After calculating all $N$ losses by $NLLLoss(log(softmax(\\hat{y}_i)), y_i)$ `compute_loss()` returns their mean.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "def compute_loss(y_hat, y):\n",
    "    y_hat = F.log_softmax(y_hat, dim=-1)\n",
    "    losses = []\n",
    "    for b_y_hat, b_y in zip(y_hat, y):\n",
    "        lv = loss_func(b_y_hat, b_y)\n",
    "        losses.append(lv)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "batch_size = 3\n",
    "seq_size = 2\n",
    "vocab_size = 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "y_hat = torch.rand(batch_size, seq_size, vocab_size)\n",
    "y = torch.tensor([[0, 1], [2, 1], [3, 0]])\n",
    "loss = compute_loss(y_hat, y)\n",
    "assert torch.all(loss == torch.tensor(1.33540785))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/brane/Documents/PMFST Projekt/internacionalizacija/Natural Language Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m n_correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y_hat_indices) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m    \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# encoder-decoder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m encoder \u001b[39m=\u001b[39m NMTEncoder(hidden_size\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49msource_embedding_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                      output_size\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(dataset\u001b[39m.\u001b[39;49msource_vectorizer\u001b[39m.\u001b[39;49mvocab))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoder \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m decoder \u001b[39m=\u001b[39m NMTDecoder(hidden_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mtarget_embedding_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brane/Documents/PMFST%20Projekt/internacionalizacija/Natural%20Language%20Processing/DL4NLP/nlp_repo/assignments/a05/nlp_a05.3_nmt_solution.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                      output_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mtarget_vectorizer\u001b[39m.\u001b[39mvocab))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'output_size'"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_hat, y):\n",
    "    _, y_hat_indices = y_hat.max(dim=-1)\n",
    "    y_hat_indices = y_hat_indices.ravel()\n",
    "    y = y.ravel()\n",
    "    n_correct = torch.eq(y_hat_indices, y).sum().item()\n",
    "    return n_correct / len(y_hat_indices) * 100    \n",
    "\n",
    "# encoder-decoder\n",
    "\n",
    "encoder = NMTEncoder(hidden_size=args.source_embedding_size,\n",
    "                     output_size=len(dataset.source_vectorizer.vocab))\n",
    "encoder = encoder.to(args.device)\n",
    "                     \n",
    "decoder = NMTDecoder(hidden_size=args.target_embedding_size,\n",
    "                     output_size=len(dataset.target_vectorizer.vocab))\n",
    "decoder = decoder.to(args.device)                \n",
    "\n",
    "# seed\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "# loss, optimizer, scheduler\n",
    "loss_func = torch.nn.NLLLoss(ignore_index=dataset.vectorizer_x.vocab.pad_idx)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=args.learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=args.learning_rate)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# progress bars\n",
    "epoch_bar = tqdm(desc='epochs', total=args.num_epochs, position=0)\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "# train state tracker\n",
    "train_state = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": [],}\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = running_acc = 0.0\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for batch_index, (x, y) in enumerate(batch_generator):\n",
    "            encoder.zero_grad()\n",
    "            y_hat, h = encoder(x, h)\n",
    "            loss = compute_loss(y_hat, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            acc_t = compute_accuracy(y_hat, y)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)        \n",
    "\n",
    "        # Iterate over val dataset\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = running_acc = 0.0\n",
    "        \n",
    "        generator.eval()\n",
    "        for batch_index, (x, y, h) in enumerate(batch_generator):\n",
    "            y_hat =  generator(x, h)\n",
    "\n",
    "            loss = compute_loss(y_hat, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_hat, y)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)   \n",
    "\n",
    "        if early_stop(train_state, generator):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_state[\"val_loss\"])\n",
    "plt.plot(train_state[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load(args.model_filename))\n",
    "\n",
    "generator = generator.to(args.device)\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "generator.eval()\n",
    "for batch_index, (x, y, h) in enumerate(batch_generator):\n",
    "    y_hat =  generator(x, h)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = compute_loss(y_hat, y)\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_hat, y)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "print(f\"Test loss: {running_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {running_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Function `sample_from_model(model, vectorizer, num_samples=10, nationality_idx=None)` must generate `num_samples` surnames. If `nationality_idx` is set to some nationality index, then generated surnames belong to specific nationality. Nationality is represented as first hidden input $h_0$ to GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, vectorizer, num_samples=10, nationality_idx=None):\n",
    "    return []\n",
    "\n",
    "for nationality in nationality_vocab:\n",
    "    print(nationality)\n",
    "    samples = sample_from_model(generator, surname_vectorizer, num_samples=3, nationality_idx=nationality_vocab[nationality])\n",
    "    for sample in samples:\n",
    "        print(\" -\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "\n",
    "emb = generator.emb.weight.data[1:,:]\n",
    "labels = list(surname_vocab.ordered_tokens())[1:]\n",
    "x = torch.tensor(pca.fit_transform(emb))\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.axis([torch.min(x).item(), torch.max(x).item(), torch.min(x).item(), torch.max(x).item()])\n",
    "for (xi, yi), lbl in zip(x, labels):\n",
    "    plt.text(xi, yi, lbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:16:26) \n[Clang 12.0.0 ]"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
