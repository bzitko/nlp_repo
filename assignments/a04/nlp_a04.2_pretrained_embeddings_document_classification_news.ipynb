{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Using Pretrained Embeddings for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading data\n",
    "\n",
    "* glove pretrained embeddings\n",
    "* dataset with splits for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_name = \"glove.6B.100d.txt.gz\"\n",
    "if not os.path.exists(download_name):\n",
    "    response = requests.get(f\"https://github.com/allenai/spv2/blob/master/model/{download_name}?raw=true\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "name = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(name):\n",
    "    with gzip.open(download_name, 'rb') as gzf, open(name, 'wb') as fp:\n",
    "        fp.write(gzf.read())\n",
    "\n",
    "download_name = \"news_with_splits.csv.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a04/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "name = \"news_with_splits.csv\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # files\n",
    "    pretrained_embedding_file = \"glove.6B.100d.txt\",\n",
    "    data_file = \"news_with_splits.csv\",\n",
    "    # hyper parameter\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stop=5,\n",
    "    # model\n",
    "    model_filename=\"model.pth\"\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reading\n",
    "\n",
    "Read embedding vectors, dataset and words from the datase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Read embeddings\n",
    "\n",
    "üëç  \n",
    "Read embedding file and store embeddings into dictionary `embeddings`.  \n",
    "Keys are the words and values are word embeddings represented as list of floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "\n",
    "assert len(embeddings) == 400000\n",
    "assert len(embeddings[\"dog\"]) == 100\n",
    "assert embeddings[\"dog\"][:3] == [0.30817, 0.30938, 0.52803]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Read words and classes from data with preprocessing\n",
    "\n",
    "Preprocessing function is used to prepare text for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = \" \" + txt + \" \"\n",
    "    txt = re.sub(r\"(\\w)([\\\"\\'*/+-,.:;?!#&\\(\\)\\\\])+\\s\", r\"\\1 \\2 \", txt) # token ends with 1 or more non-words\n",
    "    txt = re.sub(r\"\\s([\\\"\\'*/+-,.:;?!#&\\(\\)\\\\])+(\\w)\", r\" \\1 \\2\", txt) # token begins with 1 or more non-words\n",
    "    txt = re.sub(r\"(\\w)n't\\s\", r\"\\1 n't \", txt)\n",
    "    txt = re.sub(r\"(\\w)'(s|re|ll|m|ve|d)\\s\", r\"\\1 '\\2 \", txt)\n",
    "    return txt.lower().strip()\n",
    "\n",
    "preprocess(\"John's hand-made glasses don't fit on her nose!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç  \n",
    "Read dataset file and store it into variable `df` as pandas DataFrame object.  \n",
    "Columns of `df` are:\n",
    "* title - title of the news,\n",
    "* category - one of four possible categories of a title,\n",
    "* split - train, val or test split.\n",
    "\n",
    "Create set of `words` by preprocessing titles from `df`.  \n",
    "`words` must include only words which appear also in `embeddings`.\n",
    "Create set of `categories` by collecting all categories from `df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "assert set(df.columns) == {\"title\", \"category\", \"split\"}\n",
    "assert len(df) == 120000, \"not good\"\n",
    "assert len(words) == 29271\n",
    "assert categories == {'Business', 'Sci/Tech', 'Sports', 'World'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create vocab and embedding\n",
    "\n",
    "Class `Vocab` is used for indexing tokens. There can be two special tokens: padding token and unknown token.  \n",
    "If special tokens are set, they would be the first entries in the vocabulary, having indexes 0 and 1 respectively.\n",
    "\n",
    "Two vocabularies are created:\n",
    "* `title_vocab` from set of `words` and is having special tokens,\n",
    "* `category_vocab` form set of `categories` without special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, tokens=None, pad_token=None, unk_token=None):\n",
    "        self._tok2idx = {}\n",
    "        self._idx2tok = {}\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_idx = None\n",
    "        if pad_token is not None:\n",
    "            self.pad_idx = self.add_token(pad_token)\n",
    "        \n",
    "        self.unk_token = unk_token\n",
    "        self.unk_idx = None\n",
    "        if unk_token is not None:\n",
    "            self.unk_idx = self.add_token(unk_token)\n",
    "\n",
    "        if tokens is not None:\n",
    "            self.add_tokens(tokens)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self._tok2idx:\n",
    "            idx = len(self._tok2idx)\n",
    "            self._tok2idx[token] = idx\n",
    "            self._idx2tok[idx] = token\n",
    "            return idx\n",
    "        return self._tok2idx[token]\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def ordered_indices(self):\n",
    "        return sorted(self._idx2tok)\n",
    "\n",
    "    def ordered_tokens(self):\n",
    "        for i in sorted(self._idx2tok):\n",
    "            yield self._idx2tok[i]\n",
    "\n",
    "    def __getitem__(self, token_or_idx):\n",
    "        if isinstance(token_or_idx, str):\n",
    "            return self._tok2idx.get(token_or_idx, self.unk_idx)\n",
    "        if isinstance(token_or_idx, int):\n",
    "            return self._idx2tok.get(token_or_idx, self.unk_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._tok2idx)\n",
    "\n",
    "    def info(self):\n",
    "        txt = f\"Vocabulary size:{len(self)}\"\n",
    "        for i in range(min(4, len(self))):\n",
    "            txt += f\" {self[i]}:{i}\"\n",
    "        txt += \" ...\"\n",
    "        print(txt)\n",
    "\n",
    "title_vocab = Vocab(tokens=sorted(words), pad_token=\"<PAD>\", unk_token=\"<UNK>\")\n",
    "category_vocab = Vocab(tokens=sorted(categories))\n",
    "\n",
    "title_vocab.info()\n",
    "category_vocab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç  \n",
    "Not all embeddings are going to be used for classification tasks.  \n",
    "Only tokens from `title_vocab` will have their embeddings.  \n",
    "`embeddings` does not have vectors for padding token and unknown token and they have to be created.\n",
    "* padding token is zero vector\n",
    "* unknown token is mean of all embeddings stored in `embeddings`\n",
    "\n",
    "Create 2D tensor `emb` whose first two rows would be embeddings for padding and unknown token.  \n",
    "Other rows must match tokens from `title_vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "assert emb.shape == (29273, 100)\n",
    "assert bool(torch.all(emb[0] == torch.zeros(100)))\n",
    "assert bool(torch.all(torch.eq(emb[96], torch.tensor(embeddings[title_vocab[96]]))))\n",
    "assert bool(torch.all(torch.eq(emb[345], torch.tensor(embeddings[title_vocab[345]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer\n",
    "\n",
    "* `vectorizer(tokens)` should return long tensor (vector). Vector values corresponds to tokens. Vector should be filled with padding indexes to satisfy vector maximal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "\n",
    "    def __init__(self, vocabulary, max_size=-1):\n",
    "        self.vocab = vocabulary\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \n",
    "        return        \n",
    "\n",
    "title_max_size = max(len(preprocess(title).split()) for title in df.title)\n",
    "title_vectorizer = Vectorizer(title_vocab, title_max_size)\n",
    "category_vectorizer = Vectorizer(category_vocab)\n",
    "\n",
    "assert torch.all(category_vectorizer.vectorize([\"World\"]) == torch.tensor([3]))\n",
    "assert torch.all(title_vectorizer.vectorize([\"john\", \"went\", \"home\"]) == torch.tensor([14357, 28510, 12839, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dataset and vectorization\n",
    "\n",
    "`NewsDataset` class inherits `torch.utils.data.Dataset`.  \n",
    "Implemented methods are:\n",
    "* `__init__(df, vectorizer_x, vectorizer_y)` initialization receives dataframe `df`, `vectorizer_x` vectorizer for data and `vectorizer_y` for targets.\n",
    "* `set_split()` for setting current data split\n",
    "* üëç `__getitem__(idx)` should return pair of vectors for data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, vectorizer_x, vectorizer_y):\n",
    "        self.df = df        \n",
    "        self.vectorizer_x = vectorizer_x\n",
    "        self.vectorizer_y = vectorizer_y\n",
    "        self._lookup = {split: df[df.split == split] for split in set(df.split)}\n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "    def set_split(self, split):\n",
    "        self._target_split = split\n",
    "        self._target_df = self._lookup[split]\n",
    "\n",
    "    def vectorize_x(self, title):\n",
    "        return self.vectorizer_x.vectorize(preprocess(title).split())\n",
    "\n",
    "    def vectorize_y(self, category):\n",
    "        return self.vectorizer_y.vectorize([category]).squeeze()\n",
    "\n",
    "    def frequency_x(self):\n",
    "        return torch.tensor([len(self.df[self.df.title==tok]) for tok in self.vectorizer_x.vocab.ordered_tokens()])\n",
    "\n",
    "    def frequency_y(self):\n",
    "        return torch.tensor([len(self.df[self.df.category==tok]) for tok in self.vectorizer_y.vocab.ordered_tokens()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True):\n",
    "    for x, y in torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle):\n",
    "        yield x.to(args.device), y.to(args.device)\n",
    "\n",
    "dataset = NewsDataset(df, title_vectorizer, category_vectorizer)\n",
    "\n",
    "assert len(dataset) == 84000\n",
    "assert torch.all(dataset.vectorize_x(\"John was there.\") == torch.tensor([14357, 28332, 26280, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "assert dataset.vectorize_y(\"World\").shape == tuple()\n",
    "assert dataset.vectorize_y(\"World\") == torch.tensor(3)\n",
    "assert torch.all(dataset[4][0] == torch.tensor([ 4086,  9729,  1905, 10689,  6558, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classifier\n",
    "\n",
    "üëç  \n",
    "`NewsClassifier` initialization receives \n",
    "* `num_channels` number of convolutional channels\n",
    "* `hidden_dim` dimension of a hidden layer\n",
    "* `num_classes` dimension of output layer\n",
    "* `dropout_p` probability of dropout\n",
    "* `embeddings` weights for embedding layer.\n",
    "\n",
    "Model will consist of: \n",
    "* Embedding layer whose weights are passed by `embeddings`, \n",
    "* 4 Convolutional layers with ELU activations,\n",
    "* 2 Fully connected layers with ReLU activation and dropout for the hidden layer\n",
    "\n",
    "In forward will apply AvgPool after convolitions to reduce last dimension to 1 and apply dropout before going to FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, num_classes, hidden_dim, dropout_p, embeddings):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def forward(self, x, apply_softmax=True):\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "classifier = NewsClassifier(num_channels=args.num_channels,\n",
    "                            num_classes=len(dataset.vectorizer_y.vocab), \n",
    "                            hidden_dim=args.hidden_dim,\n",
    "                            dropout_p=args.dropout_p,\n",
    "                            embeddings=emb)\n",
    "\n",
    "\n",
    "\n",
    "x_batch, y_batch = next(generate_batches(dataset, batch_size=3))\n",
    "assert classifier(x_batch).shape == (3, 4)\n",
    "assert y_batch.shape == (3, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "def compute_accuracy(y_hat, y):\n",
    "    _, y_hat_indices = y_hat.max(dim=1)\n",
    "    n_correct = torch.eq(y_hat_indices, y).sum().item()\n",
    "    return n_correct / len(y_hat_indices) * 100\n",
    "\n",
    "# early stopping\n",
    "def early_stop(train_state, model):\n",
    "    val_loss = train_state[\"val_loss\"]\n",
    "    if len(val_loss) < 2:\n",
    "        torch.save(model.state_dict(), args.model_filename)\n",
    "        return False\n",
    "    \n",
    "    if val_loss[-1] < val_loss[-2]:\n",
    "        torch.save(model.state_dict(), args.model_filename)\n",
    "    \n",
    "    if len(val_loss) >= args.early_stop:\n",
    "        val_loss =  val_loss[-args.early_stop:]\n",
    "        return all(val_loss[i] < val_loss[i + 1] \n",
    "                   for i in range(args.early_stop - 1))\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "# loss, optimizer, scheduler\n",
    "loss_func = torch.nn.CrossEntropyLoss(1 - torch.log(dataset.frequency_y()))\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# progress bars\n",
    "epoch_bar = tqdm(desc='epochs', total=args.num_epochs, position=0)\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "# train state tracker\n",
    "train_state = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"val_loss\": [],\n",
    "               \"val_acc\": [],}\n",
    "\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = running_acc = 0.0\n",
    "        \n",
    "        classifier.train()\n",
    "        for batch_index, (x, y) in enumerate(batch_generator):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = classifier(x)\n",
    "\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_t = compute_accuracy(y_hat, y)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)        \n",
    "\n",
    "        # Iterate over val dataset\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = running_acc = 0.0\n",
    "        \n",
    "        classifier.eval()\n",
    "        for batch_index, (x, y) in enumerate(batch_generator):\n",
    "            y_hat =  classifier(x)\n",
    "\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            \n",
    "            acc_t = compute_accuracy(y_hat, y)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)   \n",
    "\n",
    "        if early_stop(train_state, classifier):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluation\n",
    "\n",
    "Calculating test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(args.model_filename))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = torch.nn.CrossEntropyLoss(1 / torch.log(dataset.frequency_y()))\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size)\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "classifier.eval()\n",
    "for batch_index, (x, y) in enumerate(batch_generator):\n",
    "    y_hat =  classifier(x)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_hat, y)\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_hat, y)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "print(f\"Test loss: {running_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {running_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç  Show cross-validation as heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Inference\n",
    "\n",
    "üëç  Make function for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, vectorizer_x, vectorizer_y, title):\n",
    "    return\n",
    "\n",
    "predict(classifier, title_vectorizer, category_vectorizer, \"John was there\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
