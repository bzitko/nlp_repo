{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"frankenstein_with_splits.csv.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a04/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "name = \"frankenstein_with_splits.csv\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Embeddings with Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nlp import StepByStep, Vectorizer, Vocabulary\n",
    "from itertools import chain\n",
    "\n",
    "def allclose(a, b, atol=1e-4):\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "    return torch.allclose(a, b, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_df = pd.read_csv(\"frankenstein_with_splits.csv\")\n",
    "\n",
    "train_df = cbow_df[cbow_df.split == \"train\"]\n",
    "val_df = cbow_df[cbow_df.split == \"val\"]\n",
    "test_df = cbow_df[cbow_df.split == \"test\"]\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "üëç\n",
    "Create variables:\n",
    "* `cbow_vocab` as instance of `Vocabulary` having `<pad>` as pad token and `<unk>` as unknown token and `cuttoff` set to `5`.\n",
    "* fill `cbow_vocab` with words from context and target words from `train_df`\n",
    "* `cbow_size` as length of the longest word window of context from `train_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test\n",
    "assert cbow_size == 6\n",
    "assert len(cbow_vocab) == 6125\n",
    "assert cbow_vocab[\"<pad>\"] == 0\n",
    "assert cbow_vocab[\"<unk>\"] == 1\n",
    "assert cbow_vocab[\"monster\"] == 3516\n",
    "assert cbow_vocab.inv[6124] == \"zeal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitted vocabularized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cbow_vocab.pad_many(train_df.context.apply(str.split), cbow_size)\n",
    "y_train = cbow_vocab.vocabularize(train_df.target.fillna(cbow_vocab.pad_tok))\n",
    "\n",
    "x_val = cbow_vocab.pad_many(val_df.context.apply(str.split), cbow_size)\n",
    "y_val = cbow_vocab.vocabularize(val_df.target.fillna(cbow_vocab.pad_tok))\n",
    "\n",
    "x_test = cbow_vocab.pad_many(test_df.context.apply(str.split), cbow_size)\n",
    "y_test = cbow_vocab.vocabularize(test_df.target.fillna(cbow_vocab.pad_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: CBOW\n",
    "\n",
    "üëç Create `CBOWModel` class as MLP with single embedding layer and fully connected layer. Methods are:\n",
    "* `__init__(self, vocabulary_size, embedding_size, padding_idx=0)` initializes perceptron with\n",
    "    * vocabulary size (number of embeddings for embedding layer and number of output features for fully connected layer)\n",
    "    * embedding size (embedding dimension for embedding layer and number of input features of fully connected layer) üëç fill embedding layer with uniform distribution of scalars between -1 and 1\n",
    "    * padding index (index of padding token for embedding layer)\n",
    "\n",
    "\n",
    "\n",
    "For example, \n",
    "`__init__(vocabulary_size=10, embedding_size=4, padding_idx=0)`\n",
    "will create:\n",
    "* one embedding layer of shape 10x4 with uniformly distributed values between -1 and 1\n",
    "* one fully connected layer of shape 10x4\n",
    "\n",
    "\n",
    "* `forward(self, x, apply_softmax=False)` for given input `x` makes forward step and eventually applies softmax on output:\n",
    "    * apply embedding layer and sum all produced enbedding vectors to get embedding vector of x\n",
    "    * apply dropout of 0.3 on embedding vector of x\n",
    "    * apply fully connected layer on dropout embedding vector of x to produce y\n",
    "    * optionally apply softmax on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module): # Simplified cbow Model\n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocabulary_size (int): number of vocabulary items, controls the\n",
    "                number of embeddings and prediction vector size\n",
    "            embedding_size (int): size of the embeddings\n",
    "            padding_idx (int): default 0; Embedding will not use this index\n",
    "        \"\"\"\n",
    "        super(CBOWModel, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# test\n",
    "StepByStep.set_seed(96)\n",
    "model = CBOWModel(10, 4, 0)\n",
    "assert len(list(model.children())) == 2\n",
    "emb, fc = model.children()\n",
    "\n",
    "assert isinstance(emb, nn.Embedding)\n",
    "assert emb.weight.shape == (10, 4)\n",
    "assert allclose(emb.weight[0], [ 0.8413,  0.0059,  0.3175, -0.9656])\n",
    "\n",
    "assert isinstance(fc, nn.Linear)\n",
    "assert fc.weight.shape == (10, 4)\n",
    "\n",
    "x = torch.tensor([[9, 6, 0, 0], \n",
    "                  [8, 2, 0, 0]])\n",
    "assert allclose(model.forward(x), [[-0.0184,  1.3356, -0.4942,  0.3283, -0.1433,  0.1653, -1.4455, -0.2786, 0.7114, -0.7395],\n",
    "                                   [-0.2586,  0.3689,  0.2119, -0.0987, -0.3589, -0.2639, -0.6216, -0.0662, 0.4040, -0.0168]])\n",
    "assert allclose(model.forward(x[0]), [ 0.0058,  0.1728,  0.4249, -0.4100, -0.8248, -0.0792, -0.3269, -0.1172, -0.1891,  0.0513])\n",
    "assert allclose(model.forward(x[1], apply_softmax=True), [0.0871, 0.1098, 0.1696, 0.0636, 0.0768, 0.0659, 0.0984, 0.0791, 0.0956, 0.1542])\n",
    "del model, x, emb, fc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.set_seed(96)\n",
    "\n",
    "model = CBOWModel(vocabulary_size=len(cbow_vocab),\n",
    "                  embedding_size=50,\n",
    "                  padding_idx=cbow_vocab[cbow_vocab.pad_tok])\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_loader = DataLoader(CBOWDataset(x_train, y_train), batch_size=32, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(CBOWDataset(x_val, y_val), batch_size=32, drop_last=True, shuffle=True)\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer, scheduler)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "\n",
    "sbs.train_by_loss_change(1e-3)\n",
    "\n",
    "sbs.plot_losses(ylog=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sbs.model(x_test).argmax(dim=1) == y_test).sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    for k, v in results:\n",
    "        print (f\"...[{v:.2f}] - {k}\")\n",
    "\n",
    "def get_closest(target_word, vocab, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Get the n closest\n",
    "    words to your word.\n",
    "    \"\"\"\n",
    "    # Calculate distances to all other words\n",
    "    word_embedding = embeddings[vocab[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in vocab.items():\n",
    "        if word == vocab.pad_tok or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"monster\"\n",
    "embeddings = model.embedding.weight.data\n",
    "pretty_print(get_closest(word, cbow_vocab, embeddings, n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = model.embedding.weight.data\n",
    "\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in cbow_vocab:\n",
    "        print(f\"{target_word} is not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, cbow_vocab, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings2d = pca.fit_transform(embeddings)\n",
    "embeddings2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings2d, columns=[\"X\", \"Y\"])\n",
    "# adding a columns for the corresponding words\n",
    "df['Words'] = list(cbow_vocab)\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# plotting a scatter plot\n",
    "fig = px.scatter(df, x=\"X\", y=\"Y\", text=\"Words\", log_x=True, size_max=60)\n",
    "# adjusting the text position\n",
    "fig.update_traces(textposition='top center')\n",
    "# setting up the height and title\n",
    "fig.update_layout(height=600, title_text='Word embeddings')\n",
    "\n",
    "# displaying the figure\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7f86253a46c5444491fbb7541857edfa"
  },
  "gist": {
   "data": {
    "description": "chapters/06_RNN/tweet_classifcation/tweeter_classification.ipynb",
    "public": true
   },
   "id": "7f86253a46c5444491fbb7541857edfa"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "255px",
    "left": "561px",
    "right": "20px",
    "top": "179px",
    "width": "266px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
