{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import bz2\n",
    "from zipfile import ZipFile\n",
    "\n",
    "url_path = \"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a04/sgns/\"\n",
    "downloads = {\"frankenstein_preprocessed_word.txt.bz2\": \"frankenstein_preprocessed_word.txt\",\n",
    "             \"frankenstein_preprocessed_lemma.txt.bz2\": \"frankenstein_preprocessed_lemma.txt\"}\n",
    "\n",
    "for download_name, extract_name in downloads.items():\n",
    "    if extract_name and os.path.exists(extract_name):\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(download_name):\n",
    "        response = requests.get(f\"{url_path}{download_name}\")\n",
    "        with open(download_name, \"wb\") as fp:\n",
    "            fp.write(response.content)\n",
    "        response.close()\n",
    "\n",
    "    if not extract_name:\n",
    "        continue\n",
    "\n",
    "    _, ext = os.path.splitext(download_name)\n",
    "    if ext == \".bz2\":    \n",
    "        with open(download_name, 'rb') as bzf, open(extract_name, 'wb') as fp:\n",
    "            fp.write(bz2.decompress(bzf.read()))\n",
    "    elif ext == \".zip\":\n",
    "        with ZipFile(download_name) as zf:\n",
    "            zf.extractall(path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation\n",
    "\n",
    "## 1.1 Read corpus\n",
    "\n",
    "üëç In this task, you're going to write a function called read_corpus that reads a text file and processes its content into a list of words. The goal is to return a \"corpus,\" which is a list of sentences from the file, with each sentence split into individual words and converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    corpus = []\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            corpus.append(line.lower().split())\n",
    "    return corpus\n",
    "\n",
    "corpus = read_corpus(\"frankenstein_preprocessed_word.txt\")\n",
    "print(corpus[:2])\n",
    "\n",
    "assert len(corpus) == 3222\n",
    "assert len(corpus[0]) == 13\n",
    "assert corpus[-1][:3] == [\"he\", \"was\", \"soon\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Word Frequency\n",
    "\n",
    "üëç In this task, you'll write a function called `build_word_freqs` that takes a \"corpus\" (which is a list of lists of words) and calculates the frequency of each word in the corpus. The function will then return a dictionary where each key is a word and its value is the proportion of how often that word appears in the entire corpus.\n",
    "\n",
    "The frequency of a word $ w $, denoted as $ f(w) $, is the ratio of the count of $ w $ in the corpus to the total number of words $ N $:\n",
    "\n",
    "$$\n",
    "f(w) = \\frac{c(w)}{N}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ f(w) $ is the **frequency function** for a word $ w $.\n",
    "- $ c(w) $ is the **count function** that returns the number of times word $ w $ appears in the corpus.\n",
    "- $ N $ is the **total number of words** in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_freqs(corpus):\n",
    "    words = [w for sent in corpus for w in sent]\n",
    "    word_counts = Counter(words)\n",
    "    total_count = sum(word_counts.values())\n",
    "    word_frequencies = {w: count / total_count \n",
    "                        for w, count in word_counts.items()}\n",
    "    return word_frequencies\n",
    "\n",
    "word_freqs = build_word_freqs(corpus)\n",
    "print(word_freqs)\n",
    "\n",
    "assert len(word_freqs) == 7042\n",
    "assert f\"{word_freqs['frankenstein']:.6f}\" == \"0.000315\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Vocabulary\n",
    "\n",
    "üëç In this task, you'll write a function called build_vocab that takes a dictionary of word frequencies (like the output from the previous function) and creates a vocabulary. The vocabulary is a mapping of words to unique indices, where the most frequent word gets the index 0, the second most frequent word gets index 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(word_freqs):\n",
    "    vocab = {word: idx \n",
    "             for idx, word in enumerate(sorted(word_freqs, \n",
    "                                               key=word_freqs.get, \n",
    "                                               reverse=True))}\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(word_freqs)\n",
    "print(vocab)\n",
    "\n",
    "len(vocab) == 7042\n",
    "assert vocab[\"frankenstein\"] == 315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Word probability\n",
    "\n",
    "üëç In this task, you'll write a function called build_word_probs that takes a dictionary of word frequencies (like the output from the build_word_freqs function) and converts those frequencies into word probabilities. These probabilities will be adjusted using a power transformation, which is controlled by a parameter power (with a default value of 3/4)\n",
    "\n",
    "The probability of choosing a word $ w $ as a negative sample is typically defined as:\n",
    "\n",
    "$$\n",
    "P(w) = \\frac{f(w)^\\alpha}{\\sum_{w' \\in V} f(w')^\\alpha}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ f(w) $ is the frequency of the word $ w $ in the corpus.\n",
    "- $ \\alpha $ is a parameter that controls the exponentiated frequency. Typically, $ \\alpha $ is set to a value between 0.5 and 1.0, with $ \\alpha = 0.75 $ being a common choice.\n",
    "- $ V $ is the vocabulary, i.e., the set of all words in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_probs(word_freqs, power=3/4):\n",
    "    word_probs = {w: freq ** power \n",
    "                  for w, freq in word_freqs.items()}\n",
    "    total_word_probs = sum(word_probs.values())\n",
    "    word_probs = {w: prob / total_word_probs \n",
    "                  for w, prob in word_probs.items()}\n",
    "    return word_probs\n",
    "\n",
    "word_probs = build_word_probs(word_freqs)\n",
    "print(word_probs)\n",
    "\n",
    "assert len(word_probs) == 7042\n",
    "assert f\"{word_probs['frankenstein']:.6f}\" == \"0.000405\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Positive sampling\n",
    "\n",
    "üëç In this task, you'll write a function called generate_positive_pairs that takes a list of sentences (called the corpus) and a window size (called window_size). The function will generate positive word-context pairs from the text, where the target word is paired with the context words around it, within the specified window size.\n",
    "\n",
    "For sentece sentence:\n",
    "\n",
    "_\"The quick brown fox jumps over the lazy dog.\"_\n",
    "\n",
    "and windows size 2, table below shows neighboring words:\n",
    "\n",
    "| **Center Word** | **Left Neighbors**  | **Right Neighbors** | **Context Words**                  |\n",
    "|-----------------|---------------------|---------------------|------------------------------------|\n",
    "| **The**         | -                   | quick, brown        | quick, brown                       |\n",
    "| **quick**       | The                 | brown, fox          | The, brown, fox                    |\n",
    "| **brown**       | The, quick          | fox, jumps          | The, quick, fox, jumps             |\n",
    "| **fox**         | quick, brown        | jumps, over         | quick, brown, jumps, over          |\n",
    "| **jumps**       | brown, fox          | over, the           | brown, fox, over, the              |\n",
    "| **over**        | fox, jumps          | the, lazy           | fox, jumps, the, lazy              |\n",
    "| **the**         | jumps, over         | lazy, dog           | jumps, over, lazy, dog             |\n",
    "| **lazy**        | over, the           | dog                 | over, the, dog                     |\n",
    "| **dog**         | the, lazy           | -                   | the, lazy                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positive_pairs(corpus, window_size):\n",
    "    pairs = []\n",
    "    for sent in corpus:\n",
    "        for i, target_word in enumerate(sent):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sent), i + window_size + 1)\n",
    "            context = [sent[j] for j in range(start, end) if j != i]\n",
    "            for context_word in context:\n",
    "                pairs.append((target_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "positive_pairs = generate_positive_pairs(corpus, 2)\n",
    "\n",
    "print(positive_pairs[:10])\n",
    "\n",
    "assert len(positive_pairs) == 323564\n",
    "assert set(positive_pairs) >= {('frankenstein', ','),  ('frankenstein', 'or'),  (',', 'frankenstein')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Negative sampling\n",
    "\n",
    "üëç In this task, you'll write a function called generate_negative_samples that takes a target word, a dictionary of word probabilities, and generates negative samples. Negative samples are words that do not co-occur with the target word but are drawn based on their probability distribution. This is typically used in tasks like training word embeddings (e.g., Word2Vec) where negative sampling is used to improve model efficiency.\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "1. **Initialize an empty list** `negative_samples` that will store the words generated as negative samples.\n",
    "\n",
    "2. **Get the weights for sampling**:\n",
    "   - Extract the word probabilities from the `word_probs` dictionary into a list called `word_weights`. Each element in `word_weights` should correspond to the probability of the word at the same index in the list of words.\n",
    "\n",
    "3. **Generate negative samples**:\n",
    "   - Use a loop to generate **`num_samples`** negative samples. Inside the loop:\n",
    "     - Use `random.choices()` to sample a word from the `word_probs` dictionary. Pass the list of words (`list(word_probs)`) and their corresponding weights (`word_weights`).\n",
    "     - If the word sampled is not the `target_word`, append it to the `negative_samples` list.\n",
    "\n",
    "4. **Ensure the target word is excluded**:\n",
    "   - Make sure that the word sampled is not the same as the `target_word`. If the word is the same as the target, repeat the sampling until a different word is chosen.\n",
    "\n",
    "5. **Return the list of negative samples**:\n",
    "   - Once the list contains the specified number of negative samples (`num_samples`), return the list `negative_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(target_word, word_probs, num_samples=5):\n",
    "    negative_samples = []\n",
    "    word_weights = [word_probs[w] for w in word_probs]\n",
    "    while len(negative_samples) < num_samples:\n",
    "        neg_word = random.choices(list(word_probs), \n",
    "                                   weights=word_weights,\n",
    "                                   k=1)[0]\n",
    "        if neg_word != target_word:\n",
    "            negative_samples.append(neg_word)\n",
    "    return negative_samples\n",
    "\n",
    "\n",
    "target_word = positive_pairs[0][0]\n",
    "sample = generate_negative_samples(target_word, word_probs, num_samples=5)\n",
    "\n",
    "assert len(sample) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Dataset\n",
    "\n",
    "The `SkipGramDataset` class is designed to prepare data for training a Skip-gram model using **negative sampling**. It transforms a given text corpus into a dataset where each sample consists of a target word, its context word, and a set of negative samples. Here's what it does:\n",
    "\n",
    "1. **Initialization (`__init__`)**:\n",
    "   - **Builds word frequencies** from the corpus using `build_word_freqs()`.\n",
    "   - **Creates a vocabulary** (`self.vocab`) using `build_vocab()` based on word frequencies.\n",
    "   - **Calculates word probabilities** (`word_probs`) using `build_word_probs()`, with an optional power parameter to adjust the probability distribution.\n",
    "   - **Generates positive word-context pairs** from the corpus using the `generate_positive_pairs()` function with the specified window size.\n",
    "   - **Generates negative samples** for each word-context pair using `generate_negative_samples()`.\n",
    "   - Each data sample is stored as a tuple containing:\n",
    "     - The index of the target word.\n",
    "     - The index of the context word.\n",
    "     - The indices of the negative samples.\n",
    "   \n",
    "2. **`__len__()`**: Returns the number of samples in the dataset (the length of `self.data`).\n",
    "\n",
    "3. **`__getitem__()`**: Returns a specific data sample (target word, context word, negative samples) given an index `idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "\n",
    "    def __init__(self, corpus, window_size, num_negative_samples, power=0.75):\n",
    "        word_freqs = build_word_freqs(corpus)\n",
    "        self.vocab = build_vocab(word_freqs)\n",
    "        word_probs = build_word_probs(word_freqs, power)\n",
    "\n",
    "        positive_pairs = generate_positive_pairs(corpus, window_size=window_size)\n",
    "\n",
    "        self.data = []\n",
    "        for target_word, context_word in tqdm(positive_pairs):\n",
    "            negative_samples = generate_negative_samples(target_word, word_probs, num_samples=num_negative_samples)\n",
    "    \n",
    "            target_idx, context_idx = vocab[target_word], vocab[context_word]\n",
    "            negative_samples_idxs = [vocab[neg_word] for neg_word in negative_samples]\n",
    "            \n",
    "            self.data.append((target_idx, context_idx, negative_samples_idxs))            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = SkipGramDataset(corpus, window_size=2, num_negative_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Skip-gram with Negative Sampling Model\n",
    "\n",
    "The Skip-gram model tries to predict the context words for a given target word. The objective is to maximize the similarity between the **target word embedding** and the **context word embedding**, while minimizing the similarity between the target and **negative samples**.\n",
    "\n",
    "#### Key Variables:\n",
    "- $ V $ is the vocabulary size (total number of unique words).\n",
    "- $ d $ is the embedding dimension (the size of the word vector).\n",
    "- $ \\mathbf{E}_t \\in \\mathbb{R}^{d} $ is the embedding vector for the target word.\n",
    "- $ \\mathbf{E}_c \\in \\mathbb{R}^{d} $ is the embedding vector for the context word.\n",
    "- $ \\mathbf{E}_n^{(i)} \\in \\mathbb{R}^{d} $ is the embedding vector for the $ i $-th negative sample.\n",
    "  \n",
    "### Model Components:\n",
    "1. **Embeddings**:\n",
    "   The model uses two embedding layers:\n",
    "   - One for the target word embeddings: $ \\mathbf{E}_t = \\text{Embedding}(\\text{target}) $\n",
    "   - One for the context word embeddings: $ \\mathbf{E}_c = \\text{Embedding}(\\text{context}) $\n",
    "   \n",
    "2. **Target and Context Word Score**:\n",
    "   For each positive pair (target word, context word), the score is computed by taking the **dot product** between the target embedding and the context embedding:\n",
    "   $$\n",
    "   \\text{positive\\_score} = \\mathbf{E}_t \\cdot \\mathbf{E}_c = \\sum_{i=1}^d \\mathbf{E}_{t,i} \\cdot \\mathbf{E}_{c,i}\n",
    "   $$\n",
    "   where $ \\mathbf{E}_{t,i} $ and $ \\mathbf{E}_{c,i} $ are the individual dimensions of the target and context word embeddings.\n",
    "\n",
    "3. **Negative Sampling Score**:\n",
    "   For each negative sample $ n^{(i)} $, the model computes the score between the target word embedding and the negative sample embeddings. The score for each negative sample $ i $ is computed as:\n",
    "   $$\n",
    "   \\text{negative\\_score}^{(i)} = \\mathbf{E}_t \\cdot \\mathbf{E}_n^{(i)}\n",
    "   $$\n",
    "   where $ \\mathbf{E}_n^{(i)} $ is the embedding for the $ i $-th negative sample. This score is negative because the model wants to minimize the similarity between the target word and negative samples.\n",
    "\n",
    "4. **Loss Function**:\n",
    "   The Skip-gram model uses **binary cross-entropy loss** to maximize the probability of predicting the correct context word and minimize the probability of predicting negative samples. This is done using the **sigmoid function** applied to the scores.\n",
    "\n",
    "   The loss for the positive pair (target, context) is:\n",
    "   $$\n",
    "   \\text{positive\\_loss} = -\\log(\\sigma(\\text{positive\\_score}))\n",
    "   $$\n",
    "   where $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $ is the **sigmoid function**.\n",
    "\n",
    "   The loss for each negative sample is:\n",
    "   $$\n",
    "   \\text{negative\\_loss}^{(i)} = -\\log(\\sigma(-\\text{negative\\_score}^{(i)}))\n",
    "   $$\n",
    "   The negative sign ensures that we want the similarity between the target word and negative samples to be small (i.e., $ \\mathbf{E}_t $ should be dissimilar to $ \\mathbf{E}_n^{(i)} $).\n",
    "\n",
    "   The **total loss** is the sum of the positive loss and the sum of the negative losses across all negative samples:\n",
    "   $$\n",
    "   \\text{total\\_loss} = -\\log(\\sigma(\\text{positive\\_score})) - \\sum_{i=1}^{K} \\log(\\sigma(-\\text{negative\\_score}^{(i)}))\n",
    "   $$\n",
    "   where $ K $ is the number of negative samples.\n",
    "\n",
    "5. **Final Loss**:\n",
    "   The average loss is returned, which is the mean of the losses across all the training examples:\n",
    "   $$\n",
    "   \\text{loss} = \\frac{1}{N} \\sum_{n=1}^{N} \\text{total\\_loss}_n\n",
    "   $$\n",
    "   where $ N $ is the batch size (number of training samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipgramModel, self).__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, target, context, negative_samples):\n",
    "        # Target embedding\n",
    "        target_emb = self.target_embeddings(target)\n",
    "        \n",
    "        # Positive context embedding\n",
    "        context_emb = self.context_embeddings(context)\n",
    "        \n",
    "        # Negative samples embeddings\n",
    "        negative_embs = self.context_embeddings(torch.cat(negative_samples))\n",
    "        \n",
    "        # Compute the loss\n",
    "        positive_score = torch.sum(target_emb * context_emb, dim=1)\n",
    "        negative_score = torch.sum(target_emb.unsqueeze(1) * negative_embs, dim=2)\n",
    "        \n",
    "        # Apply sigmoid and log to get the loss\n",
    "        loss = -torch.log(torch.sigmoid(positive_score)) - torch.sum(torch.log(torch.sigmoid(-negative_score)), dim=1)\n",
    "        \n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = SkipgramModel(vocab_size=len(dataset.vocab), embedding_dim=embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    total_loss = 0\n",
    "    for target, context, negative_samples in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target, context, negative_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "\n",
    "    scheduler.step(total_loss.mean())        \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch={epoch + 1}, batch loss={total_loss.mean()}, lr={current_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.target_embeddings.weight.data\n",
    "vocab = dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings[vocab[\"frankenstein\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    for k, v in results:\n",
    "        print (f\"...[{v:.2f}] - {k}\")\n",
    "\n",
    "def get_closest(target_word, vocab, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Get the n closest\n",
    "    words to your word.\n",
    "    \"\"\"\n",
    "    # Calculate distances to all other words\n",
    "    word_embedding = embeddings[vocab[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in vocab.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results\n",
    "\n",
    "get_closest(\"frankenstein\", dataset.vocab, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = model.target_embeddings.weight.data\n",
    "\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"====== {target_word} ======\")\n",
    "    if target_word not in dataset.vocab:\n",
    "        print(f\"{target_word} is not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, dataset.vocab, word_embeddings, n=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutomat_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
