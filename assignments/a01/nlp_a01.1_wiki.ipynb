{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"wiki.train.tokens.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a01/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "    \n",
    "name = \"wiki.train.tokens\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b52b8c",
   "metadata": {},
   "source": [
    "# 1. Read data\n",
    "\n",
    "Filename \"wiki.train.tokens\" contains token-prepared text from Wikipedia.  \n",
    "Store the content of the file into variable `wikitext`.\n",
    "Print first 100 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903a1f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('wiki.train.tokens', encoding=\"utf8\") as fp:\n",
    "    wikitext = fp.read()\n",
    "\n",
    "wikitext[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449d31c",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "Create function *tokenize* which: \n",
    "* for a given text  \n",
    "* returns list of tokens which are separated by the whitespace chars\n",
    "\n",
    "Print first 10 tokens from `wikitext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "tokenize(wikitext)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e756366",
   "metadata": {},
   "source": [
    "# 3. Heaps' law\n",
    "\n",
    "Empirical law.\n",
    "\n",
    "* $N$ = number of tokens\n",
    "* $V$ = vocabulary size\n",
    "\n",
    "$V = k N^{\\beta}$ where:\n",
    "* $k$ is between 10 and 100\n",
    "* $\\beta$ is between 0.4 and 0.6\n",
    "\n",
    "First determine and print $N$ and $V$, then for each $k$ between 10 and 100 with step 10 print $k$ and corresponding $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(1 for t in tokenize(wikitext))\n",
    "V = len(set(tokenize(wikitext)))\n",
    "\n",
    "\n",
    "print(f\"N={N}, V={V}\\n\")\n",
    "\n",
    "import math\n",
    "\n",
    "for k in range(10, 101, 10):\n",
    "    beta = (math.log(V) - math.log(k)) / math.log(N)\n",
    "    print(f\"k={k}, beta = {beta:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7652adb",
   "metadata": {},
   "source": [
    "# 3. Word-to-index mapping\n",
    "\n",
    "Create two dictionaries:\n",
    "* w2i - that maps word to index and (index can be cummulative determined by the size of current dictionary)\n",
    "* i2w - that maps index to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {}\n",
    "for token in tokenize(wikitext):\n",
    "    if token not in w2i:\n",
    "        w2i[token] = len(w2i) \n",
    "        \n",
    "        \n",
    "i2w = {i: w for w, i in w2i.items()}\n",
    "\n",
    "assert i2w[w2i[\"language\"]] == \"language\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15762d4c",
   "metadata": {},
   "source": [
    "# 4. Word counter\n",
    "\n",
    "`Counter` class from `collections` module can be used  for counting frequencies of words  \n",
    "`from collections import Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ba258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokenize(wikitext)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450b266",
   "metadata": {},
   "source": [
    "Print 10 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8702d",
   "metadata": {},
   "source": [
    "Print 10 least common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39967584",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2f7f6",
   "metadata": {},
   "source": [
    "###  Relation between frequencies \n",
    "\n",
    "Make plot where:\n",
    "* x axis represents words sorted by their frequency (descending)\n",
    "* y axis are word frequencies (in log scale)\n",
    "\n",
    "For example,  \n",
    "token \"the\" has frequency 113161 and is the first x value,  \n",
    "token \",\" has frequency 99913 and is the second x value,  \n",
    "...  \n",
    "token \"gallinae\" has frequency 3 and is the last x value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = sorted(counter.values(), reverse=True)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.plot(range(len(freqs)), freqs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27605db4",
   "metadata": {},
   "source": [
    "# 5. Zipf's law\n",
    "\n",
    "Empirical law:\n",
    "* r - rank of the word (word with the largest frequency has rank 1)\n",
    "* f - frequency of the word\n",
    "\n",
    "$f \\propto \\frac{1}{r}$ \n",
    "\n",
    "Make plot where:\n",
    "* x axis represents words sorted by frequencies (use log scale for this axis) and\n",
    "* y axis are frequencies (use log scale for this axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, counts = zip(*counter.most_common())\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Frequency')\n",
    "plt.plot(range(len(words)), counts)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5004b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
