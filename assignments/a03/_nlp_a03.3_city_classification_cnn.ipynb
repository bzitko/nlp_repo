{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"worldcities_with_splits.csv.zip\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a03/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "        \n",
    "name = \"worldcities_with_splits.csv\"\n",
    "if not os.path.exists(name):\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(download_name) as zf:\n",
    "        zf.extract(name)\n",
    "\n",
    "download_name = \"nlp.py\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a03/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "del download_name, name   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying Cities with a Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nlp import StepByStep, ConvVectorizer, Vocabulary\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "üëç\n",
    "Create variables:\n",
    "* `country_vocab` as instance of `Vocabulary` filled with cities from `train_df`\n",
    "* `city_vocab` as instance of `Vocabulary` whose unknown token is `@` and is filled with all letters from all cities in `train_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "city_df = pd.read_csv(\"worldcities_with_splits.csv\")\n",
    "\n",
    "train_df = city_df[city_df.split == \"train\"]\n",
    "val_df = city_df[city_df.split == \"val\"]\n",
    "test_df = city_df[city_df.split == \"test\"]\n",
    "\n",
    "# vocabulary\n",
    "\n",
    "\n",
    "\n",
    "# test\n",
    "assert len(country_vocab) == 80\n",
    "assert len(city_vocab) == 199\n",
    "\n",
    "assert country_vocab == {'Albania': 0, 'Algeria': 1, 'Angola': 2, 'Argentina': 3, 'Armenia': 4, 'Australia': 5, 'Austria': 6, 'Azerbaijan': 7, 'Belarus': 8, 'Belgium': 9, 'Bolivia': 10, 'Brazil': 11, 'Bulgaria': 12, 'Canada': 13, 'Chile': 14, 'China': 15, 'Colombia': 16, 'Congo (Kinshasa)': 17, 'Croatia': 18, 'Cuba': 19, 'Czechia': 20, 'Dominican Republic': 21, 'Ecuador': 22, 'Egypt': 23, 'El Salvador': 24, 'Finland': 25, 'France': 26, 'Germany': 27, 'Greece': 28, 'Guatemala': 29, 'Hungary': 30, 'India': 31, 'Indonesia': 32, 'Iran': 33, 'Israel': 34, 'Italy': 35, 'Japan': 36, 'Kazakhstan': 37, 'Kenya': 38, 'Lithuania': 39, 'Macedonia': 40, 'Malta': 41, 'Mexico': 42, 'Moldova': 43, 'Morocco': 44, 'Netherlands': 45, 'New Zealand': 46, 'Nicaragua': 47, 'Niger': 48, 'Nigeria': 49, 'Norway': 50, 'Pakistan': 51, 'Peru': 52, 'Philippines': 53, 'Poland': 54, 'Portugal': 55, 'Puerto Rico': 56, 'Romania': 57, 'Russia': 58, 'Serbia': 59, 'Slovakia': 60, 'Slovenia': 61, 'South Africa': 62, 'South Korea': 63, 'Spain': 64, 'Sweden': 65, 'Switzerland': 66, 'Syria': 67, 'Tanzania': 68, 'Thailand': 69, 'Tunisia': 70, 'Turkey': 71, 'Uganda': 72, 'Ukraine': 73, 'United Kingdom': 74, 'United States': 75, 'Uruguay': 76, 'Uzbekistan': 77, 'Venezuela': 78, 'Vietnam': 79}\n",
    "assert \"\".join(city_vocab) == \"@ '()-./34ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz¬∑√Ä√Å√Ñ√Ö√á√â√ç√é√ì√ñ√ò√ö√ú√ü√†√°√¢√£√§√•√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√∏√π√∫√ª√º√ΩƒÄƒÅƒÉƒÖƒÜƒáƒãƒåƒçƒèƒêƒëƒìƒóƒôƒõƒüƒ†ƒ°ƒ¶ƒßƒ©ƒ™ƒ´ƒ∞ƒ±ƒΩƒæ≈Å≈Ç≈Ñ≈à≈å≈ç≈è≈ë≈ì≈ô≈ö≈õ≈û≈ü≈†≈°≈¢≈£≈•≈©≈´≈¨≈≠≈Ø≈≥≈∫≈ª≈º≈Ω≈æ∆è∆°∆∞…ôÃÑÃß·∏ê·∏ë·∏®·∏©·∏Ø·π¨·π≠·∫î·∫ï·∫ñ·∫°·∫£·∫§·∫ß·∫©·∫Ø·∫±·∫µ·∫ø·ªá·ªã·ªë·ªì·ªô·ª£·ªß·ª©·ª≥·ªπ‚Äò‚Äô\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "üëç\n",
    "Create variable `city_vectorizer` as instance of `ConvVectorizer` whose vocabulary is `city_vocab` and maximal size is the longest city name from `train_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "city_vectorizer = ConvVectorizer(city_vocab, max_size=max(train_df.city.apply(len)))\n",
    "\n",
    "# test\n",
    "assert city_vectorizer.vectorize(\"split\").shape == (199, 38)\n",
    "assert city_vectorizer.vectorize(\"split\").nonzero().tolist() == [[44, 3], [47, 2], [51, 1], [54, 0], [55, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitted vectorized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = city_vectorizer.vectorize_many(train_df.city)\n",
    "y_train = country_vocab.vocabularize(train_df.country)\n",
    "\n",
    "x_val = city_vectorizer.vectorize_many(val_df.city)\n",
    "y_val = country_vocab.vocabularize(val_df.country)\n",
    "\n",
    "x_test = city_vectorizer.vectorize_many(test_df.city)\n",
    "y_test = country_vocab.vocabularize(test_df.country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights\n",
    "\n",
    "üëç\n",
    "Create variable `class_counts` as dictionary whose keys are countries and values are frequencies of countries in `train_df` dataframe.\n",
    "Order counts by order of their appearance in `country_vocab` vocabulary.\n",
    "\n",
    "Create variable `y_weight_tensor` whose value is \n",
    "$$ \\frac{1}{log(freq(country))} $$\n",
    "where $ freq(country) $ is frequency of country in `train_df` dataframe.\n",
    "\n",
    "Order weights by order of their appearance in `country_vocab` vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class frequencies and weights\n",
    "\n",
    "# test\n",
    "\n",
    "assert y_weight_tensor.shape == (80,)\n",
    "assert torch.allclose(y_weight_tensor[:5], torch.tensor([0.2813, 0.2132, 0.2396, 0.1849, 0.2749]), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting frequencies and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Class frequencies\")\n",
    "plt.barh(list(class_counts), y_freq_tensor)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Class weights\")\n",
    "plt.barh(list(class_counts), y_weight_tensor)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: CityClassifier\n",
    "\n",
    "üëç Create `CityClassifier` class as MLP with single hidden layer. Methods are:\n",
    "* `__init__(self, in_channel, in_dim, conv_channels, conv_kernel_sizes, conv_strides, conv_activation, out_dim)` initializes perceptron with\n",
    "* input channel (the first input channel size - row number of one-hot matrix)\n",
    "* input dimension (column number of one-hot matrix)\n",
    "* list of 1D convolution output/input channels, kernel sizes and strides, \n",
    "* convolution activation function.\n",
    "* output dimension (should be the number of output classes)\n",
    "\n",
    "For example, \n",
    "`__init__(in_channel=10, in_dim=15, conv_channels=[16, 32, 64], conv_kernel_sizes=[2, 3, 2], conv_strides=[1, 2, 1], conv_activation, out_dim=6)`\n",
    "will create three 1D convolution layers with activations after each:\n",
    "* conv1 with 10 input channels (in_channel argument), 16 output channels, kernel size 2, stride 1 following by activation,\n",
    "* conv2 with 16 input channels, 32 output channels, kernel size 3, stride 2 followed by activation,\n",
    "* conv3 with 32 input channels, 64 output channels, kernel size 2, stride 1 (outputs feature vector) followed by activation.\n",
    "\n",
    "and single fully connected layer fc:\n",
    "* with input size X (size of flattened output features vector from the last convolutional layer, must be computed by applying convolutions on `in_dim`)\n",
    "* and output size 6 (this is the number of output classes)\n",
    "\n",
    "\n",
    "* `forward(self, x_in, apply_softmax=False)` for given input `x_in` makes forward step and eventually applies softmax on output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, in_dim, conv_channels, conv_kernel_sizes, conv_strides, conv_activation, out_dim):\n",
    "        assert len(conv_channels) == len(conv_kernel_sizes) == len(conv_strides)        \n",
    "        super(CityClassifier, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        pass\n",
    "\n",
    "\n",
    "# test\n",
    "StepByStep.set_seed(96)        \n",
    "model = CityClassifier(10, 15, [16, 32, 64], [2, 3, 2], [1, 2, 1], nn.ReLU, 6)\n",
    "x_in = torch.rand(size=(2,10,15))\n",
    "model(x_in)\n",
    "\n",
    "assert torch.allclose(model(x_in), torch.tensor([[-0.0039,  0.0378,  0.0248,  0.0332, -0.0369, -0.0778],\n",
    "                                                 [ 0.0006,  0.0397,  0.0128,  0.0444, -0.0118, -0.0676]]), atol=1e-4)\n",
    "del model, x_in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepByStep.set_seed(96)\n",
    "\n",
    "model = CityClassifier(in_channel=len(city_vocab), \n",
    "                       in_dim=city_vectorizer.max_size, \n",
    "                       conv_channels=[256, 256, 256, 256], \n",
    "                       conv_kernel_sizes=[3, 3, 3, 3], \n",
    "                       conv_strides=[1, 2, 2, 1], \n",
    "                       conv_activation=nn.ReLU,\n",
    "                       out_dim=len(country_vocab))\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=y_weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_loader = DataLoader(CityDataset(x_train, y_train), batch_size=64, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(CityDataset(x_val, y_val), batch_size=64, drop_last=True, shuffle=True)\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer, scheduler)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "\n",
    "sbs.train_by_loss_change(1e-3)\n",
    "\n",
    "sbs.plot_losses(ylog=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "(sbs.predict(x_test).argmax(dim=1) == y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sbs.model(x_test, apply_softmax=True).argmax(dim=1)\n",
    "\n",
    "labels = list(country_vocab)\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "conf_df = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "#conf_df[conf_df==0] = \"\"\n",
    "\n",
    "sns.heatmap(conf_df, annot=True, cbar=None, cmap=\"GnBu\", fmt=\"d\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\"), \n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()\n",
    "#confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_country(city, model, city_vectorizer, country_vocab):\n",
    "    x = city_vectorizer.vectorize(city).unsqueeze(dim=0)\n",
    "    y_pred = model(x, apply_softmax=True)\n",
    "    i = y_pred.argmax(dim=1).item()\n",
    "    country = country_vocab.inv[i]\n",
    "    return country\n",
    "\n",
    "predict_country(\"Split\", sbs.model, city_vectorizer, country_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_country(city, model, city_vectorizer, country_vocab, k=5):\n",
    "    x = city_vectorizer.vectorize(city).unsqueeze(dim=0)\n",
    "    y_pred = model(x, apply_softmax=True)\n",
    "\n",
    "    probs, indices = torch.topk(y_pred, k=k)\n",
    "    probs = probs.squeeze().tolist()\n",
    "    indices = indices.squeeze().tolist()\n",
    "\n",
    "    nationalities = {}\n",
    "    print(f\"Top {k} predictions:\")\n",
    "    for i, p in zip(indices, probs):\n",
    "        country = country_vocab.inv[i]\n",
    "        nationalities[country] = p\n",
    "        print(f\"{city} => {country} (p={p:.3f})\")\n",
    "\n",
    "    return nationalities\n",
    "\n",
    "predict_topk_country(\"Jarofski\", sbs.model, city_vectorizer, country_vocab, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "üëç for some city, plot its vectorization using heatmap. One dimension should have city letters ticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_city = city_vectorizer.vectorize(\"Split\")\n",
    "vectorized_city = torch.tensor(vectorized_city).unsqueeze(0)\n",
    "\n",
    "plt.figure(figsize=(4, 30))\n",
    "plt.title(\"City Vector\")\n",
    "sns.heatmap(vectorized_city.squeeze(0), yticklabels=city_vocab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç pass vectorized city through all classifier's convolutional layers and plot their vectorization using heatmap. The last vector is feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_vec = vectorized_city\n",
    "\n",
    "for model in model.convnet:\n",
    "    interim_vec = model(interim_vec)\n",
    "    plt.title(str(model))\n",
    "    sns.heatmap(interim_vec.squeeze(0).detach())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç pass feature vector through classifier's fully connected layer and plot their vectorization using heatmap. One dimension should have country ticks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vec = F.softmax(model.fc(interim_vec.flatten(start_dim=1)), dim=1)\n",
    "\n",
    "plt.figure(figsize=(20, 1))\n",
    "plt.title(str(model.fc))\n",
    "sns.heatmap(out_vec.detach(), xticklabels=country_vocab)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
