{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"worldcities_with_splits.csv.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/lectures/p03/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "        \n",
    "name = \"worldcities_with_splits.csv\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Cities with a Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityVectorizer(object):\n",
    "    def __init__(self, city_vocab, country_vocab):\n",
    "        self.city_vocab = city_vocab\n",
    "        self.country_vocab = country_vocab\n",
    "\n",
    "    def vectorize(self, city):\n",
    "        vocab = self.city_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        for token in city:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, city_df):\n",
    "        city_vocab = Vocabulary(unk_token=\"@\")\n",
    "        country_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in city_df.iterrows():\n",
    "            for letter in row.city:\n",
    "                city_vocab.add_token(letter)\n",
    "            country_vocab.add_token(row.country)\n",
    "\n",
    "        return cls(city_vocab, country_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        city_vocab = Vocabulary.from_serializable(contents['city_vocab'])\n",
    "        country_vocab =  Vocabulary.from_serializable(contents['country_vocab'])\n",
    "        return cls(city_vocab=city_vocab, country_vocab=country_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'city_vocab': self.city_vocab.to_serializable(),\n",
    "                'country_vocab': self.country_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityDataset(Dataset):\n",
    "    def __init__(self, city_df, vectorizer):\n",
    "        self.city_df = city_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.city_df[self.city_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.city_df[self.city_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.city_df[self.city_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = city_df.country.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.country_vocab.lookup_token(item[0])\n",
    "\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, city_csv):\n",
    "        city_df = pd.read_csv(city_csv)\n",
    "        train_city_df = city_df[city_df.split=='train']\n",
    "        return cls(city_df, CityVectorizer.from_dataframe(train_city_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, city_csv, vectorizer_filepath):\n",
    "        city_df = pd.read_csv(city_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(city_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CityVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        city_vector = self._vectorizer.vectorize(row.city)\n",
    "\n",
    "        country_index = self._vectorizer.country_vocab.lookup_token(row.country)\n",
    "\n",
    "        return {'x_city': city_vector,\n",
    "                'y_country': country_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: CityClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CityClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector\n",
    "\n",
    "\n",
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=2, output_dim=3, \n",
    "                 num_hidden_layers=1, hidden_activation=torch.nn.ReLU):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.module_list = torch.nn.ModuleList()\n",
    "        \n",
    "        interim_input_dim = input_dim\n",
    "        interim_output_dim = hidden_dim\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.module_list.append(torch.nn.Linear(interim_input_dim, interim_output_dim))\n",
    "            self.module_list.append(hidden_activation())\n",
    "            interim_input_dim = interim_output_dim\n",
    "\n",
    "        self.fc_final = nn.Linear(interim_input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "        output = self.fc_final(x)\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t./vectorizer.json\n",
      "\t./model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    city_csv=\"worldcities_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\".\",\n",
    "    # Model hyper parameters\n",
    "    hidden_dim=700,\n",
    "    # Training  hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(f\"Using CUDA: {args.cuda}\")\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fresh!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptron(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Linear(in_features=211, out_features=700, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=700, out_features=700, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc_final): Linear(in_features=700, out_features=174, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    print(\"Reloading!\")\n",
    "    dataset = CityDataset.load_dataset_and_load_vectorizer(args.city_csv,   \n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    print(\"Creating fresh!\")\n",
    "    dataset = CityDataset.load_dataset_and_make_vectorizer(args.city_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = CityClassifier(input_dim=len(vectorizer.city_vocab), \n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            output_dim=len(vectorizer.country_vocab))\n",
    "\n",
    "classifier = MultilayerPerceptron(input_dim=len(vectorizer.city_vocab), \n",
    "                                  num_hidden_layers=2,\n",
    "                                  hidden_dim=args.hidden_dim, \n",
    "                                  output_dim=len(vectorizer.country_vocab))\n",
    "\n",
    "classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060883d040ff4b629de3420d8946c980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c668c174f324e99af2e32a66fca9d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e3bd16d7a847208cb2dd67c5c2038f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting loop\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                 total=args.num_epochs,\n",
    "                 position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                 total=dataset.get_num_batches(args.batch_size), \n",
    "                 position=1, \n",
    "                 leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "               total=dataset.get_num_batches(args.batch_size), \n",
    "               position=1, \n",
    "               leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_city'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_country'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_country'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_city'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_country'])\n",
    "            loss_t = loss.to(\"cpu\").item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_country'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_city'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_country'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_country'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.109314627507152;\n",
      "Test Accuracy: 19.653799019607842\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {train_state['test_loss']};\")\n",
    "print(f\"Test Accuracy: {train_state['test_acc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_country(city, classifier, vectorizer):\n",
    "    vectorized_city = vectorizer.vectorize(city)\n",
    "    vectorized_city = torch.tensor(vectorized_city).view(1, -1)\n",
    "    result = classifier(vectorized_city, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_country = vectorizer.country_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'country': predicted_country, 'probability': probability_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_country(city, classifier, vectorizer, k=5):\n",
    "    vectorized_city = vectorizer.vectorize(city)\n",
    "    vectorized_city = torch.tensor(vectorized_city).view(1, -1)\n",
    "    prediction_vector = classifier(vectorized_city, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "    \n",
    "    # returned size is 1,k\n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "    \n",
    "    results = []\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        country = vectorizer.country_vocab.lookup_index(index)\n",
    "        results.append({'country': country, \n",
    "                        'probability': prob_value})\n",
    "    \n",
    "    return results\n",
    "\n",
    "def topk_predictions(city, k=5):\n",
    "    if k > len(vectorizer.country_vocab):\n",
    "        print(\"Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)\")\n",
    "        k = len(vectorizer.country_vocab)\n",
    "    predictions = predict_topk_country(city, classifier, vectorizer, k=k)\n",
    "    \n",
    "    print(f\"Top {k} predictions:\")\n",
    "    print(\"===================\")\n",
    "    for prediction in predictions:\n",
    "        print(f\"{city} -> {prediction['country']} (p={prediction['probability']:0.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "===================\n",
      "Beograd -> Somalia (p=0.20)\n",
      "Beograd -> Zimbabwe (p=0.15)\n",
      "Beograd -> India (p=0.09)\n",
      "Beograd -> Italy (p=0.06)\n",
      "Beograd -> Colombia (p=0.03)\n"
     ]
    }
   ],
   "source": [
    "topk_predictions(\"Beograd\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@': 0,\n",
       " 'F': 1,\n",
       " 'a': 2,\n",
       " 'r': 3,\n",
       " 'ā': 4,\n",
       " 'h': 5,\n",
       " 'K': 6,\n",
       " 'b': 7,\n",
       " 'u': 8,\n",
       " 'l': 9,\n",
       " 'T': 10,\n",
       " 'q': 11,\n",
       " 'n': 12,\n",
       " 'Z': 13,\n",
       " 'g': 14,\n",
       " 'ū': 15,\n",
       " ' ': 16,\n",
       " 'S': 17,\n",
       " 'B': 18,\n",
       " 'm': 19,\n",
       " 'y': 20,\n",
       " 'A': 21,\n",
       " 'd': 22,\n",
       " 'k': 23,\n",
       " 'ō': 24,\n",
       " 'P': 25,\n",
       " '-': 26,\n",
       " 'e': 27,\n",
       " 'ī': 28,\n",
       " 'L': 29,\n",
       " 's': 30,\n",
       " 'G': 31,\n",
       " 't': 32,\n",
       " 'z': 33,\n",
       " 'J': 34,\n",
       " 'N': 35,\n",
       " 'M': 36,\n",
       " 'f': 37,\n",
       " 'ẕ': 38,\n",
       " 'C': 39,\n",
       " 'ē': 40,\n",
       " 'j': 41,\n",
       " '‘': 42,\n",
       " 'H': 43,\n",
       " 'ṯ': 44,\n",
       " '̲': 45,\n",
       " 'Q': 46,\n",
       " 'o': 47,\n",
       " 'w': 48,\n",
       " 'ç': 49,\n",
       " 'v': 50,\n",
       " 'ë': 51,\n",
       " 'i': 52,\n",
       " 'R': 53,\n",
       " 'p': 54,\n",
       " 'D': 55,\n",
       " 'V': 56,\n",
       " 'c': 57,\n",
       " 'E': 58,\n",
       " 'O': 59,\n",
       " 'Y': 60,\n",
       " 'I': 61,\n",
       " '’': 62,\n",
       " 'è': 63,\n",
       " 'ï': 64,\n",
       " 'é': 65,\n",
       " 'â': 66,\n",
       " 'U': 67,\n",
       " 'í': 68,\n",
       " 'x': 69,\n",
       " 'X': 70,\n",
       " '.': 71,\n",
       " 'á': 72,\n",
       " 'ú': 73,\n",
       " 'Á': 74,\n",
       " 'ñ': 75,\n",
       " 'ó': 76,\n",
       " 'ü': 77,\n",
       " 'W': 78,\n",
       " 'ö': 79,\n",
       " 'ä': 80,\n",
       " 'ş': 81,\n",
       " 'ğ': 82,\n",
       " 'ı': 83,\n",
       " 'ə': 84,\n",
       " 'Ş': 85,\n",
       " 'İ': 86,\n",
       " 'Ə': 87,\n",
       " 'É': 88,\n",
       " 'à': 89,\n",
       " 'ž': 90,\n",
       " 'ć': 91,\n",
       " 'š': 92,\n",
       " 'č': 93,\n",
       " 'Č': 94,\n",
       " 'ã': 95,\n",
       " 'ô': 96,\n",
       " 'ê': 97,\n",
       " 'õ': 98,\n",
       " 'Ó': 99,\n",
       " 'Í': 100,\n",
       " 'À': 101,\n",
       " 'ò': 102,\n",
       " 'ì': 103,\n",
       " '/': 104,\n",
       " \"'\": 105,\n",
       " '3': 106,\n",
       " '4': 107,\n",
       " 'Î': 108,\n",
       " 'Ü': 109,\n",
       " 'Ž': 110,\n",
       " 'ý': 111,\n",
       " 'ě': 112,\n",
       " 'Ú': 113,\n",
       " 'ř': 114,\n",
       " 'ň': 115,\n",
       " 'Š': 116,\n",
       " 'Ř': 117,\n",
       " 'ť': 118,\n",
       " 'ď': 119,\n",
       " 'ŭ': 120,\n",
       " 'ů': 121,\n",
       " 'ø': 122,\n",
       " 'Ḩ': 123,\n",
       " 'ţ': 124,\n",
       " 'Ţ': 125,\n",
       " 'ḑ': 126,\n",
       " 'Ḑ': 127,\n",
       " 'ḩ': 128,\n",
       " 'Ā': 129,\n",
       " 'ð': 130,\n",
       " 'Ä': 131,\n",
       " 'å': 132,\n",
       " 'û': 133,\n",
       " 'œ': 134,\n",
       " 'ÿ': 135,\n",
       " 'î': 136,\n",
       " 'ß': 137,\n",
       " 'Ö': 138,\n",
       " '(': 139,\n",
       " ')': 140,\n",
       " 'ḯ': 141,\n",
       " 'ő': 142,\n",
       " 'æ': 143,\n",
       " 'ḥ': 144,\n",
       " 'ṭ': 145,\n",
       " 'Ṭ': 146,\n",
       " 'Ẕ': 147,\n",
       " 'ẖ': 148,\n",
       " 'ù': 149,\n",
       " 'Ō': 150,\n",
       " 'Ī': 151,\n",
       " 'ļ': 152,\n",
       " 'Ķ': 153,\n",
       " 'ė': 154,\n",
       " 'ų': 155,\n",
       " '̈': 156,\n",
       " 'Ż': 157,\n",
       " 'ħ': 158,\n",
       " 'ż': 159,\n",
       " 'ġ': 160,\n",
       " 'ċ': 161,\n",
       " 'Ġ': 162,\n",
       " 'Ñ': 163,\n",
       " 'ă': 164,\n",
       " 'ĭ': 165,\n",
       " 'ḍ': 166,\n",
       " '̇': 167,\n",
       " '̄': 168,\n",
       " 'ṅ': 169,\n",
       " 'ṣ': 170,\n",
       " 'ṇ': 171,\n",
       " 'ŏ': 172,\n",
       " 'Ŏ': 173,\n",
       " 'Å': 174,\n",
       " 'Ø': 175,\n",
       " 'ś': 176,\n",
       " 'ę': 177,\n",
       " 'Ś': 178,\n",
       " 'ą': 179,\n",
       " 'Ł': 180,\n",
       " 'ł': 181,\n",
       " 'ń': 182,\n",
       " 'ź': 183,\n",
       " '̧': 184,\n",
       " 'đ': 185,\n",
       " 'Ć': 186,\n",
       " 'ľ': 187,\n",
       " 'Ľ': 188,\n",
       " 'Ç': 189,\n",
       " ',': 190,\n",
       " 'ả': 191,\n",
       " 'ư': 192,\n",
       " 'ơ': 193,\n",
       " 'ĩ': 194,\n",
       " 'ũ': 195,\n",
       " 'Đ': 196,\n",
       " 'ị': 197,\n",
       " 'ẩ': 198,\n",
       " 'ủ': 199,\n",
       " 'ồ': 200,\n",
       " 'ế': 201,\n",
       " 'ầ': 202,\n",
       " 'ộ': 203,\n",
       " 'ệ': 204,\n",
       " 'ố': 205,\n",
       " 'ắ': 206,\n",
       " 'ạ': 207,\n",
       " 'ỳ': 208,\n",
       " 'ỹ': 209,\n",
       " 'ứ': 210}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = dataset.get_vectorizer()\n",
    "vec.city_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Afghanistan': 0,\n",
       " 'Albania': 1,\n",
       " 'Algeria': 2,\n",
       " 'Andorra': 3,\n",
       " 'Angola': 4,\n",
       " 'Argentina': 5,\n",
       " 'Armenia': 6,\n",
       " 'Australia': 7,\n",
       " 'Austria': 8,\n",
       " 'Azerbaijan': 9,\n",
       " 'Bangladesh': 10,\n",
       " 'Belarus': 11,\n",
       " 'Belgium': 12,\n",
       " 'Belize': 13,\n",
       " 'Benin': 14,\n",
       " 'Bhutan': 15,\n",
       " 'Bolivia': 16,\n",
       " 'Bosnia And Herzegovina': 17,\n",
       " 'Botswana': 18,\n",
       " 'Brazil': 19,\n",
       " 'Brunei': 20,\n",
       " 'Bulgaria': 21,\n",
       " 'Burkina Faso': 22,\n",
       " 'Burundi': 23,\n",
       " 'Cabo Verde': 24,\n",
       " 'Cambodia': 25,\n",
       " 'Cameroon': 26,\n",
       " 'Canada': 27,\n",
       " 'Central African Republic': 28,\n",
       " 'Chad': 29,\n",
       " 'Chile': 30,\n",
       " 'China': 31,\n",
       " 'Colombia': 32,\n",
       " 'Congo (Brazzaville)': 33,\n",
       " 'Congo (Kinshasa)': 34,\n",
       " 'Costa Rica': 35,\n",
       " 'Croatia': 36,\n",
       " 'Cuba': 37,\n",
       " 'Cyprus': 38,\n",
       " 'Czechia': 39,\n",
       " \"Côte d'Ivoire\": 40,\n",
       " 'Denmark': 41,\n",
       " 'Dominican Republic': 42,\n",
       " 'Ecuador': 43,\n",
       " 'Egypt': 44,\n",
       " 'El Salvador': 45,\n",
       " 'Equatorial Guinea': 46,\n",
       " 'Eritrea': 47,\n",
       " 'Estonia': 48,\n",
       " 'Ethiopia': 49,\n",
       " 'Faroe Islands': 50,\n",
       " 'Fiji': 51,\n",
       " 'Finland': 52,\n",
       " 'France': 53,\n",
       " 'French Guiana': 54,\n",
       " 'Gabon': 55,\n",
       " 'Georgia': 56,\n",
       " 'Germany': 57,\n",
       " 'Ghana': 58,\n",
       " 'Greece': 59,\n",
       " 'Greenland': 60,\n",
       " 'Guatemala': 61,\n",
       " 'Guinea': 62,\n",
       " 'Guinea-Bissau': 63,\n",
       " 'Guyana': 64,\n",
       " 'Haiti': 65,\n",
       " 'Honduras': 66,\n",
       " 'Hungary': 67,\n",
       " 'Iceland': 68,\n",
       " 'India': 69,\n",
       " 'Indonesia': 70,\n",
       " 'Iran': 71,\n",
       " 'Iraq': 72,\n",
       " 'Ireland': 73,\n",
       " 'Israel': 74,\n",
       " 'Italy': 75,\n",
       " 'Jamaica': 76,\n",
       " 'Japan': 77,\n",
       " 'Jordan': 78,\n",
       " 'Kazakhstan': 79,\n",
       " 'Kenya': 80,\n",
       " 'Kosovo': 81,\n",
       " 'Kyrgyzstan': 82,\n",
       " 'Laos': 83,\n",
       " 'Latvia': 84,\n",
       " 'Lebanon': 85,\n",
       " 'Lesotho': 86,\n",
       " 'Liberia': 87,\n",
       " 'Libya': 88,\n",
       " 'Liechtenstein': 89,\n",
       " 'Lithuania': 90,\n",
       " 'Luxembourg': 91,\n",
       " 'Macedonia': 92,\n",
       " 'Madagascar': 93,\n",
       " 'Malawi': 94,\n",
       " 'Malaysia': 95,\n",
       " 'Maldives': 96,\n",
       " 'Mali': 97,\n",
       " 'Malta': 98,\n",
       " 'Mauritania': 99,\n",
       " 'Mexico': 100,\n",
       " 'Moldova': 101,\n",
       " 'Mongolia': 102,\n",
       " 'Montenegro': 103,\n",
       " 'Morocco': 104,\n",
       " 'Mozambique': 105,\n",
       " 'Myanmar': 106,\n",
       " 'Namibia': 107,\n",
       " 'Nepal': 108,\n",
       " 'Netherlands': 109,\n",
       " 'New Zealand': 110,\n",
       " 'Nicaragua': 111,\n",
       " 'Niger': 112,\n",
       " 'Nigeria': 113,\n",
       " 'North Korea': 114,\n",
       " 'Norway': 115,\n",
       " 'Oman': 116,\n",
       " 'Pakistan': 117,\n",
       " 'Panama': 118,\n",
       " 'Papua New Guinea': 119,\n",
       " 'Paraguay': 120,\n",
       " 'Peru': 121,\n",
       " 'Philippines': 122,\n",
       " 'Poland': 123,\n",
       " 'Portugal': 124,\n",
       " 'Puerto Rico': 125,\n",
       " 'Qatar': 126,\n",
       " 'Romania': 127,\n",
       " 'Russia': 128,\n",
       " 'Rwanda': 129,\n",
       " 'Samoa': 130,\n",
       " 'San Marino': 131,\n",
       " 'Sao Tome And Principe': 132,\n",
       " 'Saudi Arabia': 133,\n",
       " 'Senegal': 134,\n",
       " 'Serbia': 135,\n",
       " 'Slovakia': 136,\n",
       " 'Slovenia': 137,\n",
       " 'Solomon Islands': 138,\n",
       " 'Somalia': 139,\n",
       " 'South Africa': 140,\n",
       " 'South Korea': 141,\n",
       " 'South Sudan': 142,\n",
       " 'Spain': 143,\n",
       " 'Sri Lanka': 144,\n",
       " 'Sudan': 145,\n",
       " 'Suriname': 146,\n",
       " 'Swaziland': 147,\n",
       " 'Sweden': 148,\n",
       " 'Switzerland': 149,\n",
       " 'Syria': 150,\n",
       " 'Taiwan': 151,\n",
       " 'Tajikistan': 152,\n",
       " 'Tanzania': 153,\n",
       " 'Thailand': 154,\n",
       " 'The Gambia': 155,\n",
       " 'Timor-Leste': 156,\n",
       " 'Togo': 157,\n",
       " 'Trinidad And Tobago': 158,\n",
       " 'Tunisia': 159,\n",
       " 'Turkey': 160,\n",
       " 'Turkmenistan': 161,\n",
       " 'Uganda': 162,\n",
       " 'Ukraine': 163,\n",
       " 'United Arab Emirates': 164,\n",
       " 'United Kingdom': 165,\n",
       " 'United States': 166,\n",
       " 'Uruguay': 167,\n",
       " 'Uzbekistan': 168,\n",
       " 'Venezuela': 169,\n",
       " 'Vietnam': 170,\n",
       " 'Yemen': 171,\n",
       " 'Zambia': 172,\n",
       " 'Zimbabwe': 173}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.country_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
