{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"frankenstein_with_splits.csv.bz2\"\n",
    "if not os.path.exists(download_name):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/p04/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "name = \"frankenstein_with_splits.csv\"\n",
    "if not os.path.exists(name):\n",
    "    import bz2\n",
    "    with open(download_name, 'rb') as bzf, open(name, 'wb') as fp:\n",
    "        fp.write(bz2.decompress(bzf.read()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Embeddings with Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token, \n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_vocab (Vocabulary): maps words to integers\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the CBOWVectorizer\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "            \n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        cbow_vocab = \\\n",
    "            Vocabulary.from_serializable(contents['cbow_vocab'])\n",
    "        return cls(cbow_vocab=cbow_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'cbow_vocab': self.cbow_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CBOWVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "        \n",
    "        self.train_df = self.cbow_df[self.cbow_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split=='train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWClassifier(nn.Module): # Simplified cbow Model\n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocabulary_size (int): number of vocabulary items, controls the\n",
    "                number of embeddings and prediction vector size\n",
    "            embedding_size (int): size of the embeddings\n",
    "            padding_idx (int): default 0; Embedding will not use this index\n",
    "        \"\"\"\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t./vectorizer.json\n",
      "\t./model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    cbow_csv=\"frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\".\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and loading vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_load_vectorizer(args.cbow_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    \n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab), \n",
    "                            embedding_size=args.embedding_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or\n",
      "the\n",
      "modern\n",
      "by\n",
      "mary\n",
      "wollstonecraft\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<MASK>': 0,\n",
       " '<UNK>': 1,\n",
       " ',': 2,\n",
       " 'or': 3,\n",
       " 'the': 4,\n",
       " 'frankenstein': 5,\n",
       " 'modern': 6,\n",
       " 'prometheus': 7,\n",
       " 'by': 8,\n",
       " 'mary': 9,\n",
       " 'wollstonecraft': 10,\n",
       " 'godwin': 11,\n",
       " 'shelley': 12,\n",
       " 'letter': 13,\n",
       " 'st': 14,\n",
       " '.': 15,\n",
       " 'petersburgh': 16,\n",
       " 'dec': 17,\n",
       " 'th': 18,\n",
       " 'to': 19,\n",
       " 'mrs': 20,\n",
       " 'saville': 21,\n",
       " 'england': 22,\n",
       " 'you': 23,\n",
       " 'will': 24,\n",
       " 'rejoice': 25,\n",
       " 'hear': 26,\n",
       " 'that': 27,\n",
       " 'no': 28,\n",
       " 'disaster': 29,\n",
       " 'has': 30,\n",
       " 'accompanied': 31,\n",
       " 'commencement': 32,\n",
       " 'of': 33,\n",
       " 'an': 34,\n",
       " 'enterprise': 35,\n",
       " 'which': 36,\n",
       " 'have': 37,\n",
       " 'regarded': 38,\n",
       " 'with': 39,\n",
       " 'such': 40,\n",
       " 'evil': 41,\n",
       " 'forebodings': 42,\n",
       " '': 43,\n",
       " nan: 44,\n",
       " 'arrived': 45,\n",
       " 'here': 46,\n",
       " 'yesterday': 47,\n",
       " 'i': 48,\n",
       " 'and': 49,\n",
       " 'my': 50,\n",
       " 'first': 51,\n",
       " 'task': 52,\n",
       " 'is': 53,\n",
       " 'assure': 54,\n",
       " 'dear': 55,\n",
       " 'sister': 56,\n",
       " 'welfare': 57,\n",
       " 'increasing': 58,\n",
       " 'confidence': 59,\n",
       " 'in': 60,\n",
       " 'success': 61,\n",
       " 'undertaking': 62,\n",
       " 'am': 63,\n",
       " 'already': 64,\n",
       " 'far': 65,\n",
       " 'north': 66,\n",
       " 'london': 67,\n",
       " 'as': 68,\n",
       " 'walk': 69,\n",
       " 'streets': 70,\n",
       " 'feel': 71,\n",
       " 'a': 72,\n",
       " 'cold': 73,\n",
       " 'northern': 74,\n",
       " 'breeze': 75,\n",
       " 'play': 76,\n",
       " 'upon': 77,\n",
       " 'cheeks': 78,\n",
       " 'braces': 79,\n",
       " 'nerves': 80,\n",
       " 'fills': 81,\n",
       " 'me': 82,\n",
       " 'delight': 83,\n",
       " 'understand': 84,\n",
       " 'this': 85,\n",
       " 'do': 86,\n",
       " 'feeling': 87,\n",
       " '?': 88,\n",
       " 'travelled': 89,\n",
       " 'from': 90,\n",
       " 'regions': 91,\n",
       " 'towards': 92,\n",
       " 'advancing': 93,\n",
       " 'gives': 94,\n",
       " 'foretaste': 95,\n",
       " 'those': 96,\n",
       " 'icy': 97,\n",
       " 'climes': 98,\n",
       " 'wind': 99,\n",
       " 'inspirited': 100,\n",
       " 'promise': 101,\n",
       " 'daydreams': 102,\n",
       " 'become': 103,\n",
       " 'more': 104,\n",
       " 'fervent': 105,\n",
       " 'vivid': 106,\n",
       " 'try': 107,\n",
       " 'vain': 108,\n",
       " 'be': 109,\n",
       " 'persuaded': 110,\n",
       " 'pole': 111,\n",
       " 'seat': 112,\n",
       " 'frost': 113,\n",
       " 'desolation': 114,\n",
       " 'it': 115,\n",
       " 'ever': 116,\n",
       " 'presents': 117,\n",
       " 'itself': 118,\n",
       " 'imagination': 119,\n",
       " 'region': 120,\n",
       " 'beauty': 121,\n",
       " 'margaret': 122,\n",
       " 'there': 123,\n",
       " 'sun': 124,\n",
       " 'forever': 125,\n",
       " 'visible': 126,\n",
       " 'its': 127,\n",
       " 'broad': 128,\n",
       " 'disk': 129,\n",
       " 'just': 130,\n",
       " 'skirting': 131,\n",
       " 'horizon': 132,\n",
       " 'diffusing': 133,\n",
       " 'perpetual': 134,\n",
       " 'splendour': 135,\n",
       " 'for': 136,\n",
       " 'your': 137,\n",
       " 'leave': 138,\n",
       " 'put': 139,\n",
       " 'some': 140,\n",
       " 'trust': 141,\n",
       " 'preceding': 142,\n",
       " 'navigators': 143,\n",
       " 'snow': 144,\n",
       " 'are': 145,\n",
       " 'banished': 146,\n",
       " 'sailing': 147,\n",
       " 'over': 148,\n",
       " 'calm': 149,\n",
       " 'sea': 150,\n",
       " 'we': 151,\n",
       " 'may': 152,\n",
       " 'wafted': 153,\n",
       " 'land': 154,\n",
       " 'surpassing': 155,\n",
       " 'wonders': 156,\n",
       " 'every': 157,\n",
       " 'hitherto': 158,\n",
       " 'discovered': 159,\n",
       " 'on': 160,\n",
       " 'habitable': 161,\n",
       " 'globe': 162,\n",
       " 'productions': 163,\n",
       " 'features': 164,\n",
       " 'without': 165,\n",
       " 'example': 166,\n",
       " 'phenomena': 167,\n",
       " 'heavenly': 168,\n",
       " 'bodies': 169,\n",
       " 'undoubtedly': 170,\n",
       " 'undiscovered': 171,\n",
       " 'solitudes': 172,\n",
       " 'not': 173,\n",
       " 'what': 174,\n",
       " 'expected': 175,\n",
       " 'country': 176,\n",
       " 'eternal': 177,\n",
       " 'light': 178,\n",
       " 'discover': 179,\n",
       " 'wondrous': 180,\n",
       " 'power': 181,\n",
       " 'attracts': 182,\n",
       " 'needle': 183,\n",
       " 'regulate': 184,\n",
       " 'thousand': 185,\n",
       " 'celestial': 186,\n",
       " 'observations': 187,\n",
       " 'require': 188,\n",
       " 'only': 189,\n",
       " 'voyage': 190,\n",
       " 'render': 191,\n",
       " 'their': 192,\n",
       " 'seeming': 193,\n",
       " 'eccentricities': 194,\n",
       " 'consistent': 195,\n",
       " 'shall': 196,\n",
       " 'satiate': 197,\n",
       " 'ardent': 198,\n",
       " 'curiosity': 199,\n",
       " 'sight': 200,\n",
       " 'part': 201,\n",
       " 'world': 202,\n",
       " 'never': 203,\n",
       " 'before': 204,\n",
       " 'visited': 205,\n",
       " 'tread': 206,\n",
       " 'imprinted': 207,\n",
       " 'foot': 208,\n",
       " 'man': 209,\n",
       " 'enticements': 210,\n",
       " 'these': 211,\n",
       " 'they': 212,\n",
       " 'sufficient': 213,\n",
       " 'conquer': 214,\n",
       " 'all': 215,\n",
       " 'fear': 216,\n",
       " 'danger': 217,\n",
       " 'death': 218,\n",
       " 'induce': 219,\n",
       " 'commence': 220,\n",
       " 'laborious': 221,\n",
       " 'joy': 222,\n",
       " 'child': 223,\n",
       " 'feels': 224,\n",
       " 'when': 225,\n",
       " 'he': 226,\n",
       " 'embarks': 227,\n",
       " 'little': 228,\n",
       " 'boat': 229,\n",
       " 'his': 230,\n",
       " 'holiday': 231,\n",
       " 'mates': 232,\n",
       " 'expedition': 233,\n",
       " 'discovery': 234,\n",
       " 'up': 235,\n",
       " 'native': 236,\n",
       " 'river': 237,\n",
       " 'supposing': 238,\n",
       " 'but': 239,\n",
       " 'conjectures': 240,\n",
       " 'false': 241,\n",
       " 'cannot': 242,\n",
       " 'contest': 243,\n",
       " 'inestimable': 244,\n",
       " 'benefit': 245,\n",
       " 'confer': 246,\n",
       " 'mankind': 247,\n",
       " 'last': 248,\n",
       " 'generation': 249,\n",
       " 'discovering': 250,\n",
       " 'passage': 251,\n",
       " 'near': 252,\n",
       " 'countries': 253,\n",
       " 'reach': 254,\n",
       " 'at': 255,\n",
       " 'present': 256,\n",
       " 'so': 257,\n",
       " 'many': 258,\n",
       " 'months': 259,\n",
       " 'requisite': 260,\n",
       " 'ascertaining': 261,\n",
       " 'secret': 262,\n",
       " 'magnet': 263,\n",
       " 'if': 264,\n",
       " 'possible': 265,\n",
       " 'can': 266,\n",
       " 'effected': 267,\n",
       " 'mine': 268,\n",
       " 'reflections': 269,\n",
       " 'dispelled': 270,\n",
       " 'agitation': 271,\n",
       " 'began': 272,\n",
       " 'heart': 273,\n",
       " 'glow': 274,\n",
       " 'enthusiasm': 275,\n",
       " 'elevates': 276,\n",
       " 'heaven': 277,\n",
       " 'nothing': 278,\n",
       " 'contributes': 279,\n",
       " 'much': 280,\n",
       " 'tranquillize': 281,\n",
       " 'mind': 282,\n",
       " 'steady': 283,\n",
       " 'purpose': 284,\n",
       " 'point': 285,\n",
       " 'soul': 286,\n",
       " 'fix': 287,\n",
       " 'intellectual': 288,\n",
       " 'eye': 289,\n",
       " 'been': 290,\n",
       " 'favourite': 291,\n",
       " 'dream': 292,\n",
       " 'early': 293,\n",
       " 'years': 294,\n",
       " 'read': 295,\n",
       " 'ardour': 296,\n",
       " 'accounts': 297,\n",
       " 'various': 298,\n",
       " 'voyages': 299,\n",
       " 'made': 300,\n",
       " 'prospect': 301,\n",
       " 'arriving': 302,\n",
       " 'pacific': 303,\n",
       " 'ocean': 304,\n",
       " 'through': 305,\n",
       " 'seas': 306,\n",
       " 'surround': 307,\n",
       " 'remember': 308,\n",
       " 'history': 309,\n",
       " 'purposes': 310,\n",
       " 'composed': 311,\n",
       " 'whole': 312,\n",
       " 'our': 313,\n",
       " 'good': 314,\n",
       " 'uncle': 315,\n",
       " 'thomas': 316,\n",
       " 'library': 317,\n",
       " 'education': 318,\n",
       " 'was': 319,\n",
       " 'neglected': 320,\n",
       " 'yet': 321,\n",
       " 'passionately': 322,\n",
       " 'fond': 323,\n",
       " 'reading': 324,\n",
       " 'volumes': 325,\n",
       " 'were': 326,\n",
       " 'study': 327,\n",
       " 'day': 328,\n",
       " 'night': 329,\n",
       " 'familiarity': 330,\n",
       " 'them': 331,\n",
       " 'increased': 332,\n",
       " 'regret': 333,\n",
       " 'had': 334,\n",
       " 'felt': 335,\n",
       " 'learning': 336,\n",
       " 'father': 337,\n",
       " 's': 338,\n",
       " 'dying': 339,\n",
       " 'injunction': 340,\n",
       " 'forbidden': 341,\n",
       " 'allow': 342,\n",
       " 'embark': 343,\n",
       " 'seafaring': 344,\n",
       " 'life': 345,\n",
       " 'visions': 346,\n",
       " 'faded': 347,\n",
       " 'perused': 348,\n",
       " 'time': 349,\n",
       " 'poets': 350,\n",
       " 'whose': 351,\n",
       " 'effusions': 352,\n",
       " 'entranced': 353,\n",
       " 'lifted': 354,\n",
       " 'also': 355,\n",
       " 'became': 356,\n",
       " 'poet': 357,\n",
       " 'one': 358,\n",
       " 'year': 359,\n",
       " 'lived': 360,\n",
       " 'paradise': 361,\n",
       " 'own': 362,\n",
       " 'creation': 363,\n",
       " 'imagined': 364,\n",
       " 'might': 365,\n",
       " 'obtain': 366,\n",
       " 'niche': 367,\n",
       " 'temple': 368,\n",
       " 'where': 369,\n",
       " 'names': 370,\n",
       " 'homer': 371,\n",
       " 'shakespeare': 372,\n",
       " 'consecrated': 373,\n",
       " 'well': 374,\n",
       " 'acquainted': 375,\n",
       " 'failure': 376,\n",
       " 'how': 377,\n",
       " 'heavily': 378,\n",
       " 'bore': 379,\n",
       " 'disappointment': 380,\n",
       " 'inherited': 381,\n",
       " 'fortune': 382,\n",
       " 'cousin': 383,\n",
       " 'thoughts': 384,\n",
       " 'turned': 385,\n",
       " 'into': 386,\n",
       " 'channel': 387,\n",
       " 'earlier': 388,\n",
       " 'bent': 389,\n",
       " 'passed': 390,\n",
       " 'six': 391,\n",
       " 'since': 392,\n",
       " 'resolved': 393,\n",
       " 'even': 394,\n",
       " 'now': 395,\n",
       " 'hour': 396,\n",
       " 'dedicated': 397,\n",
       " 'myself': 398,\n",
       " 'great': 399,\n",
       " 'commenced': 400,\n",
       " 'inuring': 401,\n",
       " 'body': 402,\n",
       " 'hardship': 403,\n",
       " 'whale': 404,\n",
       " 'fishers': 405,\n",
       " 'several': 406,\n",
       " 'expeditions': 407,\n",
       " 'voluntarily': 408,\n",
       " 'endured': 409,\n",
       " 'famine': 410,\n",
       " 'thirst': 411,\n",
       " 'want': 412,\n",
       " 'sleep': 413,\n",
       " 'often': 414,\n",
       " 'worked': 415,\n",
       " 'harder': 416,\n",
       " 'than': 417,\n",
       " 'common': 418,\n",
       " 'sailors': 419,\n",
       " 'during': 420,\n",
       " 'devoted': 421,\n",
       " 'nights': 422,\n",
       " 'mathematics': 423,\n",
       " 'theory': 424,\n",
       " 'medicine': 425,\n",
       " 'branches': 426,\n",
       " 'physical': 427,\n",
       " 'science': 428,\n",
       " 'naval': 429,\n",
       " 'adventurer': 430,\n",
       " 'derive': 431,\n",
       " 'greatest': 432,\n",
       " 'practical': 433,\n",
       " 'advantage': 434,\n",
       " 'actually': 435,\n",
       " 'hired': 436,\n",
       " 'twice': 437,\n",
       " 'under': 438,\n",
       " 'mate': 439,\n",
       " 'greenland': 440,\n",
       " 'whaler': 441,\n",
       " 'acquitted': 442,\n",
       " 'admiration': 443,\n",
       " 'must': 444,\n",
       " 'proud': 445,\n",
       " 'captain': 446,\n",
       " 'offered': 447,\n",
       " 'second': 448,\n",
       " 'dignity': 449,\n",
       " 'vessel': 450,\n",
       " 'entreated': 451,\n",
       " 'remain': 452,\n",
       " 'earnestness': 453,\n",
       " 'valuable': 454,\n",
       " 'did': 455,\n",
       " 'consider': 456,\n",
       " 'services': 457,\n",
       " 'deserve': 458,\n",
       " 'accomplish': 459,\n",
       " 'ease': 460,\n",
       " 'luxury': 461,\n",
       " 'preferred': 462,\n",
       " 'glory': 463,\n",
       " 'enticement': 464,\n",
       " 'wealth': 465,\n",
       " 'placed': 466,\n",
       " 'path': 467,\n",
       " 'oh': 468,\n",
       " 'encouraging': 469,\n",
       " 'voice': 470,\n",
       " 'would': 471,\n",
       " 'answer': 472,\n",
       " 'affirmative': 473,\n",
       " '!': 474,\n",
       " 'courage': 475,\n",
       " 'resolution': 476,\n",
       " 'firm': 477,\n",
       " 'hopes': 478,\n",
       " 'fluctuate': 479,\n",
       " 'spirits': 480,\n",
       " 'depressed': 481,\n",
       " 'about': 482,\n",
       " 'proceed': 483,\n",
       " 'long': 484,\n",
       " 'difficult': 485,\n",
       " 'emergencies': 486,\n",
       " 'demand': 487,\n",
       " 'fortitude': 488,\n",
       " 'required': 489,\n",
       " 'raise': 490,\n",
       " 'others': 491,\n",
       " 'sometimes': 492,\n",
       " 'sustain': 493,\n",
       " 'theirs': 494,\n",
       " 'failing': 495,\n",
       " 'most': 496,\n",
       " 'favourable': 497,\n",
       " 'period': 498,\n",
       " 'travelling': 499,\n",
       " 'russia': 500,\n",
       " 'fly': 501,\n",
       " 'quickly': 502,\n",
       " 'sledges': 503,\n",
       " 'motion': 504,\n",
       " 'pleasant': 505,\n",
       " 'opinion': 506,\n",
       " 'agreeable': 507,\n",
       " 'english': 508,\n",
       " 'stagecoach': 509,\n",
       " 'excessive': 510,\n",
       " 'wrapped': 511,\n",
       " 'furs': 512,\n",
       " 'dress': 513,\n",
       " 'adopted': 514,\n",
       " 'difference': 515,\n",
       " 'between': 516,\n",
       " 'walking': 517,\n",
       " 'deck': 518,\n",
       " 'remaining': 519,\n",
       " 'seated': 520,\n",
       " 'motionless': 521,\n",
       " 'hours': 522,\n",
       " 'exercise': 523,\n",
       " 'prevents': 524,\n",
       " 'blood': 525,\n",
       " 'freezing': 526,\n",
       " 'veins': 527,\n",
       " 'ambition': 528,\n",
       " 'lose': 529,\n",
       " 'post': 530,\n",
       " 'road': 531,\n",
       " 'archangel': 532,\n",
       " 'depart': 533,\n",
       " 'latter': 534,\n",
       " 'town': 535,\n",
       " 'fortnight': 536,\n",
       " 'three': 537,\n",
       " 'weeks': 538,\n",
       " 'intention': 539,\n",
       " 'hire': 540,\n",
       " 'ship': 541,\n",
       " 'easily': 542,\n",
       " 'done': 543,\n",
       " 'paying': 544,\n",
       " 'insurance': 545,\n",
       " 'owner': 546,\n",
       " 'engage': 547,\n",
       " 'think': 548,\n",
       " 'necessary': 549,\n",
       " 'among': 550,\n",
       " 'who': 551,\n",
       " 'accustomed': 552,\n",
       " 'fishing': 553,\n",
       " 'intend': 554,\n",
       " 'sail': 555,\n",
       " 'until': 556,\n",
       " 'month': 557,\n",
       " 'june': 558,\n",
       " 'return': 559,\n",
       " 'ah': 560,\n",
       " 'question': 561,\n",
       " 'succeed': 562,\n",
       " 'perhaps': 563,\n",
       " 'pass': 564,\n",
       " 'meet': 565,\n",
       " 'fail': 566,\n",
       " 'see': 567,\n",
       " 'again': 568,\n",
       " 'soon': 569,\n",
       " 'farewell': 570,\n",
       " 'excellent': 571,\n",
       " 'shower': 572,\n",
       " 'down': 573,\n",
       " 'blessings': 574,\n",
       " 'save': 575,\n",
       " 'testify': 576,\n",
       " 'gratitude': 577,\n",
       " 'love': 578,\n",
       " 'kindness': 579,\n",
       " 'affectionate': 580,\n",
       " 'brother': 581,\n",
       " 'r': 582,\n",
       " 'walton': 583,\n",
       " 'march': 584,\n",
       " 'slowly': 585,\n",
       " 'passes': 586,\n",
       " 'encompassed': 587,\n",
       " 'step': 588,\n",
       " 'taken': 589,\n",
       " 'occupied': 590,\n",
       " 'collecting': 591,\n",
       " 'whom': 592,\n",
       " 'engaged': 593,\n",
       " 'appear': 594,\n",
       " 'men': 595,\n",
       " 'depend': 596,\n",
       " 'certainly': 597,\n",
       " 'possessed': 598,\n",
       " 'dauntless': 599,\n",
       " 'able': 600,\n",
       " 'satisfy': 601,\n",
       " 'absence': 602,\n",
       " 'object': 603,\n",
       " 'severe': 604,\n",
       " 'friend': 605,\n",
       " 'glowing': 606,\n",
       " 'none': 607,\n",
       " 'participate': 608,\n",
       " 'assailed': 609,\n",
       " 'endeavour': 610,\n",
       " 'dejection': 611,\n",
       " 'commit': 612,\n",
       " 'paper': 613,\n",
       " 'true': 614,\n",
       " 'poor': 615,\n",
       " 'medium': 616,\n",
       " 'communication': 617,\n",
       " 'desire': 618,\n",
       " 'company': 619,\n",
       " 'could': 620,\n",
       " 'sympathize': 621,\n",
       " 'eyes': 622,\n",
       " 'reply': 623,\n",
       " 'deem': 624,\n",
       " 'romantic': 625,\n",
       " 'bitterly': 626,\n",
       " 'gentle': 627,\n",
       " 'courageous': 628,\n",
       " 'cultivated': 629,\n",
       " 'capacious': 630,\n",
       " 'tastes': 631,\n",
       " 'like': 632,\n",
       " 'approve': 633,\n",
       " 'amend': 634,\n",
       " 'plans': 635,\n",
       " 'repair': 636,\n",
       " 'faults': 637,\n",
       " 'too': 638,\n",
       " 'execution': 639,\n",
       " 'impatient': 640,\n",
       " 'difficulties': 641,\n",
       " 'still': 642,\n",
       " 'greater': 643,\n",
       " 'self': 644,\n",
       " 'educated': 645,\n",
       " 'fourteen': 646,\n",
       " 'ran': 647,\n",
       " 'wild': 648,\n",
       " 'books': 649,\n",
       " 'age': 650,\n",
       " 'celebrated': 651,\n",
       " 'ceased': 652,\n",
       " 'important': 653,\n",
       " 'benefits': 654,\n",
       " 'conviction': 655,\n",
       " 'perceived': 656,\n",
       " 'necessity': 657,\n",
       " 'becoming': 658,\n",
       " 'languages': 659,\n",
       " 'twenty': 660,\n",
       " 'eight': 661,\n",
       " 'reality': 662,\n",
       " 'illiterate': 663,\n",
       " 'schoolboys': 664,\n",
       " 'fifteen': 665,\n",
       " 'thought': 666,\n",
       " 'extended': 667,\n",
       " 'magnificent': 668,\n",
       " 'painters': 669,\n",
       " 'call': 670,\n",
       " 'keeping': 671,\n",
       " 'greatly': 672,\n",
       " 'need': 673,\n",
       " 'sense': 674,\n",
       " 'enough': 675,\n",
       " 'despise': 676,\n",
       " 'affection': 677,\n",
       " 'useless': 678,\n",
       " 'complaints': 679,\n",
       " 'find': 680,\n",
       " 'wide': 681,\n",
       " 'nor': 682,\n",
       " 'merchants': 683,\n",
       " 'seamen': 684,\n",
       " 'feelings': 685,\n",
       " 'unallied': 686,\n",
       " 'dross': 687,\n",
       " 'human': 688,\n",
       " 'nature': 689,\n",
       " 'beat': 690,\n",
       " 'rugged': 691,\n",
       " 'bosoms': 692,\n",
       " 'lieutenant': 693,\n",
       " 'instance': 694,\n",
       " 'wonderful': 695,\n",
       " 'madly': 696,\n",
       " 'desirous': 697,\n",
       " 'rather': 698,\n",
       " 'word': 699,\n",
       " 'phrase': 700,\n",
       " 'characteristically': 701,\n",
       " 'advancement': 702,\n",
       " 'profession': 703,\n",
       " 'englishman': 704,\n",
       " 'midst': 705,\n",
       " 'national': 706,\n",
       " 'professional': 707,\n",
       " 'prejudices': 708,\n",
       " 'unsoftened': 709,\n",
       " 'cultivation': 710,\n",
       " 'retains': 711,\n",
       " 'noblest': 712,\n",
       " 'endowments': 713,\n",
       " 'humanity': 714,\n",
       " 'him': 715,\n",
       " 'board': 716,\n",
       " 'finding': 717,\n",
       " 'unemployed': 718,\n",
       " 'city': 719,\n",
       " 'assist': 720,\n",
       " 'master': 721,\n",
       " 'person': 722,\n",
       " 'disposition': 723,\n",
       " 'remarkable': 724,\n",
       " 'gentleness': 725,\n",
       " 'mildness': 726,\n",
       " 'discipline': 727,\n",
       " 'circumstance': 728,\n",
       " 'added': 729,\n",
       " 'known': 730,\n",
       " 'integrity': 731,\n",
       " 'very': 732,\n",
       " 'youth': 733,\n",
       " 'solitude': 734,\n",
       " 'best': 735,\n",
       " 'spent': 736,\n",
       " 'feminine': 737,\n",
       " 'fosterage': 738,\n",
       " 'refined': 739,\n",
       " 'groundwork': 740,\n",
       " 'character': 741,\n",
       " 'overcome': 742,\n",
       " 'intense': 743,\n",
       " 'distaste': 744,\n",
       " 'usual': 745,\n",
       " 'brutality': 746,\n",
       " 'exercised': 747,\n",
       " 'believed': 748,\n",
       " 'heard': 749,\n",
       " 'mariner': 750,\n",
       " 'equally': 751,\n",
       " 'noted': 752,\n",
       " 'kindliness': 753,\n",
       " 'respect': 754,\n",
       " 'obedience': 755,\n",
       " 'paid': 756,\n",
       " 'crew': 757,\n",
       " 'peculiarly': 758,\n",
       " 'fortunate': 759,\n",
       " 'being': 760,\n",
       " 'secure': 761,\n",
       " 'manner': 762,\n",
       " 'lady': 763,\n",
       " 'owes': 764,\n",
       " 'happiness': 765,\n",
       " 'her': 766,\n",
       " 'briefly': 767,\n",
       " 'story': 768,\n",
       " 'ago': 769,\n",
       " 'loved': 770,\n",
       " 'young': 771,\n",
       " 'russian': 772,\n",
       " 'moderate': 773,\n",
       " 'having': 774,\n",
       " 'amassed': 775,\n",
       " 'considerable': 776,\n",
       " 'sum': 777,\n",
       " 'prize': 778,\n",
       " 'money': 779,\n",
       " 'girl': 780,\n",
       " 'consented': 781,\n",
       " 'match': 782,\n",
       " 'saw': 783,\n",
       " 'mistress': 784,\n",
       " 'once': 785,\n",
       " 'destined': 786,\n",
       " 'ceremony': 787,\n",
       " 'she': 788,\n",
       " 'bathed': 789,\n",
       " 'tears': 790,\n",
       " 'throwing': 791,\n",
       " 'herself': 792,\n",
       " 'feet': 793,\n",
       " 'spare': 794,\n",
       " 'confessing': 795,\n",
       " 'same': 796,\n",
       " 'another': 797,\n",
       " 'consent': 798,\n",
       " 'union': 799,\n",
       " 'generous': 800,\n",
       " 'reassured': 801,\n",
       " 'suppliant': 802,\n",
       " 'informed': 803,\n",
       " 'name': 804,\n",
       " 'lover': 805,\n",
       " 'instantly': 806,\n",
       " 'abandoned': 807,\n",
       " 'pursuit': 808,\n",
       " 'bought': 809,\n",
       " 'farm': 810,\n",
       " 'designed': 811,\n",
       " 'remainder': 812,\n",
       " 'bestowed': 813,\n",
       " 'rival': 814,\n",
       " 'together': 815,\n",
       " 'remains': 816,\n",
       " 'purchase': 817,\n",
       " 'stock': 818,\n",
       " 'then': 819,\n",
       " 'himself': 820,\n",
       " 'solicited': 821,\n",
       " 'woman': 822,\n",
       " 'marriage': 823,\n",
       " 'old': 824,\n",
       " 'decidedly': 825,\n",
       " 'refused': 826,\n",
       " 'thinking': 827,\n",
       " 'bound': 828,\n",
       " 'honour': 829,\n",
       " 'found': 830,\n",
       " 'inexorable': 831,\n",
       " 'quitted': 832,\n",
       " 'returned': 833,\n",
       " 'former': 834,\n",
       " 'married': 835,\n",
       " 'according': 836,\n",
       " 'inclinations': 837,\n",
       " 'noble': 838,\n",
       " 'fellow': 839,\n",
       " 'exclaim': 840,\n",
       " 'wholly': 841,\n",
       " 'uneducated': 842,\n",
       " 'silent': 843,\n",
       " 'turk': 844,\n",
       " 'kind': 845,\n",
       " 'ignorant': 846,\n",
       " 'carelessness': 847,\n",
       " 'attends': 848,\n",
       " 'while': 849,\n",
       " 'renders': 850,\n",
       " 'conduct': 851,\n",
       " 'astonishing': 852,\n",
       " 'detracts': 853,\n",
       " 'interest': 854,\n",
       " 'sympathy': 855,\n",
       " 'otherwise': 856,\n",
       " 'command': 857,\n",
       " 'suppose': 858,\n",
       " 'because': 859,\n",
       " 'complain': 860,\n",
       " 'conceive': 861,\n",
       " 'consolation': 862,\n",
       " 'toils': 863,\n",
       " 'know': 864,\n",
       " 'wavering': 865,\n",
       " 'resolutions': 866,\n",
       " 'fixed': 867,\n",
       " 'fate': 868,\n",
       " 'delayed': 869,\n",
       " 'weather': 870,\n",
       " 'permit': 871,\n",
       " 'embarkation': 872,\n",
       " 'winter': 873,\n",
       " 'dreadfully': 874,\n",
       " 'spring': 875,\n",
       " 'promises': 876,\n",
       " 'considered': 877,\n",
       " 'remarkably': 878,\n",
       " 'season': 879,\n",
       " 'sooner': 880,\n",
       " 'rashly': 881,\n",
       " 'sufficiently': 882,\n",
       " 'confide': 883,\n",
       " 'prudence': 884,\n",
       " 'considerateness': 885,\n",
       " 'whenever': 886,\n",
       " 'safety': 887,\n",
       " 'committed': 888,\n",
       " 'care': 889,\n",
       " 'describe': 890,\n",
       " 'sensations': 891,\n",
       " 'impossible': 892,\n",
       " 'communicate': 893,\n",
       " 'conception': 894,\n",
       " 'trembling': 895,\n",
       " 'sensation': 896,\n",
       " 'half': 897,\n",
       " 'pleasurable': 898,\n",
       " 'fearful': 899,\n",
       " 'preparing': 900,\n",
       " 'going': 901,\n",
       " 'unexplored': 902,\n",
       " 'mist': 903,\n",
       " 'kill': 904,\n",
       " 'albatross': 905,\n",
       " 'therefore': 906,\n",
       " 'alarmed': 907,\n",
       " 'should': 908,\n",
       " 'come': 909,\n",
       " 'back': 910,\n",
       " 'worn': 911,\n",
       " 'woeful': 912,\n",
       " 'ancient': 913,\n",
       " 'smile': 914,\n",
       " 'allusion': 915,\n",
       " 'disclose': 916,\n",
       " 'attributed': 917,\n",
       " 'attachment': 918,\n",
       " 'passionate': 919,\n",
       " 'dangerous': 920,\n",
       " 'mysteries': 921,\n",
       " 'production': 922,\n",
       " 'imaginative': 923,\n",
       " 'something': 924,\n",
       " 'work': 925,\n",
       " 'practically': 926,\n",
       " 'industrious': 927,\n",
       " 'painstaking': 928,\n",
       " 'workman': 929,\n",
       " 'execute': 930,\n",
       " 'perseverance': 931,\n",
       " 'labour': 932,\n",
       " 'besides': 933,\n",
       " 'marvellous': 934,\n",
       " 'belief': 935,\n",
       " 'intertwined': 936,\n",
       " 'projects': 937,\n",
       " 'hurries': 938,\n",
       " 'out': 939,\n",
       " 'pathways': 940,\n",
       " 'unvisited': 941,\n",
       " 'explore': 942,\n",
       " 'dearer': 943,\n",
       " 'considerations': 944,\n",
       " 'after': 945,\n",
       " 'traversed': 946,\n",
       " 'immense': 947,\n",
       " 'southern': 948,\n",
       " 'cape': 949,\n",
       " 'africa': 950,\n",
       " 'america': 951,\n",
       " 'dare': 952,\n",
       " 'expect': 953,\n",
       " 'bear': 954,\n",
       " 'look': 955,\n",
       " 'reverse': 956,\n",
       " 'picture': 957,\n",
       " 'continue': 958,\n",
       " 'write': 959,\n",
       " 'opportunity': 960,\n",
       " 'receive': 961,\n",
       " 'letters': 962,\n",
       " 'occasions': 963,\n",
       " 'support': 964,\n",
       " 'tenderly': 965,\n",
       " 'robert': 966,\n",
       " 'july': 967,\n",
       " 'few': 968,\n",
       " 'lines': 969,\n",
       " 'haste': 970,\n",
       " 'say': 971,\n",
       " 'safe': 972,\n",
       " 'advanced': 973,\n",
       " 'merchantman': 974,\n",
       " 'homeward': 975,\n",
       " 'however': 976,\n",
       " 'bold': 977,\n",
       " 'apparently': 978,\n",
       " 'floating': 979,\n",
       " 'sheets': 980,\n",
       " 'ice': 981,\n",
       " 'continually': 982,\n",
       " 'us': 983,\n",
       " 'indicating': 984,\n",
       " 'dangers': 985,\n",
       " 'dismay': 986,\n",
       " 'reached': 987,\n",
       " 'high': 988,\n",
       " 'latitude': 989,\n",
       " 'height': 990,\n",
       " 'summer': 991,\n",
       " 'although': 992,\n",
       " 'warm': 993,\n",
       " 'gales': 994,\n",
       " 'blow': 995,\n",
       " 'speedily': 996,\n",
       " 'shores': 997,\n",
       " 'ardently': 998,\n",
       " 'attain': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "row_idx = 5\n",
    "for i in dataset[row_idx][\"x_data\"]:\n",
    "    print(vectorizer.cbow_vocab.lookup_index(i))\n",
    "\n",
    "vectorizer.cbow_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c68d9193d8463997b9c10ff3986564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4874c55d39474bcf9261e5bcabc03967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/1984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d273a350b67a4ec1ba2e95c45bd8c1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                 total=args.num_epochs,\n",
    "                 position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                 total=dataset.get_num_batches(args.batch_size),  \n",
    "                 position=1, \n",
    "                 leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "               total=dataset.get_num_batches(args.batch_size), \n",
    "               position=1, \n",
    "               leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(x_in=batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 7.672896399778476;\n",
      "Test Accuracy: 13.007352941176478\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    Pretty print embedding results.\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Get the n closest\n",
    "    words to your word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate distances to all other words\n",
    "    \n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...[6.53] - horseback\n",
      "...[6.54] - literally\n",
      "...[6.59] - delighted\n",
      "...[6.79] - thousand\n",
      "...[6.86] - ran\n",
      "...[6.88] - visions\n"
     ]
    }
   ],
   "source": [
    "word = input('Enter a word: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[7.24] - irradiated\n",
      "...[7.68] - enslaved\n",
      "...[7.71] - men\n",
      "...[7.75] - gush\n",
      "...[7.76] - mode\n",
      "...[7.76] - austria\n",
      "=======monster=======\n",
      "...[7.57] - cares\n",
      "...[7.70] - griefs\n",
      "...[7.74] - saw\n",
      "...[7.78] - confused\n",
      "...[7.81] - without\n",
      "...[7.82] - truly\n",
      "=======science=======\n",
      "...[7.02] - mutual\n",
      "...[7.02] - impression\n",
      "...[7.06] - mist\n",
      "...[7.16] - swelling\n",
      "...[7.24] - darkened\n",
      "...[7.30] - tempted\n",
      "=======sickness=======\n",
      "...[6.21] - while\n",
      "...[6.59] - awoke\n",
      "...[6.60] - foundations\n",
      "...[6.66] - consoles\n",
      "...[6.69] - literally\n",
      "...[6.69] - know\n",
      "=======lonely=======\n",
      "...[6.77] - excessive\n",
      "...[6.85] - moonlight\n",
      "...[6.90] - ought\n",
      "...[7.10] - bed\n",
      "...[7.12] - three\n",
      "...[7.20] - superhuman\n",
      "=======happy=======\n",
      "...[6.33] - bottom\n",
      "...[6.42] - penetrated\n",
      "...[6.44] - wand\n",
      "...[6.52] - chivalry\n",
      "...[6.52] - joys\n",
      "...[6.53] - altered\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7f86253a46c5444491fbb7541857edfa"
  },
  "gist": {
   "data": {
    "description": "chapters/06_RNN/tweet_classifcation/tweeter_classification.ipynb",
    "public": true
   },
   "id": "7f86253a46c5444491fbb7541857edfa"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "255px",
    "left": "561px",
    "right": "20px",
    "top": "179px",
    "width": "266px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
