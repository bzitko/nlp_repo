{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"imdb.zip\"\n",
    "if not os.path.exists(\"imdb.zip\"):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a01/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "name = \"imdb\"\n",
    "if not os.path.exists(name):\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(download_name) as zf:\n",
    "        zf.extractall(path=name)\n",
    "    \n",
    "name = \"stopwords.txt\"\n",
    "if not os.path.exists(name):\n",
    "    name = \"stopwords.txt\"\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a01/{name}\")\n",
    "    with open(name, \"wb\") as fp:\n",
    "        fp.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c107ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0aa00d",
   "metadata": {},
   "source": [
    "# 1. Read data\n",
    "\n",
    "Path \"data/imdb/pos\" has 1000 txt files with positive movie reviews.  \n",
    "Path \"data/imdb/neg\" has 1000 txt files with negative movie reviews.  \n",
    "Each text filename incorporates counter. For example, \"data/imdb/pos/pos009_29592.txt\".  \n",
    "Return dictionary with keys \"pos\" and \"neg\" where value of \"pos\" is a list of text filename content sorted by filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd93c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884601a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data[\"neg\"]) == 1000\n",
    "assert len(data[\"pos\"]) == 1000\n",
    "\n",
    "assert data[\"pos\"][0][:30] == \"films adapted from comic books\"\n",
    "assert data[\"neg\"][-1][-30:] == \"left with exactly the same . \\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dcfc7",
   "metadata": {},
   "source": [
    "# 1.  Read stop words\n",
    "\n",
    "Path \"data/stopwords.txt\" contains english stop words.  \n",
    "Read stopwords from file and store them in a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148b56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf58119",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(stopwords) == 318\n",
    "assert {w for w in stopwords if w.startswith(\"so\")} == {'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebc5db",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "Create function *tokenize* which: \n",
    "* for a given text \n",
    "* returns list of tokens in lower cases (token can contain only letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353150f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a651c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list(tokenize(data[\"pos\"][0]))) == 649\n",
    "assert len(list(tokenize(data[\"neg\"][0]))) == 669\n",
    "\n",
    "assert list(tokenize(data[\"pos\"][0]))[:10] == ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success']\n",
    "assert list(tokenize(data[\"neg\"][0]))[-10:] == ['crow', 'salvation', 'lost', 'highway', 'memento', 'the', 'others', 'stir',  'of', 'echoes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e91891",
   "metadata": {},
   "source": [
    "# 3. Word counter\n",
    "\n",
    "Create function *make_counter* which: \n",
    "* receives corpus (a list of strings) and set of stopwords,\n",
    "* tokenize each string, \n",
    "* remove stop words and \n",
    "* returns dictionary whose keys are tokens and values frequencies.\n",
    "\n",
    "Note: `Counter` class from `collections` module can be used  \n",
    "`from collections import Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(make_counter(data[\"neg\"][:2], stopwords)) == 284\n",
    "assert len(make_counter(data[\"pos\"][-2:], stopwords=[])) == 613"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c290b1",
   "metadata": {},
   "source": [
    "# 4. Term frequency - inverse term frequency\n",
    "\n",
    "Create function *make_tfidf* which: \n",
    "* receives corpus and stopwords, and\n",
    "* returns tensor whose size is #doc x #words\n",
    "\n",
    "Note: Use `TfidfVectorizer` from `sklearn.feature_extraction.text`.  \n",
    "Instantiate `TfidfVectorizer` by setting `stop_words` and `tokenizer` arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22d1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e675123",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert make_tfidf(data[\"neg\"], stopwords).shape == (1000, 26764) \n",
    "assert make_tfidf(data[\"pos\"], stopwords).shape == (1000, 28699) \n",
    "\n",
    "assert torch.isclose(torch.sum(make_tfidf(data[\"neg\"], stopwords)), torch.tensor(11988.8478, dtype=torch.float64))\n",
    "assert torch.isclose(torch.sum(make_tfidf(data[\"pos\"], stopwords)), torch.tensor(12237.4325, dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fb45c",
   "metadata": {},
   "source": [
    "## 5.1. Most common words\n",
    "\n",
    "Count words from both positive and negative reviews by using `make_counter`.  \n",
    "List top ten most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2757d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = make_counter(data[\"pos\"] + data[\"neg\"], stopwords)\n",
    "pd.DataFrame(counter.most_common(10), columns=[\"word\", \"freq\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217121e",
   "metadata": {},
   "source": [
    "## 5.2. Plot word frequencies\n",
    "\n",
    "Make scatter plot where:\n",
    "* x axis represents words sorted by frequencies and\n",
    "* y axis are frequencies (use log scale for this axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855309af",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = sorted(counter.values())\n",
    "\n",
    "plt.scatter(range(len(freqs)), freqs, marker=\".\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.xlabel(\"words\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0415047",
   "metadata": {},
   "source": [
    "## 6.1. TF-IDF tensor\n",
    "\n",
    "Make TF-IDF tensor for positive and negative reviews. Tensor must have 2000 rows (1000 positive and 1000 negative reviews).  \n",
    "Calculate ratio of non-zero values in the TF-IDF tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0f3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e49e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf[:1000].count_nonzero() == torch.tensor(242389)\n",
    "assert tfidf[1000:].count_nonzero() == torch.tensor(218810)\n",
    "\n",
    "assert torch.isclose(torch.sum(tfidf), torch.tensor(24080.7049, dtype=torch.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7510e4f",
   "metadata": {},
   "source": [
    "## 6.2. TF-IDF tensor slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494003cf",
   "metadata": {},
   "source": [
    "Create two tensors, one for positive and one for negative reviews by slicing previously made TF-IDF tensor of both reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ab4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf_pos.count_nonzero() == torch.tensor(242389)\n",
    "assert tfidf_neg.count_nonzero() == torch.tensor(218810)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc5151",
   "metadata": {},
   "source": [
    "## 6.3. Similarity\n",
    "\n",
    "Create similarity tensor whose (i, j) value is cosine similarity of i-th positive review and j-th negative review.  \n",
    "Cosine similarity between two vectors $u$ and $v$ is \n",
    "$$cos(u, v) = \\frac{uv}{\\lVert u \\rVert \\lVert v \\rVert} = \\frac{\\sum_{i=1}^{n} u_i v_i}{\\sqrt{\\sum_{i=1}^{n} u_i^2} \\sqrt{\\sum_{i=1}^{n} v_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bf649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6429b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim.shape == (tfidf_pos.shape[0], tfidf_neg.shape[0]), f\"shape of sim tensor is {tuple(sim.shape)}, but must be (1000, 1000)\"\n",
    "\n",
    "assert torch.allclose(sim[:3,:3], torch.tensor([[0.0492, 0.0326, 0.0477],\n",
    "                                                [0.0463, 0.0240, 0.0539],\n",
    "                                                [0.0462, 0.0409, 0.0445]], dtype=torch.float64), atol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a33acd",
   "metadata": {},
   "source": [
    "## 6.4. Most similar positive and negative review\n",
    "\n",
    "Find most similar positive and negative review. \n",
    "Print their sorted tokenized text without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c1f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a117286",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (pos_i, neg_i) == (789, 697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens = sorted(tok for tok in tokenize(data[\"pos\"][pos_i]) if tok not in stopwords)\n",
    "\n",
    "print(\" \".join(pos_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27356ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_tokens = sorted(tok for tok in tokenize(data[\"neg\"][neg_i]) if tok not in stopwords)\n",
    "\n",
    "print(\" \".join(neg_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb847e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
