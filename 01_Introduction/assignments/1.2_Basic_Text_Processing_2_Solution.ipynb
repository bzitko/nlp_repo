{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a6d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_name = \"imdb.zip\"\n",
    "if not os.path.exists(\"imdb.zip\"):\n",
    "    import requests\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a01/{download_name}\")\n",
    "    with open(download_name, \"wb\") as fp:\n",
    "        fp.write(response.content)\n",
    "    response.close()\n",
    "\n",
    "name = \"imdb\"\n",
    "if not os.path.exists(name):\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(download_name) as zf:\n",
    "        zf.extractall(path=name)\n",
    "    \n",
    "name = \"stopwords.txt\"\n",
    "if not os.path.exists(name):\n",
    "    name = \"stopwords.txt\"\n",
    "    response = requests.get(f\"https://raw.githubusercontent.com/bzitko/nlp_repo/main/assignments/a01/{name}\")\n",
    "    with open(name, \"wb\") as fp:\n",
    "        fp.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c107ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0aa00d",
   "metadata": {},
   "source": [
    "# 1. Read data\n",
    "\n",
    "Path \"imdb/pos\" has 1000 txt files with positive movie reviews.  \n",
    "Path \"imdb/neg\" has 1000 txt files with negative movie reviews.  \n",
    "Each text filename incorporates counter. For example, \"imdb/pos/pos009_29592.txt\".  \n",
    "Return dictionary with keys \"pos\" and \"neg\" where value of \"pos\" is a list of text filename content sorted by filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15c0819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films adapted from comic books have had plenty of success , whether they\\'re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there\\'s never really been a comic book like from hell before . \\nfor starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid \\'80s with a 12-part series called the watchmen . \\nto say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \\nthe book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \\nin other words , don\\'t dismiss this film because of its source . \\nif you can get past the whole comic book thing , you might find another stumbling block in from hell\\'s directors , albert and allen hughes . \\ngetting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that\\'s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \\nthe ghetto in question is , of course , whitechapel in 1888 london\\'s east end . \\nit\\'s a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \\nwhen the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \\nabberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \\nupon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn\\'t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can\\'t stomach . \\ni don\\'t think anyone needs to be briefed on jack the ripper , so i won\\'t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \\nin the comic , they don\\'t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \\nit\\'s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \\nand from hell\\'s ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \\ndon\\'t worry - it\\'ll all make sense when you see it . \\nnow onto from hell\\'s appearance : it\\'s certainly dark and bleak enough , and it\\'s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \\nthe print i saw wasn\\'t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don\\'t say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \\noscar winner martin childs\\' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \\neven the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \\nians holm ( joe gould\\'s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \\ni cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn\\'t half bad . \\nthe film , however , is all good . \\n2 : 00 - r for strong violence/gore , sexuality , language and drug content \\n',\n",
       " 'every now and then a movie comes along from a suspect studio , with every indication that it will be a stinker , and to everybody\\'s surprise ( perhaps even the studio ) the film becomes a critical darling . \\nmtv films\\' _election , a high school comedy starring matthew broderick and reese witherspoon , is a current example . \\ndid anybody know this film existed a week before it opened ? \\nthe plot is deceptively simple . \\ngeorge washington carver high school is having student elections . \\ntracy flick ( reese witherspoon ) is an over-achiever with her hand raised at nearly every question , way , way , high . \\nmr . \" m \" ( matthew broderick ) , sick of the megalomaniac student , encourages paul , a popular-but-slow jock to run . \\nand paul\\'s nihilistic sister jumps in the race as well , for personal reasons . \\nthe dark side of such sleeper success is that , because expectations were so low going in , the fact that this was quality stuff made the reviews even more enthusiastic than they have any right to be . \\nyou can\\'t help going in with the baggage of glowing reviews , which is in contrast to the negative baggage that the reviewers were likely to have . \\n_election , a good film , does not live up to its hype . \\nwhat makes _election_ so disappointing is that it contains significant plot details lifted directly from _rushmore_ , released a few months earlier . \\nthe similarities are staggering : \\ntracy flick ( _election_ ) is the president of an extraordinary number of clubs , and is involved with the school play . \\nmax fischer ( _rushmore_ ) is the president of an extraordinary number of clubs , and is involved with the school play . \\nthe most significant tension of _election_ is the potential relationship between a teacher and his student . \\nthe most significant tension of _rushmore_ is the potential relationship between a teacher and his student . \\ntracy flick is from a single parent home , which has contributed to her drive . \\nmax fischer is from a single parent home , which has contributed to his drive . \\nthe male bumbling adult in _election_ ( matthew broderick ) pursues an extramarital affair , gets caught , and his whole life is ruined . \\nhe even gets a bee sting . \\nthe male bumbling adult in _rushmore_ ( bill murray ) pursues an extramarital affair , gets caught , and his whole life is ruined . \\nhe gets several bee stings . \\nand so on . \\nwhat happened ? \\nhow is it that an individual screenplay ( _rushmore_ ) and a novel ( _election_ ) contain so many significant plot points , and yet both films were probably not even aware of each other , made from two different studios , from a genre ( the high school geeks revenge movie ) that hadn\\'t been fully formed yet ? \\neven so , the strengths of _election_ rely upon its fantastic performances from broderick , witherspoon , and newcomer jessica campbell , as paul\\'s anti-social sister , tammy . \\nbroderick here is playing the mr . rooney role from _ferris bueller_ , and he seems to be having the most fun he\\'s had since then . \\nwitherspoon is a revelation . \\nit\\'s early in the year , it\\'s a comedy , and teenagers have little clout , but for my money , witherspoon deserves an oscar nomination . \\nand once campbell\\'s character gets going , like in her fantastic speech in the gymnasium , then you\\'re won over . \\none thing that\\'s been bothering me since i\\'ve seen it . \\nthere is an extraordinary amount of sexuality in this film . \\ni suppose that , coming from mtv films , i should expect no less . . . \\nbut the film starts off light and airy , like a sitcom . \\nas the screws tighten , and the tensions mount , alexander payne decides to add elements that , frankly , distract from the story . \\nit is bad enough that mr . m doesn\\'t like tracy\\'s determination to win at all costs , but did they have to throw in the student/teacher relationship ? \\neven so , there\\'s no logical reason why mr . m has an affair when he does . \\nthere\\'s a lot to like in _election_ , but the plot similarities to _rushmore_ , and the tonal nosedive it takes as it gets explicitly sex-driven , mark this as a disappointment . \\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "folder = \"imdb\"\n",
    "for cat in os.listdir(folder):\n",
    "    path = os.path.join(folder, cat)\n",
    "    if not os.path.isdir(path):\n",
    "        continue\n",
    "    data[cat] = []\n",
    "    \n",
    "    for fname in sorted(os.listdir(path)):\n",
    "        if not fname.endswith(\".txt\"):\n",
    "            continue\n",
    "        fname = os.path.join(path, fname)\n",
    "        data[cat].append(open(fname).read())\n",
    "\n",
    "data[\"pos\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884601a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data[\"neg\"]) == 1000\n",
    "assert len(data[\"pos\"]) == 1000\n",
    "\n",
    "assert data[\"pos\"][0][:30] == \"films adapted from comic books\"\n",
    "assert data[\"neg\"][-1][-30:] == \"left with exactly the same . \\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dcfc7",
   "metadata": {},
   "source": [
    "# 1.  Read stop words\n",
    "\n",
    "Path \"stopwords.txt\" contains english stop words.  \n",
    "Read stopwords from file and store them in a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3148b56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bill',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'co',\n",
       " 'con',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'do',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'in',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'interest',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mill',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'rather',\n",
       " 're',\n",
       " 'same',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'sincere',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'system',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set()\n",
    "with open(\"stopwords.txt\") as fp:\n",
    "    for w in fp:\n",
    "        stopwords.add(w.rstrip())\n",
    "        \n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf58119",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(stopwords) == 318\n",
    "assert {w for w in stopwords if w.startswith(\"so\")} == {'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebc5db",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "Create function *tokenize* which: \n",
    "* for a given text \n",
    "* returns list of tokens in lower cases (token can contain only letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e353150f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " 'the',\n",
       " 'deal',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'sorta',\n",
       " 'find',\n",
       " 'out',\n",
       " 'critique',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " 'lost',\n",
       " 'highway',\n",
       " 'memento',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'well',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " 'normal',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " 'fantasy',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'going',\n",
       " 'on',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " 'biggest',\n",
       " 'problem',\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " 'not',\n",
       " 'really',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'point',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " 'into',\n",
       " 'it',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " 'okay',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'apparently',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " 'there',\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'suits',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " 'unraveling',\n",
       " 'overall',\n",
       " 'the',\n",
       " 'film',\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " 'entertain',\n",
       " 'confusing',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " 'oh',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'whatever',\n",
       " 'skip',\n",
       " 'it',\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " 'blair',\n",
       " 'witch',\n",
       " 'the',\n",
       " 'crow',\n",
       " 'the',\n",
       " 'crow',\n",
       " 'salvation',\n",
       " 'lost',\n",
       " 'highway',\n",
       " 'memento',\n",
       " 'the',\n",
       " 'others',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    for tok in text.split():\n",
    "        if tok.isalpha():\n",
    "            yield tok.lower()\n",
    "\n",
    "list(tokenize(data[\"neg\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a651c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list(tokenize(data[\"pos\"][0]))) == 649\n",
    "assert len(list(tokenize(data[\"neg\"][0]))) == 669\n",
    "\n",
    "assert list(tokenize(data[\"pos\"][0]))[:10] == ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success']\n",
    "assert list(tokenize(data[\"neg\"][0]))[-10:] == ['crow', 'salvation', 'lost', 'highway', 'memento', 'the', 'others', 'stir',  'of', 'echoes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e91891",
   "metadata": {},
   "source": [
    "# 3. Word counter\n",
    "\n",
    "Create function *make_counter* which: \n",
    "* receives corpus (a list of strings) and set of stopwords,\n",
    "* tokenize each string, \n",
    "* remove stop words and \n",
    "* returns dictionary whose keys are tokens and values frequencies.\n",
    "\n",
    "Note: `Counter` class from `collections` module can be used  \n",
    "`from collections import Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0616a8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'accident': 1,\n",
       "         'acting': 2,\n",
       "         'action': 1,\n",
       "         'actors': 1,\n",
       "         'actually': 2,\n",
       "         'ago': 1,\n",
       "         'american': 1,\n",
       "         'apparently': 2,\n",
       "         'apparitions': 1,\n",
       "         'applaud': 1,\n",
       "         'arrow': 1,\n",
       "         'assuming': 1,\n",
       "         'attempt': 1,\n",
       "         'audience': 2,\n",
       "         'average': 1,\n",
       "         'away': 2,\n",
       "         'bad': 2,\n",
       "         'baldwin': 3,\n",
       "         'beauty': 1,\n",
       "         'bentley': 1,\n",
       "         'big': 2,\n",
       "         'biggest': 2,\n",
       "         'bit': 1,\n",
       "         'blair': 1,\n",
       "         'body': 1,\n",
       "         'brain': 1,\n",
       "         'break': 1,\n",
       "         'bringing': 1,\n",
       "         'brother': 1,\n",
       "         'bug': 1,\n",
       "         'came': 1,\n",
       "         'cgi': 1,\n",
       "         'character': 1,\n",
       "         'characters': 1,\n",
       "         'chase': 2,\n",
       "         'chasing': 1,\n",
       "         'chopped': 1,\n",
       "         'church': 1,\n",
       "         'clue': 1,\n",
       "         'comes': 1,\n",
       "         'coming': 2,\n",
       "         'completely': 1,\n",
       "         'concept': 1,\n",
       "         'confusing': 1,\n",
       "         'continues': 1,\n",
       "         'cool': 2,\n",
       "         'correctly': 1,\n",
       "         'couples': 1,\n",
       "         'course': 2,\n",
       "         'craziness': 1,\n",
       "         'crew': 2,\n",
       "         'critique': 1,\n",
       "         'crow': 2,\n",
       "         'curtis': 2,\n",
       "         'damn': 1,\n",
       "         'dead': 2,\n",
       "         'deal': 1,\n",
       "         'decent': 1,\n",
       "         'decided': 1,\n",
       "         'deserted': 1,\n",
       "         'design': 1,\n",
       "         'despite': 1,\n",
       "         'did': 2,\n",
       "         'dies': 1,\n",
       "         'different': 1,\n",
       "         'dig': 1,\n",
       "         'director': 1,\n",
       "         'disappearances': 1,\n",
       "         'does': 1,\n",
       "         'donald': 1,\n",
       "         'downshifts': 1,\n",
       "         'dreams': 1,\n",
       "         'drink': 1,\n",
       "         'drive': 1,\n",
       "         'drunkenly': 1,\n",
       "         'echoes': 1,\n",
       "         'edge': 1,\n",
       "         'elm': 1,\n",
       "         'ending': 1,\n",
       "         'engaging': 1,\n",
       "         'enter': 1,\n",
       "         'entertain': 1,\n",
       "         'entertaining': 2,\n",
       "         'entire': 1,\n",
       "         'exact': 1,\n",
       "         'excites': 1,\n",
       "         'executed': 1,\n",
       "         'explained': 1,\n",
       "         'explanation': 1,\n",
       "         'fantasy': 1,\n",
       "         'fed': 1,\n",
       "         'feeling': 1,\n",
       "         'feels': 2,\n",
       "         'figured': 1,\n",
       "         'film': 5,\n",
       "         'films': 2,\n",
       "         'final': 1,\n",
       "         'flash': 1,\n",
       "         'flashy': 1,\n",
       "         'flick': 1,\n",
       "         'flicks': 1,\n",
       "         'folks': 1,\n",
       "         'generally': 1,\n",
       "         'generation': 1,\n",
       "         'genre': 1,\n",
       "         'girlfriend': 1,\n",
       "         'given': 1,\n",
       "         'giving': 1,\n",
       "         'going': 4,\n",
       "         'good': 3,\n",
       "         'gore': 2,\n",
       "         'got': 2,\n",
       "         'guess': 2,\n",
       "         'guys': 1,\n",
       "         'halloween': 1,\n",
       "         'happen': 1,\n",
       "         'happy': 1,\n",
       "         'harder': 1,\n",
       "         'head': 2,\n",
       "         'hey': 1,\n",
       "         'hide': 2,\n",
       "         'highway': 2,\n",
       "         'hit': 1,\n",
       "         'holds': 1,\n",
       "         'horror': 1,\n",
       "         'hot': 1,\n",
       "         'idea': 2,\n",
       "         'insight': 1,\n",
       "         'jamie': 1,\n",
       "         'joblo': 1,\n",
       "         'jumbled': 1,\n",
       "         'just': 6,\n",
       "         'kick': 2,\n",
       "         'kids': 1,\n",
       "         'kind': 1,\n",
       "         'know': 5,\n",
       "         'kudos': 1,\n",
       "         'lazy': 1,\n",
       "         'lee': 1,\n",
       "         'life': 1,\n",
       "         'like': 6,\n",
       "         'likely': 1,\n",
       "         'likes': 1,\n",
       "         'line': 1,\n",
       "         'little': 3,\n",
       "         'look': 2,\n",
       "         'looooot': 1,\n",
       "         'lost': 2,\n",
       "         'main': 1,\n",
       "         'make': 5,\n",
       "         'makes': 1,\n",
       "         'making': 1,\n",
       "         'mean': 1,\n",
       "         'meantime': 1,\n",
       "         'melissa': 1,\n",
       "         'member': 1,\n",
       "         'memento': 2,\n",
       "         'mess': 1,\n",
       "         'middle': 1,\n",
       "         'mind': 1,\n",
       "         'minutes': 2,\n",
       "         'mir': 1,\n",
       "         'mold': 1,\n",
       "         'movie': 11,\n",
       "         'movies': 1,\n",
       "         'music': 1,\n",
       "         'neat': 1,\n",
       "         'need': 1,\n",
       "         'neighborhood': 1,\n",
       "         'new': 1,\n",
       "         'nightmare': 1,\n",
       "         'nightmares': 1,\n",
       "         'normal': 1,\n",
       "         'obviously': 1,\n",
       "         'occasional': 1,\n",
       "         'offering': 1,\n",
       "         'oh': 1,\n",
       "         'okay': 1,\n",
       "         'origin': 1,\n",
       "         'overall': 1,\n",
       "         'package': 1,\n",
       "         'packaged': 1,\n",
       "         'parts': 1,\n",
       "         'party': 1,\n",
       "         'password': 1,\n",
       "         'people': 2,\n",
       "         'personally': 1,\n",
       "         'picking': 1,\n",
       "         'pink': 1,\n",
       "         'plain': 1,\n",
       "         'playing': 1,\n",
       "         'plot': 1,\n",
       "         'point': 1,\n",
       "         'power': 2,\n",
       "         'presents': 1,\n",
       "         'pretty': 6,\n",
       "         'problem': 2,\n",
       "         'problems': 1,\n",
       "         'production': 1,\n",
       "         'quick': 1,\n",
       "         'rarely': 1,\n",
       "         'real': 1,\n",
       "         'really': 4,\n",
       "         'redundant': 1,\n",
       "         'regarding': 1,\n",
       "         'review': 2,\n",
       "         'robot': 1,\n",
       "         'robots': 2,\n",
       "         'running': 1,\n",
       "         'runtime': 1,\n",
       "         'russian': 1,\n",
       "         'sad': 1,\n",
       "         'sagemiller': 2,\n",
       "         'salvation': 1,\n",
       "         'scenes': 2,\n",
       "         'schnazzy': 1,\n",
       "         'secret': 2,\n",
       "         'sense': 2,\n",
       "         'sequences': 1,\n",
       "         'shelves': 1,\n",
       "         'ship': 3,\n",
       "         'shot': 1,\n",
       "         'showing': 1,\n",
       "         'shows': 1,\n",
       "         'simply': 2,\n",
       "         'sitting': 1,\n",
       "         'skip': 1,\n",
       "         'slasher': 1,\n",
       "         'snag': 1,\n",
       "         'sorta': 1,\n",
       "         'stan': 1,\n",
       "         'star': 1,\n",
       "         'starring': 1,\n",
       "         'start': 2,\n",
       "         'starts': 1,\n",
       "         'stick': 1,\n",
       "         'stir': 1,\n",
       "         'story': 1,\n",
       "         'strange': 1,\n",
       "         'strangeness': 3,\n",
       "         'street': 1,\n",
       "         'studio': 1,\n",
       "         'stumbling': 1,\n",
       "         'substance': 1,\n",
       "         'suits': 1,\n",
       "         'sunken': 1,\n",
       "         'sure': 1,\n",
       "         'sutherland': 2,\n",
       "         'taken': 1,\n",
       "         'tech': 1,\n",
       "         'teen': 4,\n",
       "         'terribly': 1,\n",
       "         'thing': 1,\n",
       "         'things': 2,\n",
       "         'thrilling': 1,\n",
       "         'time': 1,\n",
       "         'tons': 1,\n",
       "         'took': 2,\n",
       "         'touches': 1,\n",
       "         'trying': 1,\n",
       "         'tugboat': 1,\n",
       "         'turn': 1,\n",
       "         'turning': 1,\n",
       "         'types': 1,\n",
       "         'understanding': 1,\n",
       "         'unravel': 1,\n",
       "         'unraveling': 1,\n",
       "         'video': 1,\n",
       "         'virus': 1,\n",
       "         'visions': 1,\n",
       "         'want': 1,\n",
       "         'wasted': 1,\n",
       "         'watch': 1,\n",
       "         'way': 2,\n",
       "         'ways': 1,\n",
       "         'weird': 1,\n",
       "         'wes': 1,\n",
       "         'william': 1,\n",
       "         'witch': 1,\n",
       "         'work': 1,\n",
       "         'world': 2,\n",
       "         'wrapped': 1,\n",
       "         'write': 1,\n",
       "         'years': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_counter(corpus, stopwords):\n",
    "    count_vectorizer = CountVectorizer(stop_words=stopwords, tokenizer=tokenize)\n",
    "    count = count_vectorizer.fit_transform(corpus).toarray()\n",
    "    vocab = count_vectorizer.get_feature_names_out()\n",
    "    return Counter(dict(zip(vocab, count.sum(axis=0))))\n",
    "\n",
    "make_counter(data[\"neg\"][:2], stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31b24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(make_counter(data[\"neg\"][:2], stopwords)) == 284\n",
    "assert len(make_counter(data[\"pos\"][-2:], stopwords=[])) == 613"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c290b1",
   "metadata": {},
   "source": [
    "# 4. Term frequency - inverse term frequency\n",
    "\n",
    "Create function *make_tfidf* which: \n",
    "* receives corpus and stopwords, and\n",
    "* returns tensor whose size is #doc x #words\n",
    "\n",
    "Note: Use `TfidfVectorizer` from `sklearn.feature_extraction.text`.  \n",
    "Instantiate `TfidfVectorizer` by setting `stop_words` and `tokenizer` arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd22d1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0493, 0.0000, 0.0000, 0.0493, 0.0987, 0.0493, 0.0493, 0.0987, 0.0493,\n",
       "         0.0493, 0.0493, 0.0493, 0.0493, 0.0987, 0.0000, 0.0987, 0.0987, 0.0000,\n",
       "         0.0493, 0.0493, 0.0351, 0.0987, 0.0493, 0.0493, 0.0000, 0.0000, 0.0493,\n",
       "         0.0000, 0.0000, 0.0000, 0.0493, 0.0000, 0.0493, 0.0493, 0.0351, 0.0493,\n",
       "         0.0493, 0.0493, 0.0493, 0.0000, 0.0987, 0.0493, 0.0493, 0.0493, 0.0493,\n",
       "         0.0987, 0.0493, 0.0493, 0.0000, 0.0493, 0.0000, 0.0493, 0.0987, 0.0000,\n",
       "         0.0000, 0.0987, 0.0493, 0.0493, 0.0493, 0.0000, 0.0000, 0.0493, 0.0987,\n",
       "         0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0000, 0.0493, 0.0493,\n",
       "         0.0493, 0.0493, 0.0000, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493,\n",
       "         0.0493, 0.0987, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493,\n",
       "         0.0493, 0.0493, 0.0351, 0.0493, 0.2467, 0.0987, 0.0493, 0.0000, 0.0000,\n",
       "         0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493,\n",
       "         0.0702, 0.0702, 0.0000, 0.0351, 0.0987, 0.0493, 0.0000, 0.0493, 0.0000,\n",
       "         0.0493, 0.0351, 0.0000, 0.0987, 0.0987, 0.0000, 0.0493, 0.0493, 0.0493,\n",
       "         0.0987, 0.0493, 0.0000, 0.0493, 0.0493, 0.1404, 0.0000, 0.0493, 0.0493,\n",
       "         0.0351, 0.0493, 0.0493, 0.0000, 0.0493, 0.1053, 0.0000, 0.0000, 0.0493,\n",
       "         0.0702, 0.0987, 0.0493, 0.0987, 0.0493, 0.2467, 0.0493, 0.0493, 0.0493,\n",
       "         0.0493, 0.0493, 0.0493, 0.0987, 0.0493, 0.0000, 0.0493, 0.0987, 0.0000,\n",
       "         0.0493, 0.2106, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493,\n",
       "         0.0493, 0.0493, 0.0493, 0.0000, 0.0493, 0.0493, 0.0493, 0.0000, 0.0493,\n",
       "         0.0493, 0.0493, 0.0000, 0.0493, 0.0493, 0.0351, 0.0493, 0.0000, 0.0000,\n",
       "         0.0493, 0.0493, 0.0493, 0.0493, 0.0000, 0.0493, 0.1755, 0.0987, 0.0493,\n",
       "         0.0493, 0.0000, 0.0493, 0.0000, 0.0702, 0.0493, 0.0000, 0.0351, 0.0000,\n",
       "         0.0000, 0.0493, 0.0493, 0.0000, 0.0493, 0.0987, 0.0493, 0.0987, 0.0000,\n",
       "         0.0987, 0.0987, 0.0000, 0.0493, 0.0000, 0.0000, 0.0493, 0.0493, 0.0987,\n",
       "         0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0000, 0.0000, 0.0000, 0.0351,\n",
       "         0.0493, 0.0493, 0.0493, 0.0000, 0.0493, 0.0702, 0.0493, 0.0493, 0.0000,\n",
       "         0.0000, 0.0493, 0.0000, 0.0493, 0.0000, 0.0493, 0.0000, 0.1974, 0.0493,\n",
       "         0.0000, 0.0987, 0.0493, 0.0000, 0.0493, 0.0351, 0.0493, 0.0493, 0.0000,\n",
       "         0.0000, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0493, 0.0000, 0.0493,\n",
       "         0.0493, 0.0000, 0.0493, 0.0987, 0.0493, 0.0493, 0.0493, 0.0000, 0.0493,\n",
       "         0.0000, 0.0987, 0.0493, 0.0493, 0.0493],\n",
       "        [0.0000, 0.1650, 0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.2475,\n",
       "         0.0000, 0.0000, 0.0587, 0.0000, 0.0000, 0.0000, 0.0825, 0.0825, 0.0000,\n",
       "         0.0825, 0.0825, 0.0825, 0.0000, 0.0825, 0.0000, 0.0000, 0.0587, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1650, 0.0000, 0.1650, 0.0000, 0.0000, 0.1650,\n",
       "         0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0825, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0587, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0825,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1174, 0.0587, 0.1650, 0.0587, 0.0000, 0.0000, 0.0825, 0.0000, 0.0825,\n",
       "         0.0000, 0.0587, 0.0825, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.1174, 0.1650, 0.0000, 0.0000,\n",
       "         0.2348, 0.0000, 0.0000, 0.0825, 0.0000, 0.1761, 0.0825, 0.0825, 0.0000,\n",
       "         0.0587, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0825,\n",
       "         0.0000, 0.2935, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0000, 0.0825, 0.0000,\n",
       "         0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0587, 0.0000, 0.0825, 0.0825,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1650, 0.0000, 0.0587, 0.0000, 0.0000,\n",
       "         0.0000, 0.0825, 0.0000, 0.0825, 0.1174, 0.0000, 0.0825, 0.0587, 0.0825,\n",
       "         0.1650, 0.0000, 0.0000, 0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825,\n",
       "         0.0000, 0.0000, 0.0825, 0.0000, 0.2475, 0.0825, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0825, 0.0825, 0.0587,\n",
       "         0.0000, 0.0000, 0.0000, 0.0825, 0.0000, 0.0587, 0.0000, 0.0000, 0.0825,\n",
       "         0.0825, 0.0000, 0.0825, 0.0000, 0.1650, 0.0000, 0.0825, 0.0000, 0.0000,\n",
       "         0.0825, 0.0000, 0.0000, 0.0825, 0.0000, 0.0587, 0.0000, 0.0000, 0.0825,\n",
       "         0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0000,\n",
       "         0.0000, 0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0825, 0.0000,\n",
       "         0.0825, 0.0000, 0.0000, 0.0000, 0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tfidf(corpus, stopwords):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, tokenizer=tokenize)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "    # vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "    return torch.tensor(tfidf)\n",
    "\n",
    "make_tfidf(data[\"neg\"][:2], stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e675123",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert make_tfidf(data[\"neg\"], stopwords).shape == (1000, 26764) \n",
    "assert make_tfidf(data[\"pos\"], stopwords).shape == (1000, 28699) \n",
    "\n",
    "assert torch.isclose(torch.sum(make_tfidf(data[\"neg\"], stopwords)), torch.tensor(11988.8478, dtype=torch.float64))\n",
    "assert torch.isclose(torch.sum(make_tfidf(data[\"pos\"], stopwords)), torch.tensor(12237.4325, dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946fb45c",
   "metadata": {},
   "source": [
    "## 5.1. Most common words\n",
    "\n",
    "Count words from both positive and negative reviews by using `make_counter`.  \n",
    "List top ten most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2757d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>8849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie</td>\n",
       "      <td>5429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>3543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just</td>\n",
       "      <td>2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>2313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>story</td>\n",
       "      <td>2110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>character</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>characters</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>way</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  freq\n",
       "0        film  8849\n",
       "1       movie  5429\n",
       "2        like  3543\n",
       "3        just  2900\n",
       "4        good  2313\n",
       "5        time  2280\n",
       "6       story  2110\n",
       "7   character  1902\n",
       "8  characters  1813\n",
       "9         way  1668"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = make_counter(data[\"pos\"] + data[\"neg\"], stopwords)\n",
    "pd.DataFrame(counter.most_common(10), columns=[\"word\", \"freq\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217121e",
   "metadata": {},
   "source": [
    "## 5.2. Plot word frequencies\n",
    "\n",
    "Make scatter plot where:\n",
    "* x axis represents words sorted by frequencies and\n",
    "* y axis are frequencies (use log scale for this axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "855309af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freqs = sorted(counter.values())\n",
    "\n",
    "plt.scatter(range(len(freqs)), freqs, marker=\".\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.xlabel(\"words\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0415047",
   "metadata": {},
   "source": [
    "## 6.1. TF-IDF tensor\n",
    "\n",
    "Make TF-IDF tensor for positive and negative reviews. Tensor must have 2000 rows (1000 positive and 1000 negative reviews).  \n",
    "Calculate ratio of non-zero values in the TF-IDF tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7563fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = make_tfidf(data[\"pos\"] + data[\"neg\"], stopwords)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bf0f3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0062)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.count_nonzero() / torch.prod(torch.tensor(tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e49e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf[:1000].count_nonzero() == torch.tensor(242389)\n",
    "assert tfidf[1000:].count_nonzero() == torch.tensor(218810)\n",
    "\n",
    "assert torch.isclose(torch.sum(tfidf), torch.tensor(24080.7049, dtype=torch.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7510e4f",
   "metadata": {},
   "source": [
    "## 6.2. TF-IDF tensor slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494003cf",
   "metadata": {},
   "source": [
    "Create two tensors, one for positive and one for negative reviews by slicing previously made TF-IDF tensor of both reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "583ab4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = len(data[\"pos\"])\n",
    "\n",
    "tfidf_pos = tfidf[:n_pos,:]\n",
    "tfidf_neg = tfidf[n_pos:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529b5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf_pos.count_nonzero() == torch.tensor(242389)\n",
    "assert tfidf_neg.count_nonzero() == torch.tensor(218810)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc5151",
   "metadata": {},
   "source": [
    "## 6.3. Similarity\n",
    "\n",
    "Create similarity tensor whose (i, j) value is cosine similarity of i-th positive review and j-th negative review.  \n",
    "Cosine similarity between two vectors $u$ and $v$ is \n",
    "$$cos(u, v) = \\frac{uv}{\\lVert u \\rVert \\lVert v \\rVert} = \\frac{\\sum_{i=1}^{n} u_i v_i}{\\sqrt{\\sum_{i=1}^{n} u_i^2} \\sqrt{\\sum_{i=1}^{n} v_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e9bf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pos = torch.sum(tfidf_pos ** 2, dim=1) ** 0.5\n",
    "norm_neg = torch.sum(tfidf_neg ** 2, dim=1) ** 0.5\n",
    "\n",
    "sim = torch.mm(tfidf_pos, tfidf_neg.T)\n",
    "sim = sim / (norm_pos * norm_neg.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6429b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim.shape == (tfidf_pos.shape[0], tfidf_neg.shape[0]), f\"shape of sim tensor is {tuple(sim.shape)}, but must be (1000, 1000)\"\n",
    "\n",
    "assert torch.allclose(sim[:3,:3], torch.tensor([[0.0492, 0.0326, 0.0477],\n",
    "                                                [0.0463, 0.0240, 0.0539],\n",
    "                                                [0.0462, 0.0409, 0.0445]], dtype=torch.float64), atol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a33acd",
   "metadata": {},
   "source": [
    "## 6.4. Most similar positive and negative review\n",
    "\n",
    "Find most similar positive and negative review. \n",
    "Print their sorted tokenized text without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec1c1f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[789, 697]]), tensor(0.7231, dtype=torch.float64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sim_indices = (sim == torch.max(sim)).nonzero()\n",
    "\n",
    "pos_i, neg_i = max_sim_indices[0]\n",
    "max_sim_indices, torch.max(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a117286",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (pos_i, neg_i) == (789, 697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "886d90ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident actually alvarado angeles animals animatronic appreciate apprehended attending attention audience babe bad bad bambi basement basement beings best biggest billing bird bird bird bird bird bird bird bird bird birthday birthday bites blamed boyfriend break buddy buy buying calls cameos cartoons cat cat cat cat characterization characters characters cheech christmas come creature crowd cruel currently customer cute cute dads dalmatian day day dies discovers disney doing doubles dressed eccentric eccentric eisenberg elderly elderly elderly end ends example exclaim experiencing explaining explains facility facility falls finally forth friend friend friend friend friends friends friends furball gena gena gets girl good good goodbye got got hackett hackett hallie heads hear hear help home home home home human human human human hysterically idea imagine impediment impression imprisoned infant influence ingredients interesting ivy ivy janitor jay jay jay jennifer jewelry kate kid kids kids kids languishing lassie lassie later laughing learns lessons lessons life like like like like line little little little little local los loses loses loses lost low luck main makes marie marie marie marie marin meandering means meeting meeting meets meets meets members memorable miniature misha misha misha mohr mohr mohr mohr moms money moral movie movies movies named neglect newborn okay old opinion owner owner owners owns pace parrot parrot parrot parrot parrot parrots participate paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie pawn pawn people perfect person person personalities picture play played plot popcorn popular portray pound pup purchased raised real refuses regales regular requires research research return rowlands rowlands russian sassy saying says science sent shalhoub shop shop sister sold soon soon speech speech statements steal story story story strutting stupid succession support sure synopsis taken talking theater think thinks time tony tries trini trying trying trying twice tykes unfairly unfortunately usually version victim voice voice watch watching way wisecracks wish years young youngsters\n"
     ]
    }
   ],
   "source": [
    "pos_tokens = sorted(tok for tok in tokenize(data[\"pos\"][pos_i]) if tok not in stopwords)\n",
    "\n",
    "print(\" \".join(pos_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27356ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acted action actress add age ages allows animal animals antics atms audience autobiography awesome bad beauty begins believe benny benny best best big biggest bird bird bird bird bird body brain brings calls cat cat character character character charm cheech chef cinematic clashing comedic comedy comes complaint composed confined contains cope creates cringes crisp crook cut cute cymbals dad dancing daughter dead definite deliberately delightful designed director dissolve doses dreamy eisenberg elicit enchanting ensues enthralling example excellent explains explode family family feels finally fine flight fly frequently friend funny gave gave genuine gets gets gina gives good good good got hairball half hallie hand heart heavy home humans humor humor humor hums immigrant impediment instant insulting janitor jay jeffrey john john kate keeps kind knows lab laurie lead learn lends length let like like lines listen literature lives living long long long loud makes manages marie marie marie marin material material merely mild mimicking minor misha misha misha mohr mohr mohr mom mosey motion movie movie movie movie movie movie movie music naturally new newcomer news nice night noting orchestra original pacing parrot parrot parrot parts parts paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie paulie perfectly performance performed pg physical pickup picture picture played played played plays precious problems profanities promise pull quality quite quite raged rapid recent reflects reinforcing research rich rid roars roberts role role routines rowlands runs runs russian russian s sam says scams scenes script seen shalhoub silence sing sitcom situation skills sleepy slow small small small soaring solo somber son speech spots standard star stealing stories story story strong strutting study stupid subtle subtle sweetness sympathy taken takes takes talk talking teacher tedium tells themes thought thought thought thousand time times tony tricking twenties u use uses viewers violin voice voice wait wanted wants watch way worth'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tokens = sorted(tok for tok in tokenize(data[\"neg\"][neg_i]) if tok not in stopwords)\n",
    "\n",
    "\" \".join(neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb847e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "358f19b5168dcc2c817c22e8ae2c189228565b53de3b91095ee770a390daccdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
